{"Title":"Sector Rotation by Factor Model and Fundamental Analysis","Link":"https:\/\/arxiv.org\/abs\/2401.00001","Authors":"Runjia Yang, Beining Shi","Abstract":"This study presents an analytical approach to sector rotation, leveraging both factor models and fundamental metrics. We initiate with a systematic classification of sectors, followed by an empirical investigation into their returns. Through factor analysis, the paper underscores the significance of momentum and short-term reversion in dictating sectoral shifts. A subsequent in-depth fundamental analysis evaluates metrics such as PE, PB, EV-to-EBITDA, Dividend Yield, among others. Our primary contribution lies in developing a predictive framework based on these fundamental indicators. The constructed models, post rigorous training, exhibit noteworthy predictive capabilities. The findings furnish a nuanced understanding of sector rotation strategies, with implications for asset management and portfolio construction in the financial domain."}
{"Title":"Representation of forward performance criteria with random endowment via FBSDE and application to forward optimized certainty equivalent","Link":"https:\/\/arxiv.org\/abs\/2401.00103","Authors":"Gechun Liang, Yifan Sun, Thaleia Zariphopoulou","Abstract":"We extend the notion of forward performance criteria to settings with random endowment in incomplete markets. Building on these results, we introduce and develop the novel concept of forward optimized certainty equivalent (forward OCE), which offers a genuinely dynamic valuation mechanism that accommodates progressively adaptive market model updates, stochastic risk preferences, and incoming claims with arbitrary maturities.\nIn parallel, we develop a new methodology to analyze the emerging stochastic optimization problems by directly studying the candidate optimal control processes for both the primal and dual problems. Specifically, we derive two new systems of forward-backward stochastic differential equations (FBSDEs) and establish necessary and sufficient conditions for optimality, and various equivalences between the two problems. This new approach is general and complements the existing one based on backward stochastic partial differential equations (backward SPDEs) for the related value functions. We, also, consider representative examples for both forward performance criteria with random endowment and forward OCE, and for the case of exponential criteria, we investigate the connection between forward OCE and forward entropic risk measures."}
{"Title":"Enhancing CVaR portfolio optimisation performance with GAM factor models","Link":"https:\/\/arxiv.org\/abs\/2401.00188","Authors":"Davide Lauria, W. Brent Lindquist, Svetlozar T. Rachev","Abstract":"We propose a discrete-time econometric model that combines autoregressive filters with factor regressions to predict stock returns for portfolio optimisation purposes. In particular, we test both robust linear regressions and general additive models on two different investment universes composed of the Dow Jones Industrial Average and the Standard & Poor's 500 indexes, and we compare the out-of-sample performances of mean-CVaR optimal portfolios over a horizon of six years. The results show a substantial improvement in portfolio performances when the factor model is estimated with general additive models."}
{"Title":"Does the World Bank's Ease of Doing Business Index Matter for FDI? Findings from Africa","Link":"https:\/\/arxiv.org\/abs\/2401.00227","Authors":"Bhaso Ndzendze","Abstract":"This paper investigates whether foreign investment (FDI) into Africa is at least partially responsive to World Bank-measured market friendliness. Specifically, I conducted analyses of four countries between 2009 and 2017, using cases that represent two of the highest scorers on the bank's Doing Business index as of 2008 (Mauritius and South Africa) and the two lowest scorers (DRC and CAR), and subsequently traced all four for growths or declines in FDI in relation to their scores in the index. The findings show that there is a moderate association between decreased costs of starting a business and growth of FDI. Mauritius, South Africa and the DRC reduced their total cost of starting a business by 71.7%, 143.7% and 122.9% for the entire period, and saw inward FDI increases of 167.6%, 79.8% and 152.21%, respectively. The CAR increased the cost of starting businesses but still saw increases in FDI. However, the country also saw the least amount of growth in FDI at only 13.3%."}
{"Title":"A framework for the valuation of insurance liabilities by production cost","Link":"https:\/\/arxiv.org\/abs\/2401.00263","Authors":"Christoph Moehr","Abstract":"This paper sets out a framework for the valuation of insurance liabilities that is intended to be economically realistic, elementary, reasonably practically applicable, and as a special case to provide a basis for the valuation in regulatory solvency systems such as Solvency II and the SST. The valuation framework is based on the cost of producing the liabilities to an insurance company that is subject to solvency regulation (regulatory solvency capital requirements) and insolvency laws (consequences of failure) in finite discrete time. Starting from the replication approach of classical no-arbitrage theory, the framework additionally considers the nature and cost of capital (expressed by a ``financiability condition\"), that the liabilities may be required to be fulfilled only ``in sufficiently many cases\" (expressed by a ``fulfillment condition\"), production using ``fully illiquid\" assets in addition to tradables, and the asymmetry between assets and liabilities. We identify necessary and sufficient conditions on the capital investment under which the framework recovers the market prices of tradables, investigate extending production to take account of insolvency, implications of using illiquid assets in the production, and show how Solvency II and SST valuation can be derived with specific assumptions."}
{"Title":"Minimalist Market Design: A Framework for Economists with Policy Aspirations","Link":"https:\/\/arxiv.org\/abs\/2401.00307","Authors":"Tayfun S\u00f6nmez","Abstract":"Earlier in my career, prevalent approaches in the emerging field of market design largely represented the experiences and perspectives of leaders who were commissioned to design or reform various institutions. Since being commissioned for a similar task seemed unlikely for me as an aspiring design economist, I developed my own minimalist approach to market design. Using the policy objectives of stakeholders, my approach creates a new institution from the existing one with minimal interference with its elements that compromise the objectives. Minimalist market design initially evolved through my integrated research and policy efforts in school choice from 1997 to 2005 and in kidney exchange from 2003 to 2007. Given its success in school choice and kidney exchange, I systematically followed this approach in many other, often unusual real-world settings. In recent years, my efforts in minimalist market design led to the 2021 reform of the US Army's branching system for its cadets to military specialties, the adoption of reserve systems during the Covid-19 pandemic for vaccine allocation in 15 states and therapies in 2 states, and the deployment of a highly efficient liver exchange system in T\u00fcrkiye. This same methodology also predicted the rescission of a 1995 Supreme Court judgment in India, resulting in countless litigations and interruptions of public recruitment for 25 years, as well as the mandates of its replacement. In this monograph, I describe the philosophy, evolution, and successful applications of minimalist market design, contrasting it with the mainstream paradigm for the field. In doing so, I also provide a paradigm for economists who want to influence policy and change institutions through their research."}
{"Title":"Optimization of portfolios with cryptocurrencies: Markowitz and GARCH-Copula model approach","Link":"https:\/\/arxiv.org\/abs\/2401.00507","Authors":"Vahidin Jeleskovic, Claudio Latini, Zahid I. Younas, Mamdouh A. S. Al-Faryan","Abstract":"The growing interest in cryptocurrencies has drawn the attention of the financial world to this innovative medium of exchange. This study aims to explore the impact of cryptocurrencies on portfolio performance. We conduct our analysis retrospectively, assessing the performance achieved within a specific time frame by three distinct portfolios: one consisting solely of equities, bonds, and commodities; another composed exclusively of cryptocurrencies; and a third, which combines both 'traditional' assets and the best-performing cryptocurrency from the second this http URL achieve this, we employ the classic variance-covariance approach, utilizing the GARCH-Copula and GARCH-Vine Copula methods to calculate the risk structure. The optimal asset weights within the optimized portfolios are determined through the Markowitz optimization problem. Our analysis predominantly reveals that the portfolio comprising both cryptocurrency and traditional assets exhibits a higher Sharpe ratio from a retrospective viewpoint and demonstrates more stable performances from a prospective perspective. We also provide an explanation for our choice of portfolio optimization based on the Markowitz approach rather than CVaR and ES."}
{"Title":"Actualised and future changes in regional economic growth through sea level rise","Link":"https:\/\/arxiv.org\/abs\/2401.00535","Authors":"Theodoros Chatzivasileiadis, Ignasi Cortes Arbues, Jochen Hinkel, Daniel Lincke, Richard S.J. Tol","Abstract":"This study investigates the long-term economic impact of sea-level rise (SLR) on coastal regions in Europe, focusing on Gross Domestic Product (GDP). Using a novel dataset covering regional SLR and economic growth from 1900 to 2020, we quantify the relationships between SLR and regional GDP per capita across 79 coastal EU & UK regions. Our results reveal that the current SLR has already negatively influenced GDP of coastal regions, leading to a cumulative 4.7% loss at 39 cm of SLR. Over the 120 year period studied, the actualised impact of SLR on the annual growth rate is between -0.02% and 0.04%. Extrapolating these findings to future climate and socio-economic scenarios, we show that in the absence of additional adaptation measures, GDP losses by 2100 could range between -6.3% and -20.8% under the most extreme SLR scenario (SSP5-RCP8.5 High-end Ice, or -4.0% to -14.1% in SSP5-RCP8.5 High Ice). This statistical analysis utilising a century-long dataset, provides an empirical foundation for designing region-specific climate adaptation strategies to mitigate economic damages caused by SLR. Our evidence supports the argument for strategically relocating assets and establishing coastal setback zones when it is economically preferable and socially agreeable, given that protection investments have an economic impact."}
{"Title":"On the implied volatility of Inverse options under stochastic volatility models","Link":"https:\/\/arxiv.org\/abs\/2401.00539","Authors":"Elisa Al\u00f2s, Eulalia Nualart, Makar Pravosud","Abstract":"In this paper we study short-time behavior of the at-the-money implied volatility for Inverse European options with fixed strike price. The asset price is assumed to follow a general stochastic volatility process. Using techniques of the Malliavin calculus such as the anticipating It^o's formula we first compute the level of the implied volatility of the option when the maturity converges to zero. Then, we find a short maturity asymptotic formula for the skew of the implied volatility that depends on the roughness of the volatility model. We also show that our results extend easily to Quanto-Inverse options. We apply our general results to the SABR and fractional Bergomi models, and provide some numerical simulations that confirm the accurateness of the asymptotic formula for the skew. Finally, we provide an empirical application using Bitcoin options traded on Debirit to show how our theoretical formulas can be used to model real market data of such options."}
{"Title":"Intraday Trading Algorithm for Predicting Cryptocurrency Price Movements Using Twitter Big Data Analysis","Link":"https:\/\/arxiv.org\/abs\/2401.00603","Authors":"Vahidin Jeleskovic, Stephen Mackay","Abstract":"Cryptocurrencies have emerged as a novel financial asset garnering significant attention in recent years. A defining characteristic of these digital currencies is their pronounced short-term market volatility, primarily influenced by widespread sentiment polarization, particularly on social media platforms such as Twitter. Recent research has underscored the correlation between sentiment expressed in various networks and the price dynamics of cryptocurrencies. This study delves into the 15-minute impact of informative tweets disseminated through foundation channels on trader behavior, with a focus on potential outcomes related to sentiment polarization. The primary objective is to identify factors that can predict positive price movements and potentially be leveraged through a trading algorithm. To accomplish this objective, we conduct a conditional examination of return and excess return rates within the 15 minutes following tweet publication. The empirical findings reveal statistically significant increases in return rates, particularly within the initial three minutes following tweet publication. Notably, adverse effects resulting from the messages were not observed. Surprisingly, sentiments were found to have no discerni-ble impact on cryptocurrency price movements. Our analysis further identifies that inves-tors are primarily influenced by the quality of tweet content, as reflected in the choice of words and tweet volume. While the basic trading algorithm presented in this study does yield some benefits within the 15-minute timeframe, these benefits are not statistically significant. Nevertheless, it serves as a foundational framework for potential enhance-ments and further investigations."}
{"Title":"Urban and non-urban contributions to the social cost of carbon","Link":"https:\/\/arxiv.org\/abs\/2401.00919","Authors":"Francisco Estrada, Veronica Lupi, Wouter Botzen, Richard S.J. Tol","Abstract":"The social cost of carbon (SCC) serves as a concise gauge of climate change's economic impact, often reported at the global and country level. SCC values are disproportionately high for less-developed, populous countries. Assessing the contributions of urban and non-urban areas to the SCC can provide additional insights for climate policy. Cities are essential for defining global emissions, influencing warming levels and associated damages. High exposure and concurrent socioenvironmental problems exacerbate climate change risks in cities. Using a spatially explicit integrated assessment model, the SCC is estimated at USD137-USD579\/tCO2, rising to USD262-USD1,075\/tCO2 when including urban heat island (UHI) warming. Urban SCC dominates, with both urban exposure and the UHI contributing significantly. A permanent 1% reduction of the UHI in urban areas yields net present benefits of USD484-USD1,562 per urban dweller. Global cities have significant leverage and incentives for a swift transition to a low-carbon economy, and for reducing local warming."}
{"Title":"A Portfolio's Common Causal Conditional Risk-neutral PDE","Link":"https:\/\/arxiv.org\/abs\/2401.00949","Authors":"Alejandro Rodriguez Dominguez","Abstract":"Portfolio's optimal drivers for diversification are common causes of the constituents' correlations. A closed-form formula for the conditional probability of the portfolio given its optimal common drivers is presented, with each pair constituent-common driver joint distribution modelled by Gaussian copulas. A conditional risk-neutral PDE is obtained for this conditional probability as a system of copulas' PDEs, allowing for dynamical risk management of a portfolio as shown in the experiments. Implied conditional portfolio volatilities and implied weights are new risk metrics that can be dynamically monitored from the PDEs or obtained from their solution."}
{"Title":"Almost Perfect Shadow Prices","Link":"https:\/\/arxiv.org\/abs\/2401.00970","Authors":"Eberhard Mayerhofer","Abstract":"Shadow prices simplify the derivation of optimal trading strategies in markets with transaction costs by transferring optimization into a more tractable, frictionless market. This paper establishes that a na\u00efve shadow price Ansatz for maximizing long term returns given average volatility yields a strategy that is, for small bid-ask-spreads, asymptotically optimal at third order. Considering the second-order impact of transaction costs, such a strategy is essentially optimal. However, for risk aversion different from one, we devise alternative strategies that outperform the shadow market at fourth order. Finally, it is shown that the risk-neutral objective rules out the existence of shadow prices."}
{"Title":"Global analysis reveals persistent shortfalls and regional differences in availability of foods needed for health","Link":"https:\/\/arxiv.org\/abs\/2401.01080","Authors":"Leah Costlow, Anna Herforth, Timothy B. Sulser, Nicola Cenacchi, William A. Masters","Abstract":"Most people around the world still lack access to sufficient quantities of all food groups needed for an active and healthy life. This study traces historical and projected changes in global food systems toward alignment with the new Healthy Diet Basket (HDB) used by UN agencies and the World Bank to monitor the cost and affordability of healthy diets worldwide. Using the HDB as a standard to measure adequacy of national, regional and global supply-demand balances, we find substantial but inconsistent progress toward closer alignment with dietary guidelines, with large global shortfalls in fruits, vegetables, and legumes, nuts, and seeds, and large disparities among regions in use of animal source foods. Projections show that additional investments aimed at reducing chronic hunger would modestly accelerate improvements in adequacy where shortfalls are greatest, revealing the need for complementary investments to increase access to under-consumed food groups especially in low-income countries."}
{"Title":"Nash Equilibria in Greenhouse Gas Offset Credit Markets","Link":"https:\/\/arxiv.org\/abs\/2401.01427","Authors":"Liam Welsh, Sebastian Jaimungal","Abstract":"One approach to reducing greenhouse gas (GHG) emissions is to incentivize carbon capturing and carbon reducing projects while simultaneously penalising excess GHG output. In this work, we present a novel market framework and characterise the optimal behaviour of GHG offset credit (OC) market participants in both single-player and two-player settings. The single player setting is posed as an optimal stopping and control problem, while the two-player setting is posed as optimal stopping and mixed-Nash equilibria problem. We demonstrate the importance of acting optimally using numerical solutions and Monte Carlo simulations and explore the differences between the homogeneous and heterogeneous players. In both settings, we find that market participants benefit from optimal OC trading and OC generation."}
{"Title":"An arbitrage driven price dynamics of Automated Market Makers in the presence of fees","Link":"https:\/\/arxiv.org\/abs\/2401.01526","Authors":"Joseph Najnudel, Shen-Ning Tung, Kazutoshi Yamazaki, Ju-Yi Yen","Abstract":"We present a model for price dynamics in the Automated Market Makers (AMM) setting. Within this framework, we propose a reference market price following a geometric Brownian motion. The AMM price is constrained by upper and lower bounds, determined by constant multiplications of the reference price. Through the utilization of local times and excursion-theoretic approaches, we derive several analytical results, including its time-changed representation and limiting behavior."}
{"Title":"Notes on the SWIFT method based on Shannon Wavelets for Option Pricing -- Revisited","Link":"https:\/\/arxiv.org\/abs\/2401.01758","Authors":"Fabien Le Floc'h","Abstract":"This note revisits the SWIFT method based on Shannon wavelets to price European options under models with a known characteristic function in 2023. In particular, it discusses some possible improvements and exposes some concrete drawbacks of the method."}
{"Title":"Impact of Green Marketing Strategy on Brand Awareness: Business, Management, and Human Resources Aspects","Link":"https:\/\/arxiv.org\/abs\/2401.02042","Authors":"Mahdi Nohekhan, Mohammadmahdi Barzegar","Abstract":"Given the move towards industrialization in societies, the increase in dynamism and competition among companies to capture market share, raising concerns about the environment, government, and international regulations and obligations, increased consumer awareness, pressure from nature-loving groups, etc., organizations have become more attentive to issues related to environmental management. Over time, concepts such as green marketing have permeated marketing literature, making environmental considerations one of the most important activities of companies. To this end, this research examines the impact of green marketing strategy on brand awareness (case study: food exporting companies). The population of this research consists of 345 employees and managers of companies like Kalleh, Solico, Pemina, Sorbon, Mac, Pol, and Castle, from which 182 individuals were randomly selected as the sample using Cochran's formula. This research is practical, and the required data have been collected through a survey and a questionnaire. The research results indicate that (1) green marketing strategy significantly affects brand awareness. (2) Green products have a significant positive effect on brand awareness. (3) Green promotions have a significant positive effect on brand awareness. (4) Green distribution has a significant positive effect on brand awareness. (5) Green pricing has a significant positive effect on brand awareness."}
{"Title":"Forecasting Bitcoin Volatility: A Comparative Analysis of Volatility Approaches","Link":"https:\/\/arxiv.org\/abs\/2401.02049","Authors":"Cristina Chinazzo, Vahidin Jeleskovic","Abstract":"This paper conducts an extensive analysis of Bitcoin return series, with a primary focus on three volatility metrics: historical volatility (calculated as the sample standard deviation), forecasted volatility (derived from GARCH-type models), and implied volatility (computed from the emerging Bitcoin options market). These measures of volatility serve as indicators of market expectations for conditional volatility and are compared to elucidate their differences and similarities. The central finding of this study underscores a notably high expected level of volatility, both on a daily and annual basis, across all the methodologies employed. However, it's crucial to emphasize the potential challenges stemming from suboptimal liquidity in the Bitcoin options market. These liquidity constraints may lead to discrepancies in the computed values of implied volatility, particularly in scenarios involving extreme moneyness or maturity. This analysis provides valuable insights into Bitcoin's volatility landscape, shedding light on the unique characteristics and dynamics of this cryptocurrency within the context of financial markets."}
{"Title":"Female Entrepreneur on Board:Assessing the Effect of Gender on Corporate Financial Constraints","Link":"https:\/\/arxiv.org\/abs\/2401.02134","Authors":"Ruiying Xiao","Abstract":"This study investigates the impact of female leadership on the financial constraints of firms, which are publicly listed entrepreneurial enterprises in China. Utilizing data from 938 companies on the China Growth Enterprise Market (GEM) over a period of 2013-2022, this paper explores how the female presence in CEO positions, senior management, and board membership influences a firm's ability to manage financial constraints. Our analysis employs the Kaplan-Zingales (KZ) Index to measure these constraints, encompassing some key financial factors such as cash flow, dividends, and leverage. The findings reveal that companies with female CEOs or a higher proportion of women in top management are associated with reduced financial constraints. However, the influence of female board members is less clear-cut. Our study also delves into the variances of these effects between high-tech and low-tech industry sectors, emphasizing how internal gender biases in high-tech industries may impede the alleviation of financing constraints on firms. This research contributes to a nuanced understanding of the role of gender dynamics in corporate financial management, especially in the context of China's evolving economic landscape. It underscores the importance of promoting female leadership not only for gender equity but also for enhancing corporate financial resilience."}
{"Title":"Airport service quality perception and flight delays: examining the influence of psychosituational latent traits of respondents in passenger satisfaction surveys","Link":"https:\/\/arxiv.org\/abs\/2401.02139","Authors":"Alessandro V. M. Oliveira, Bruno F. Oliveira, Moises D. Vassallo","Abstract":"The service quality of a passenger transport operator can be measured through face-to-face surveys at the terminals or on board. However, the resulting responses may suffer from the influence of the intrinsic aspects of the respondent's personality and emotional context at the time of the interview. This study proposes a methodology to generate and select control variables for these latent psychosituational traits, thus mitigating the risk of omitted variable bias. We developed an econometric model of the determinants of passenger satisfaction in a survey conducted at the largest airport in Latin America, S\u00e3o Paulo GRU Airport. Our focus was on the role of flight delays in the perception of quality. The results of this study confirm the existence of a relationship between flight delays and the global satisfaction of passengers with airports. In addition, favorable evaluations regarding airports' food\/beverage concessions and Wi-Fi services, but not their retail options, have a relevant moderating effect on that relationship. Furthermore, dissatisfaction arising from passengers' interaction with the airline can have negative spillover effects on their satisfaction with the airport. We also found evidence of blame-attribution behavior, in which only delays of internal origin, such as failures in flight management, are significant, indicating that passengers overlook weather-related flight delays. Finally, the results suggest that an empirical specification that does not consider the latent psychosituational traits of passengers produces a relevant overestimation of the absolute effect of flight delays on passenger satisfaction."}
{"Title":"Game Mining: How to Make Money from those about to Play a Game","Link":"https:\/\/arxiv.org\/abs\/2401.02353","Authors":"James W. Bono, David H. Wolpert","Abstract":"It is known that a player in a noncooperative game can benefit by publicly restricting his possible moves before play begins. We show that, more generally, a player may benefit by publicly committing to pay an external party an amount that is contingent on the game's outcome. We explore what happens when external parties -- who we call ``game miners'' -- discover this fact and seek to profit from it by entering an outcome-contingent contract with the players. We analyze various structured bargaining games between miners and players for determining such an outcome-contingent contract. These bargaining games include playing the players against one another, as well as allowing the players to pay the miner(s) for exclusivity and first-mover advantage. We establish restrictions on the strategic settings in which a game miner can profit and bounds on the game miner's profit. We also find that game miners can lead to both efficient and inefficient equilibria."}
{"Title":"Opinion formation in the world trade network","Link":"https:\/\/arxiv.org\/abs\/2401.02378","Authors":"C\u00e9lestin Coquid\u00e9, Jos\u00e9 Lages, Dima L. Shepelyansky","Abstract":"We extend the opinion formation approach to probe the world influence of economical organizations. Our opinion formation model mimics a battle between currencies within the international trade network. Based on the United Nations Comtrade database, we construct the world trade network for the years of the last decade from 2010 to 2020. We consider different core groups constituted by countries preferring to trade in a specific currency. We will consider principally two core groups, namely, 5 Anglo-Saxon countries which prefer to trade in US dollar and the 11 BRICS+ which prefer to trade in a hypothetical currency, hereafter called BRI, pegged to their economies. We determine the trade currency preference of the other countries via a Monte Carlo process depending on the direct transactions between the countries. The results obtained in the frame of this mathematical model show that starting from year 2014 the majority of the world countries would have preferred to trade in BRI than USD. The Monte Carlo process reaches a steady state with 3 distinct groups: two groups of countries preferring, whatever is the initial distribution of the trade currency preferences, to trade, one in BRI and the other in USD, and a third group of countries swinging as a whole between USD and BRI depending on the initial distribution of the trade currency preferences. We also analyze the battle between USD, EUR and BRI, and present the reduced Google matrix description of the trade relations between the Anglo-Saxon countries and the BRICS+."}
{"Title":"Cu\\'anto es demasiada inflaci\\'on? Una clasificaci\\'on de reg\\'imenes inflacionarios","Link":"https:\/\/arxiv.org\/abs\/2401.02428","Authors":"Manuel de Mier, Fernando Delbianco","Abstract":"The classifications of inflationary regimes proposed in the literature have mostly been based on arbitrary characterizations, subject to value judgments by researchers. The objective of this study is to propose a new methodological approach that reduces subjectivity and improves accuracy in the construction of such regimes. The method is built upon a combination of clustering techniques and classification trees, which allows for an historical periodization of Argentina's inflationary history for the period 1943-2022. Additionally, two procedures are introduced to smooth out the classification over time: a measure of temporal contiguity of observations and a rolling method based on the simple majority rule. The obtained regimes are compared against the existing literature on the inflation-relative price variability relationship, revealing a better performance of the proposed regimes."}
{"Title":"Constrained Max Drawdown: a Fast and Robust Portfolio Optimization Approach","Link":"https:\/\/arxiv.org\/abs\/2401.02601","Authors":"Albert Dorador","Abstract":"We propose an alternative linearization to the classical Markowitz quadratic portfolio optimization model, based on maximum drawdown. This model, which minimizes maximum portfolio drawdown, is particularly appealing during times of financial distress, like during the COVID-19 pandemic. In addition, we will present a Mixed-Integer Linear Programming variation of our new model that, based on our out-of-sample results and sensitivity analysis, delivers a more profitable and robust solution with a 200 times faster solving time compared to the standard Markowitz quadratic formulation."}
{"Title":"Displaying risk in mergers: a diagrammatic approach for exchange ratio determination","Link":"https:\/\/arxiv.org\/abs\/2401.02681","Authors":"Alessandra Mainini, Enrico Moretto, Daniela Visetti","Abstract":"This article extends, in a stochastic setting, previous results in the determination of feasible exchange ratios for merging companies. A first outcome is that shareholders of the companies involved in the merging process face both an upper and a lower bounds for acceptable exchange ratios. Secondly, in order for the improved `bargaining region' to be intelligibly displayed, the diagrammatic approach developed by Kulpa is exploited."}
{"Title":"L'utilit\\'e de l'\\'echelle op\\'eratique pour consid\\'erer des strat\\'egies d'intelligence et de guerre \\'economique","Link":"https:\/\/arxiv.org\/abs\/2401.02963","Authors":"St\u00e9phane Goria (Crem)","Abstract":"The 20th century saw the emergence of an intermediate level of consideration, situated between the tactical and strategic levels: the operational level. This level of scale is that of a large area of operations, that is to say coordinated engagements bringing together forces belonging to different corps over a considerable geographical distance. In this article, we approach operatics and its implementation, called operative art or operational art, first from its contributions to military thinking, then we transpose what can be transposed beyond this domain. We consider operative art as a new point of view generating a particular thought and solutions based on agile systems, but which may have reached their limits. We will limit ourselves here to the question of the potential contributions of the operatic perspective to understand or implement a set of actions related to competitive or economic warfare. To do this, we will begin by providing some answers to the question of what operatic military is in terms of its characteristics. Then, we propose a way to transpose them in the context of companies preferably confronted with a context of competitive or economic warfare by taking as an example the acquisition of the essential activities of the company Alstom by the company General Electric (GE)."}
{"Title":"The Effects of COVID-19 and the Russia-Ukraine War on Inward Foreign Direct Investment","Link":"https:\/\/arxiv.org\/abs\/2401.03096","Authors":"MS Hosen, SM Hossain, MN Mia, MR Chowdhury","Abstract":"Inward Foreign Direct Investment (IFDI) into Europe and Asian developing countries like Bangladesh is experimentally examined in this study. IFDI in emerging markets has been boosted by global investment and inflow influenced by resource availability and public policy. The economic policy uncertainty on IFDI in 13 countries is explored at a time when the crisis between Russia and Ukraine war is having a global impact. Microeconomic factors affected Gross Domestic Product (GDP) growth, inflation, interest rates, and the currency rate fluctuated with IFDI, which mostly shocked during COVID-19 and the Russia-Ukraine war. With data from the World Bank and the United Nations Conference on Trade and Development (UNCTAD) database, we compile a panel dataset covering 2018-2022. The researchers used a mixture of panel and linear regression analysis using a random effect model. Our findings show that the impact of global rates hurts IFDI in 13 selected countries. There is a correlation between a country's ability to enforce contracts and the amount of Inward FDI it receives. Using the top 13 hosts of incoming FDI flows COVID-19 and Russia-Ukraine wartime series analysis gives valuable information for policymakers in the remaining countries chosen to attract IFDI inflows."}
{"Title":"Optimal Order Execution subject to Reservation Strategies under Execution Risk","Link":"https:\/\/arxiv.org\/abs\/2401.03305","Authors":"Xue Cheng, Peng Guo, Tai-ho Wang","Abstract":"The paper addresses the problem of meta order execution from a broker-dealer's point of view in Almgren-Chriss model under order fill uncertainty. A broker-dealer agency is authorized to execute an order of trading on client's behalf. The strategies that the agent is allowed to deploy is subject to a benchmark, referred to as the reservation strategy, regulated by the client. We formulate the broker's problem as a utility maximization problem in which the broker seeks to maximize his utility of excess profit-and-loss at the execution horizon. Optimal strategy in feedback form is obtained in closed form. In the absence of execution risk, the optimal strategies subject to reservation strategies are deterministic. We establish an affine structure among the trading trajectories under optimal strategies subject to general reservation strategies using implementation shortfall and target close orders as basis. We conclude the paper with numerical experiments illustrating the trading trajectories as well as histograms of terminal wealth and utility at investment horizon under optimal strategies versus those under TWAP strategies."}
{"Title":"Volatility models in practice: Rough, Path-dependent or Markovian?","Link":"https:\/\/arxiv.org\/abs\/2401.03345","Authors":"Eduardo Abi Jaber, Shaun (Xiaoyuan)Li","Abstract":"An extensive empirical study of the class of Volterra Bergomi models using SPX options data between 2011 and 2022 reveals the following fact-check on two fundamental claims echoed in the rough volatility literature:\nDo rough volatility models with Hurst index H \\in (0,1\/2) really capture well SPX implied volatility surface with very few parameters? No, rough volatility models are inconsistent with the global shape of SPX smiles. They suffer from severe structural limitations imposed by the roughness component, with the Hurst parameter H \\in (0,1\/2) controlling the smile in a poor way. In particular, the SPX at-the-money skew is incompatible with the power-law shape generated by rough volatility models. The skew of rough volatility models increases too fast on the short end, and decays too slow on the longer end where \"negative\" H is sometimes needed.\nDo rough volatility models really outperform consistently their classical Markovian counterparts? No, for short maturities they underperform their one-factor Markovian counterpart with the same number of parameters. For longer maturities, they do not systematically outperform the one-factor model and significantly underperform when compared to an under-parametrized two-factor Markovian model with only one additional calibratable parameter.\nOn the positive side: our study identifies a (non-rough) path-dependent Bergomi model and an under-parametrized two-factor Markovian Bergomi model that consistently outperform their rough counterpart in capturing SPX smiles between one week and three years with only 3 to 4 calibratable parameters. \\end{abstract}"}
{"Title":"Modelling and Predicting the Conditional Variance of Bitcoin Daily Returns: Comparsion of Markov Switching GARCH and SV Models","Link":"https:\/\/arxiv.org\/abs\/2401.03393","Authors":"Dennis Koch, Vahidin Jeleskovic, Zahid I. Younas","Abstract":"This paper introduces a unique and valuable research design aimed at analyzing Bitcoin price volatility. To achieve this, a range of models from the Markov Switching-GARCH and Stochastic Autoregressive Volatility (SARV) model classes are considered and their out-of-sample forecasting performance is thoroughly examined. The paper provides insights into the rationale behind the recommendation for a two-stage estimation approach, emphasizing the separate estimation of coefficients in the mean and variance equations. The results presented in this paper indicate that Stochastic Volatility models, particularly SARV models, outperform MS-GARCH models in forecasting Bitcoin price volatility. Moreover, the study suggests that in certain situations, persistent simple GARCH models may even outperform Markov-Switching GARCH models in predicting the variance of Bitcoin log returns. These findings offer valuable guidance for risk management experts, highlighting the potential advantages of SARV models in managing and forecasting Bitcoin price volatility."}
{"Title":"Structured factor copulas for modeling the systemic risk of European and United States banks","Link":"https:\/\/arxiv.org\/abs\/2401.03443","Authors":"Hoang Nguyen, Audron\u0117 Virbickait\u0117, M. Concepci\u00f3n Aus\u00edn, Pedro Galeano","Abstract":"In this paper, we employ Credit Default Swaps (CDS) to model the joint and conditional distress probabilities of banks in Europe and the U.S. using factor copulas. We propose multi-factor, structured factor, and factor-vine models where the banks in the sample are clustered according to their geographic location. We find that within each region, the co-dependence between banks is best described using both, systematic and idiosyncratic, financial contagion channels. However, if we consider the banking system as a whole, then the systematic contagion channel prevails, meaning that the distress probabilities are driven by a latent global factor and region-specific factors. In all cases, the co-dependence structure of bank CDS spreads is highly correlated in the tail. The out-of-sample forecasts of several measures of systematic risk allow us to identify the periods of distress in the banking sector over the recent years including the COVID-19 pandemic, the interest rate hikes in 2022, and the banking crisis in 2023."}
{"Title":"Spiritual Intelligence's Role in Reducing Technostress through Ethical Work Climates","Link":"https:\/\/arxiv.org\/abs\/2401.03658","Authors":"Saleh Ghobbeh, Armita Atrian","Abstract":"This study explores the impact of spiritual intelligence (SI) on technostress, with a focus on the mediating role of the ethical environment. In an era where technological advancements continually reshape our work and personal lives, understanding the interplay between human intelligence, well-being, and ethics within organizations is increasingly significant. Spiritual intelligence, transcending traditional cognitive and emotional intelligences, emphasizes understanding personal meaning and values. This paper investigates how higher levels of SI enable individuals to integrate technology into their lives without undue stress, and how a robust ethical environment within organizations supports and amplifies these benefits. Through a comprehensive review of literature, empirical research, and detailed analysis, the study highlights the protective role of SI against technostress and the significant influence of an ethical climate in enhancing this effect. The findings offer valuable insights for organizational strategies aimed at promoting a harmonious, stress-free workplace environment."}
{"Title":"Can Large Language Models Beat Wall Street? Unveiling the Potential of AI in Stock Selection","Link":"https:\/\/arxiv.org\/abs\/2401.03737","Authors":"Georgios Fatouros, Konstantinos Metaxas, John Soldatos, Dimosthenis Kyriazis","Abstract":"This paper introduces MarketSenseAI, an innovative framework leveraging GPT-4's advanced reasoning for selecting stocks in financial markets. By integrating Chain of Thought and In-Context Learning, MarketSenseAI analyzes diverse data sources, including market trends, news, fundamentals, and macroeconomic factors, to emulate expert investment decision-making. The development, implementation, and validation of the framework are elaborately discussed, underscoring its capability to generate actionable and interpretable investment signals. A notable feature of this work is employing GPT-4 both as a predictive mechanism and signal evaluator, revealing the significant impact of the AI-generated explanations on signal accuracy, reliability and acceptance. Through empirical testing on the competitive S&P 100 stocks over a 15-month period, MarketSenseAI demonstrated exceptional performance, delivering excess alpha of 10% to 30% and achieving a cumulative return of up to 72% over the period, while maintaining a risk profile comparable to the broader market. Our findings highlight the transformative potential of Large Language Models in financial decision-making, marking a significant leap in integrating generative AI into financial analytics and investment strategies."}
{"Title":"Saving for sunny days: The impact of climate (change) on consumer prices in the euro area","Link":"https:\/\/arxiv.org\/abs\/2401.03740","Authors":"Paulo M. M. Rodrigues, Mirjam Salish, Nazarii Salish","Abstract":"Climate (change) affects the prices of goods and services in different countries or regions differently. Simply relying on aggregate measures or summary statistics, such as the impact of average country temperature changes on HICP headline inflation, conceals a large heterogeneity across (sub-)sectors of the economy. Additionally, the impact of a weather anomaly on consumer prices depends not only on its sign and magnitude, but also on its location and the size of the area affected by the shock. This is especially true for larger countries or regions with diverse climate zones, since the geographical distribution of climatic effects plays a role in shaping economic outcomes. Using time series data of geolocations, we demonstrate that relying solely on country averages fails to adequately capture and explain the influence of weather on consumer prices in the euro area. We conclude that the information content hidden in rich and complex surface data can provide valuable insights into the role of weather and climate variables for price stability, and more generally may help to inform economic policy."}
{"Title":"Approximating Smiles: A Time Change Approach","Link":"https:\/\/arxiv.org\/abs\/2401.03776","Authors":"Liexin Cheng, Xue Cheng","Abstract":"We present a new method for approximating the shape of implied volatility smiles. The method is applicable to common semimartingale models, such as jump-diffusion, rough volatility, and infinite activity models. We firstly adopt an Edgeworth expansion method to approximate the at-the-money skew and curvature and propose conditions under which the approximation errors converge. Then, we explicitly approximate the volatility skew and curvature under a time change framework using moment-based formula. Additionally, we derive the characteristics of volatility skew and curvature and explain their implications for model selection based on the approximation. The accuracy of the short-term approximation results on models is tested via numerical methods and on empirical data. The method is then applied to the calibration problem."}
{"Title":"Income inequality and the oil resource curse","Link":"https:\/\/arxiv.org\/abs\/2401.04046","Authors":"Osiris Jorge Parcero, Elissaios Papyrakis","Abstract":"Surprisingly, there has been little research conducted about the cross-country relationship between oil dependence\/abundance and income inequality. At the same time, there is some tentative evidence suggesting that oil rich nations tend to under-report data on income inequality, which can potentially influence the estimated empirical relationships between oil richness and income inequality. In this paper we contribute to the literature in a twofold manner. First, we explore in depth the empirical relationship between oil and income inequality by making use of the Standardized World Income Inequality Database; the most comprehensive dataset on income inequality providing comparable data for the broadest set of country-year observations. Second, this is the first study to our knowledge that adopts an empirical framework to examine whether oil rich nations tend to under-report data on income inequality and the possible implications thereof. We make use of Heckman selection models to validate the tendency of oil rich countries to under-report and correct for the bias that might arise as a result of this; we find that oil is associated with lower income inequality with the exception of the very oil-rich economies."}
{"Title":"Economic Forces in Stock Returns","Link":"https:\/\/arxiv.org\/abs\/2401.04132","Authors":"Yue Chen, Mohan Li","Abstract":"When analyzing the components influencing the stock prices, it is commonly believed that economic activities play an important role. More specifically, asset prices are more sensitive to the systematic economic news that impose a pervasive effect on the whole market. Moreover, the investors will not be rewarded for bearing idiosyncratic risks as such risks are diversifiable. In the paper Economic Forces and the Stock Market 1986, the authors introduced an attribution model to identify the specific systematic economic forces influencing the market. They first defined and examined five classic factors from previous research papers: Industrial Production, Unanticipated Inflation, Change in Expected Inflation, Risk Premia, and The Term Structure. By adding in new factors, the Market Indices, Consumptions and Oil Prices, one by one, they examined the significant contribution of each factor to the stock return. The paper concluded that the stock returns are exposed to the systematic economic news, and they are priced with respect to their risk exposure. Also, the significant factors can be identified by simply adopting their model. Driven by such motivation, we conduct an attribution analysis based on the general framework of their model to further prove the importance of the economic factors and identify the specific identity of significant factors."}
{"Title":"Becoming a Knowledge Economy: the Case of Qatar, UAE and 17 Benchmark Countries","Link":"https:\/\/arxiv.org\/abs\/2401.04214","Authors":"Osiris Parcero, James Christopher Ryan","Abstract":"This paper assesses the performance of Qatar and the United Arab Emirates (UAE) in terms of their achievements towards becoming knowledge-based economies. This is done through a comparison against 17 benchmark countries using a four pillars' framework comprising; (1) information and communication technology, (2) education, (3) innovation and (4) economy and regime. Results indicate that the UAE ranks slightly better than the median rank of the 19 compared countries while Qatar ranks somewhat below. Results also indicate that both countries lag considerably behind knowledge economy leaders; particularly evidenced in the innovation pillar. Policy recommendations are mainly addressed at further developing the two countries' research culture as well as improving the incentives to attract top quality researchers and highly talented workers."}
{"Title":"Optimal National policies towards multinationals when local regions can choose between firm-specific and non-firm-specific policies","Link":"https:\/\/arxiv.org\/abs\/2401.04243","Authors":"Osiris Jorge Parcero","Abstract":"This paper looks at a country's optimal central-government optimal policy in a setting where its two identical local jurisdictions compete to attract footloose multinationals to their sites, and where the considered multinationals strictly prefer this country to the rest of the world. For the sake of realism the model allows the local jurisdictions to choose between firm-specific and non-firm-specific policies. We show that the implementation of the jurisdictional firm-specific policy is weakly welfare dominant. Hence the frequent calls for the central government to ban the former type of policies go against the advice of this paper."}
{"Title":"Entrepreneurial Intention and UAE Youth: Unique Influencers of Entrepreneurial Intentions in an Emerging Country Context","Link":"https:\/\/arxiv.org\/abs\/2401.04253","Authors":"N. Y. Al Saiqal, James. C. Ryabn, Osiris Jorge Parcero","Abstract":"The United Arab Emirates (UAE) is a young, oil-rich country, where national youth display a clear preference for public sector employment. Growing youth unemployment reinforces the importance of non-government employment, including entrepreneurship. This study investigates UAE national youth intentions towards entrepreneurship through the Theory of Planned Behavior and the Entrepreneurial Intention Questionnaire (EIQ). Analysis (N=544) identifies the direct influence of attitude and perceived behavioral control, and indirect influence of subjective norms on entrepreneurship intention. Results also examine several demographic variables and highlight the potential importance of family and social groups in promoting entrepreneurial intentions in this emerging country context."}
{"Title":"Expiring Assets in Automated Market Makers","Link":"https:\/\/arxiv.org\/abs\/2401.04289","Authors":"Kenan Wood, Maurice Herlihy, Hammurabi Mendes, Jonad Pulaj","Abstract":"An automated market maker (AMM) is a state machine that manages pools of assets, allowing parties to buy and sell those assets according to a fixed mathematical formula. AMMs are typically implemented as smart contracts on blockchains, and its prices are kept in line with the overall market price by arbitrage: if the AMM undervalues an asset with respect to the market, an \"arbitrageur\" can make a risk-free profit by buying just enough of that asset to bring the AMM's price back in line with the market.\nAMMs, however, are not designed for assets that expire: that is, assets that cannot be produced or resold after a specified date. As assets approach expiration, arbitrage may not be able to reconcile supply and demand, and the liquidity providers that funded the AMM may have excessive exposure to risk due to rapid price variations.\nThis paper formally describes the design of a decentralized exchange (DEX) for assets that expire, combining aspects of AMMs and limit-order books. We ensure liveness and market clearance, providing mechanisms for liquidity providers to control their exposure to risk and adjust prices dynamically in response to situations where arbitrage may fail."}
{"Title":"Proof of Efficient Liquidity: A Staking Mechanism for Capital Efficient Liquidity","Link":"https:\/\/arxiv.org\/abs\/2401.04521","Authors":"Arman Abgaryan, Utkarsh Sharma, Joshua Tobkin","Abstract":"The Proof of Efficient Liquidity (PoEL) protocol, designed for specialised Proof of Stake (PoS) consensus-based blockchains that incorporate intrinsic DeFi applications, aims to support sustainable liquidity bootstrapping and network security. This concept seeks to efficiently utilise budgeted staking rewards to attract and sustain liquidity through a risk-structuring engine and incentive allocation strategy, both of which are designed to maximise capital efficiency. The proposed protocol serves the dual objective of: (i) capital creation by attracting risk capital efficiently and maximising its operational utility for intrinsic DeFi applications, thereby asserting sustainability; and (ii) enhancing the adopting blockchain network's economic security by augmenting their staking (PoS) mechanism with a harmonious layer seeking to attract a diversity of digital assets. Finally, the protocol's conceptual framework, as detailed in the appendix, is extended to encompass service fee credits. This extension capitalises on the network's auxiliary services to disperse incentives and attract liquidity, ensuring the network achieves and maintains the critical usage threshold essential for its sustained operational viability and progressive growth."}
{"Title":"Ecosystem orchestration practices for industrial firms: A qualitative meta-analysis, framework development and research agenda","Link":"https:\/\/arxiv.org\/abs\/2401.04526","Authors":"Lei Shen, Qingyue Shi, Vinit Parida, Marin Jovanovic","Abstract":"This study ventures into the dynamic realm of ecosystem orchestration for industrial firms, emphasizing its significance in maintaining competitive advantage in the digital era. The fragmented research on this important subject poses challenges for firms aiming to navigate and capitalize on ecosystem orchestration. To bridge this knowledge gap, we conducted a comprehensive qualitative meta-analysis of 31 case studies and identified multifaceted orchestration practices employed by industrial firms. The core contribution of this research is the illumination of five interdependent but interrelated orchestration practices: strategic design, relational, resource integration, technological, and innovation. Together, these practices are synthesized into an integrative framework termed the \"Stirring Model,\" which serves as a practical guide to the orchestration practices. Furthermore, the conceptual framework clarifies the synergy between the identified practices and highlights their collective impact. This study proposes theoretical and practical implications for ecosystem orchestration literature and suggests avenues for further research."}
{"Title":"Opportunities to upgrade the scientific disciplines space","Link":"https:\/\/arxiv.org\/abs\/2401.04573","Authors":"Nestor Gandelman, Osiris Jorge Parcero, Flavia Roldan","Abstract":"Knowledge generated in a given scientific domain may spill over into other close scientific disciplines, thereby improving performance. Using bibliometric data from the SCImago database drawn from a sample of 174 countries, we implement a measure of proximity based on revealed comparative advantage (RCA). Our estimates show that proximity between disciplines positively and significantly affects the RCA growth rate. This impact is larger on disciplines that currently do not have RCA."}
{"Title":"Effect on New Loan Repayment Fine Clause on Bank Jaya Artha's Customer Satisfaction and Recommendation","Link":"https:\/\/arxiv.org\/abs\/2401.04605","Authors":"Mustaqim Adamrah, Yos Sunitiyoso","Abstract":"The growing population of older people in Indonesia--the world's fourth-most populous country--makes a larger cake for the pension business, including in the banking sector. PT Bank Jaya Artha is one of the Indonesian banks that provide products and services for people who are set to enter retirement age. In the wake of tight competition in the pension business market, Bank Jaya Artha has since 2019 imposed a fine of three times of installments in addition to 5% of outstanding debt on customers planning to repay their debt in a mission to prevent them from leaving for competitors. While the clause is not included in loan agreements signed before the implementation, it applies to past loan agreements as well. This, in turn, has led to customer complaints. The research is meant to find out how the implementation of the unconsented clause has affected customer satisfaction and willingness to recommend the bank and what the bank should do to become more customer-centric, according to customers. Using a design thinking framework, the research collects quantitative and qualitative data from the bank's pension customers through questionnaires and forum group discussions. Statistical analysis is utilized on quantitative data from questionnaires, and content analysis is utilized on qualitative data from questionnaires. A narrative analysis is also used to explain qualitative data from forum group discussions. The result shows that there are problems in the way the bank communicates information to customers, particularly information about the loan repayment fine. Lack of transparency, a reactive approach instead of a proactive one, the obscurity of the information, and the time the information is delivered have affected customers' satisfaction toward the bank."}
{"Title":"World Wine Exports: What Determines the Success of New World Wine Producers?","Link":"https:\/\/arxiv.org\/abs\/2401.04696","Authors":"Osiris Jorge Parcero, Emiliano Villanueva","Abstract":"By using an econometric approach this paper looks at the evolution of the world wine industry in the period 1961-2005. A particular stylized fact is the appearance of nontraditional producing and exporting countries of wine from the beginning of the nineties. We show that the success of these new producing and exporting countries can be explained by the importance of the demand from non-producing countries with little or no tradition of wine consumption, relative to the world demand. This stylized fact is consistent with a testable implication of the switching cost literature and to the best of our knowledge this is the first time that this implication is tested."}
{"Title":"Scaling Laws And Statistical Properties of The Transaction Flows And Holding Times of Bitcoin","Link":"https:\/\/arxiv.org\/abs\/2401.04702","Authors":"Didier Sornette, Yu Zhang","Abstract":"We study the temporal evolution of the holding-time distribution of bitcoins and find that the average distribution of holding-time is a heavy-tailed power law extending from one day to over at least 200 weeks with an exponent approximately equal to 0.9, indicating very long memory effects. We also report significant sample-to-sample variations of the distribution of holding times, which can be best characterized as multiscaling, with power-law exponents varying between 0.3 and 2.5 depending on bitcoin price regimes. We document significant differences between the distributions of book-to-market and of realized returns, showing that traders obtain far from optimal performance. We also report strong direct qualitative and quantitative evidence of the disposition effect in the Bitcoin Blockchain data. Defining age-dependent transaction flows as the fraction of bitcoins that are traded at a given time and that were born (last traded) at some specific earlier time, we document that the time-averaged transaction flow fraction has a power law dependence as a function of age, with an exponent close to -1.5, a value compatible with priority queuing theory. We document the existence of multifractality on the measure defined as the normalized number of bitcoins exchanged at a given time."}
{"Title":"Revealed comparative advantages in scientific and technological disciplines in Uruguay","Link":"https:\/\/arxiv.org\/abs\/2401.04752","Authors":"Nestor Gandelman, Osiris Jorge Parcero, Matilde Pereira, Flavia Roldan","Abstract":"Based on bibliometric information from Scopus for the period 1996-2019, this document characterizes the evolution of Uruguayan scientific production and establishes the areas in which the country has a revealed comparative advantage (RCA). Methodologically, it is proposed that there is a RCA in an area if this area has a greater share in national scientific production than the share of the area in world scientific production. The evidence presented considers two measurements of scientific production (published articles and citations) and three levels of aggregation in the areas (a minor one with 5 large areas, a more detailed one with 27 disciplines and another even more granular with more than 300 disaggregations). Within Health Sciences there is a RCA in Veterinary, Nursing and Medicine. Within Life Sciences there is a RCA in Agricultural and Biological Sciences, Immunology and Microbiology and Biochemistry, Genetics and Molecular Biology. In Physical Sciences there is only a RCA in Environmental Science and in Social Sciences only in Economics, Econometrics and Finance."}
{"Title":"Markowitz Portfolio Construction at Seventy","Link":"https:\/\/arxiv.org\/abs\/2401.05080","Authors":"Stephen Boyd, Kasper Johansson, Ronald Kahn, Philipp Schiele, Thomas Schmelzer","Abstract":"More than seventy years ago Harry Markowitz formulated portfolio construction as an optimization problem that trades off expected return and risk, defined as the standard deviation of the portfolio returns. Since then the method has been extended to include many practical constraints and objective terms, such as transaction cost or leverage limits. Despite several criticisms of Markowitz's method, for example its sensitivity to poor forecasts of the return statistics, it has become the dominant quantitative method for portfolio construction in practice. In this article we describe an extension of Markowitz's method that addresses many practical effects and gracefully handles the uncertainty inherent in return statistics forecasting. Like Markowitz's original formulation, the extension is also a convex optimization problem, which can be solved with high reliability and speed."}
{"Title":"Behind the Eastern-Western European convergence path: the role of geography and trade liberalization","Link":"https:\/\/arxiv.org\/abs\/2401.05107","Authors":"Adolfo Cristobal Campoamor, Osiris Jorge Parcero","Abstract":"This paper proposes a two blocks and three regions economic geography model that can account for the most salient stylized facts experienced by Eastern European transition economies during the period 1990 2005. In contrast to the existing literature, which has favored technological explanations, trade liberalization is the only driving force. The model correctly predicts that in the first half of the period, trade liberalization led to divergence in GDP per capita, both between the West and the East and within the East. Consistent with the data, in the second half of the period, this process was reversed and convergence became the dominant force."}
{"Title":"Is there a size premium for nations?","Link":"https:\/\/arxiv.org\/abs\/2401.05116","Authors":"Jo\u017ee P. Damijan, Sandra Damijan, Osiris Jorge Parcero","Abstract":"This paper examines whether there is a premium in country size. We study whether there are significant gains from being a small or a large country in terms of certain socioeconomic indicators and how large this premium is. Using panel data for 200 countries over 50 years, we estimate premia for various sizes of nations across a variety of key economic and socioeconomic performance indicators. We find that smaller countries are richer, have larger governments, and are more prudent in terms of fiscal policies than larger ones. On the other hand, smaller countries seem to be subject to higher absolute and per capita costs for the provision of essential public goods, which may lower their socioeconomic performance in terms of health and education. In terms of economic performance, small countries seem to do better than large countries, compensating for smallness by relying on foreign trade and foreign direct investment. The latter comes at the cost of higher vulnerability to external shocks, resulting in higher volatility of growth rates. This paper's findings offer essential guidance to policymakers, international organizations, and business researchers, especially those assessing a country's economic or socioeconomic performance or potential. The study implies that comparisons with medium-sized or large countries may be of little utility in predicting the performance of small countries."}
{"Title":"Can unions impose costs on employers in education strikes? Evidence from pension disputes in UK universities","Link":"https:\/\/arxiv.org\/abs\/2401.05183","Authors":"Nils Braakmann, Barbara Eberth","Abstract":"The impact of strikes in educational institutions, specifically universities, on employers remains understudied. This paper investigates the impact of education strikes in UK universities from 2018 to 2022, primarily due to pension disputes. Using data from the Guardian University Guide and the 2014 and 2021 Research Excellence Frameworks and leveraging difference-in-differences and regression discontinuity approaches, our findings suggest significant declines in several student related outcomes, such as student satisfaction, and a more mixed picture for student attainment and research performance. These results highlight the substantial, albeit indirect, cost unions can impose on university employers during strikes."}
{"Title":"Tournaments, Contestant Heterogeneity and Performance","Link":"https:\/\/arxiv.org\/abs\/2401.05210","Authors":"Enzo Brox, Daniel Goller","Abstract":"Tournaments are frequently used incentive mechanisms to enhance performance. In this paper, we use field data and show that skill disparities among contestants asymmetrically affect the performance of contestants. Skill disparities have detrimental effects on the performance of the lower-ability contestant but positive effects on the performance of the higher-ability contestant. We discuss the potential of different behavioral approaches to explain our findings and discuss the implications of our results for the optimal design of contests. Beyond that, our study reveals two important empirical results: (a) affirmative action-type policies may help to mitigate the adverse effects on lower-ability contestants, and (b) the skill level of potential future contestants in subsequent tournament stages can detrimentally influence the performance of higher-ability contestants but does not affect the lower-ability contestant."}
{"Title":"A Mean Field Game between Informed Traders and a Broker","Link":"https:\/\/arxiv.org\/abs\/2401.05257","Authors":"Philippe Bergault, Leandro S\u00e1nchez-Betancourt","Abstract":"We find closed-form solutions to the stochastic game between a broker and a mean-field of informed traders. In the finite player game, the informed traders observe a common signal and a private signal. The broker, on the other hand, observes the trading speed of each of his clients and provides liquidity to the informed traders. Each player in the game optimises wealth adjusted by inventory penalties. In the mean field version of the game, using a G\u00e2teaux derivative approach, we characterise the solution to the game with a system of forward-backward stochastic differential equations that we solve explicitly. We find that the optimal trading strategy of the broker is linear on his own inventory, on the average inventory among informed traders, and on the common signal or the average trading speed of the informed traders. The Nash equilibrium we find helps informed traders decide how to use private information, and helps brokers decide how much of the order flow they should externalise or internalise when facing a large number of clients."}
{"Title":"Comparison of Markowitz Model and Single-Index Model on Portfolio Selection of Malaysian Stocks","Link":"https:\/\/arxiv.org\/abs\/2401.05264","Authors":"Zhang Chern Lee, Wei Yun Tan, Hoong Khen Koo, Wilson Pang","Abstract":"Our article is focused on the application of Markowitz Portfolio Theory and the Single Index Model on 10-year historical monthly return data for 10 stocks included in FTSE Bursa Malaysia KLCI, which is also our market index, as well as a risk-free asset which is the monthly fixed deposit rate. We will calculate the minimum variance portfolio and maximum Sharpe portfolio for both the Markowitz model and Single Index model subject to five different constraints, with the results presented in the form of tables and graphs such that comparisons between the different models and constraints can be made. We hope this article will help provide useful information for future investors who are interested in the Malaysian stock market and would like to construct an efficient investment portfolio. Keywords: Markowitz Portfolio Theory, Single Index Model, FTSE Bursa Malaysia KLCI, Efficient Portfolio"}
{"Title":"Optimal Linear Signal: An Unsupervised Machine Learning Framework to Optimize PnL with Linear Signals","Link":"https:\/\/arxiv.org\/abs\/2401.05337","Authors":"Pierre Renucci","Abstract":"This study presents an unsupervised machine learning approach for optimizing Profit and Loss (PnL) in quantitative finance. Our algorithm, akin to an unsupervised variant of linear regression, maximizes the Sharpe Ratio of PnL generated from signals constructed linearly from exogenous variables. The methodology employs a linear relationship between exogenous variables and the trading signal, with the objective of maximizing the Sharpe Ratio through parameter optimization. Empirical application on an ETF representing U.S. Treasury bonds demonstrates the model's effectiveness, supported by regularization techniques to mitigate overfitting. The study concludes with potential avenues for further development, including generalized time steps and enhanced corrective terms."}
{"Title":"Income and emotional well-being: Evidence for well-being plateauing around $200,000 per year","Link":"https:\/\/arxiv.org\/abs\/2401.05347","Authors":"Mikkel Bennedsen","Abstract":"Is emotional well-being monotonically increasing in the level of income or does it reach a plateau at some income threshold, whereafter additional income does not contribute to further well-being? Conflicting answers to this question has been suggested in the academic literature. In a recent paper, using an income threshold of 100,000 per year, Killingsworth et al. (2023) appears to have resolved these conflicts, concluding that emotional well-being is monotonically increasing in income for all but the unhappiest individuals. In this paper, we show that this conclusion is sensitive to the placement of the income threshold at which the relationship between emotional well-being and income is allowed to plateau. Using standard econometric methods, we propose a data-driven approach to detect the placement of the threshold. Using this data-driven income threshold, a flat relationship between household income and emotional well-being above a threshold around 200,000 per year is found. While our analysis relaxes the assumption of a pre-specified income threshold, it relies on a number of other assumptions, which we briefly discuss. We conclude that although the analysis of this paper provides some evidence for well-being plateauing around $200,000 per year, more research is needed before any definite conclusions about the relationship between emotional well-being and income can be drawn."}
{"Title":"RIVCoin: an alternative, integrated, CeFi\/DeFi-Vaulted Cryptocurrency","Link":"https:\/\/arxiv.org\/abs\/2401.05393","Authors":"Roberto Rivera, Guido Rocco, Massimiliano Marzo, Enrico Talin","Abstract":"This whitepaper introduces RIVCoin, a cryptocurrency built on Cosmos, fully stabilized by a diversified portfolio of both CeFi and DeFi assets, available in a digital, non-custodial wallet called RIV Wallet, that aims to provide Users an easy way to access the cryptocurrency markets, compliant to the strictest AML laws and regulations up to date. The token is a cryptocurrency at any time stabilized by a basket of assets: reserves are invested in a portfolio composed long term by 50% of CeFi assets, comprised of Fixed Income, Equity, Mutual and Hedge Funds and 50% of diversified strategies focused on digital assets, mainly staking and LP farming on the major, battle tested DeFi protocols. The cryptocurrency, as well as the dollar before Bretton Woods, is always fully stabilized by vaulted proof of assets: it is born and managed as a decentralized token, minted by a Decentralized Autonomous Organization, and entirely stabilized by assets evaluated by professional independent third parties. Users will trade, pool, and exchange the token without any intermediary, being able to merge them into a Liquidity Pool whose rewards will be composed by both the trading fees and the liquidity rewards derived from the reserve's seigniorage.\nUsers who wish and decide to pool RIVCoin in the Liquidity Pool will receive additional RIVCoin for themselves, and new RIVCoin are minted when the reserves increase in value or in case of purchase of new RIVCoin. The proposed model allows for alignment of incentives: decreasing the risk exposure by wealthier Users, but implicitly increasing that of smaller ones to a level perceived by them as still sustainable. Users indirectly benefit from the access to the rewards of sophisticated cryptocurrency portfolios hitherto precluded to them, without this turning into a disadvantage for the wealthy User."}
{"Title":"SRNI-CAR: A comprehensive dataset for analyzing the Chinese automotive market","Link":"https:\/\/arxiv.org\/abs\/2401.05395","Authors":"Ruixin Ding, Bowei Chen, James M. Wilson, Zhi Yan, Yufei Huang","Abstract":"The automotive industry plays a critical role in the global economy, and particularly important is the expanding Chinese automobile market due to its immense scale and influence. However, existing automotive sector datasets are limited in their coverage, failing to adequately consider the growing demand for more and diverse variables. This paper aims to bridge this data gap by introducing a comprehensive dataset spanning the years from 2016 to 2022, encompassing sales data, online reviews, and a wealth of information related to the Chinese automotive industry. This dataset serves as a valuable resource, significantly expanding the available data. Its impact extends to various dimensions, including improving forecasting accuracy, expanding the scope of business applications, informing policy development and regulation, and advancing academic research within the automotive sector. To illustrate the dataset's potential applications in both business and academic contexts, we present two application examples. Our developed dataset enhances our understanding of the Chinese automotive market and offers a valuable tool for researchers, policymakers, and industry stakeholders worldwide."}
{"Title":"On the Three Demons in Causality in Finance: Time Resolution, Nonstationarity, and Latent Factors","Link":"https:\/\/arxiv.org\/abs\/2401.05414","Authors":"Xinshuai Dong, Haoyue Dai, Yewen Fan, Songyao Jin, Sathyamoorthy Rajendran, Kun Zhang","Abstract":"Financial data is generally time series in essence and thus suffers from three fundamental issues: the mismatch in time resolution, the time-varying property of the distribution - nonstationarity, and causal factors that are important but unknown\/unobserved. In this paper, we follow a causal perspective to systematically look into these three demons in finance. Specifically, we reexamine these issues in the context of causality, which gives rise to a novel and inspiring understanding of how the issues can be addressed. Following this perspective, we provide systematic solutions to these problems, which hopefully would serve as a foundation for future research in the area."}
{"Title":"Multiple-bubble testing in the cryptocurrency market: a case study of bitcoin","Link":"https:\/\/arxiv.org\/abs\/2401.05417","Authors":"Sanaz Behzadi, Mahmonir Bayanati, Hamed Nozari","Abstract":"Economic periods and financial crises have highlighted the importance of evaluating financial markets to investors and researchers in recent decades."}
{"Title":"Introduction of L0 norm and application of L1 and C1 norm in the study of time-series","Link":"https:\/\/arxiv.org\/abs\/2401.05423","Authors":"Victor Ujaldon Garcia","Abstract":"Four markets are considered: Cryptocurrencies \/ South American exchange rate \/ Spanish Banking indices and European Indices and studied using TDA (Topological Data Analysis) tools. These tools are used to predict and showcase both strengths and weakness of the current TDA tools. In this paper a new tool L0 norm is defined and complemented with the already existing C1 norm."}
{"Title":"Multi-relational Graph Diffusion Neural Network with Parallel Retention for Stock Trends Classification","Link":"https:\/\/arxiv.org\/abs\/2401.05430","Authors":"Zinuo You, Pengju Zhang, Jin Zheng, John Cartlidge","Abstract":"Stock trend classification remains a fundamental yet challenging task, owing to the intricate time-evolving dynamics between and within stocks. To tackle these two challenges, we propose a graph-based representation learning approach aimed at predicting the future movements of multiple stocks. Initially, we model the complex time-varying relationships between stocks by generating dynamic multi-relational stock graphs. This is achieved through a novel edge generation algorithm that leverages information entropy and signal energy to quantify the intensity and directionality of inter-stock relations on each trading day. Then, we further refine these initial graphs through a stochastic multi-relational diffusion process, adaptively learning task-optimal edges. Subsequently, we implement a decoupled representation learning scheme with parallel retention to obtain the final graph representation. This strategy better captures the unique temporal features within individual stocks while also capturing the overall structure of the stock graph. Comprehensive experiments conducted on real-world datasets from two US markets (NASDAQ and NYSE) and one Chinese market (Shanghai Stock Exchange: SSE) validate the effectiveness of our method. Our approach consistently outperforms state-of-the-art baselines in forecasting next trading day stock trends across three test periods spanning seven years. Datasets and code have been released (this https URL)."}
{"Title":"An adaptive network-based approach for advanced forecasting of cryptocurrency values","Link":"https:\/\/arxiv.org\/abs\/2401.05441","Authors":"Ali Mehrban, Pegah Ahadian","Abstract":"This paper describes an architecture for predicting the price of cryptocurrencies for the next seven days using the Adaptive Network Based Fuzzy Inference System (ANFIS). Historical data of cryptocurrencies and indexes that are considered are Bitcoin (BTC), Ethereum (ETH), Bitcoin Dominance (BTC.D), and Ethereum Dominance (ETH.D) in a daily timeframe. The methods used to teach the data are hybrid and backpropagation algorithms, as well as grid partition, subtractive clustering, and Fuzzy C-means clustering (FCM) algorithms, which are used in data clustering. The architectural performance designed in this paper has been compared with different inputs and neural network models in terms of statistical evaluation criteria. Finally, the proposed method can predict the price of digital currencies in a short time."}
{"Title":"Can ChatGPT Compute Trustworthy Sentiment Scores from Bloomberg Market Wraps?","Link":"https:\/\/arxiv.org\/abs\/2401.05447","Authors":"Baptiste Lefort, Eric Benhamou, Jean-Jacques Ohana, David Saltiel, Beatrice Guez, Damien Challet","Abstract":"We used a dataset of daily Bloomberg Financial Market Summaries from 2010 to 2023, reposted on large financial media, to determine how global news headlines may affect stock market movements using ChatGPT and a two-stage prompt approach. We document a statistically significant positive correlation between the sentiment score and future equity market returns over short to medium term, which reverts to a negative correlation over longer horizons. Validation of this correlation pattern across multiple equity markets indicates its robustness across equity regions and resilience to non-linearity, evidenced by comparison of Pearson and Spearman correlations. Finally, we provide an estimate of the optimal horizon that strikes a balance between reactivity to new information and correlation."}
{"Title":"Boundary conditions at infinity for Black-Scholes equations","Link":"https:\/\/arxiv.org\/abs\/2401.05549","Authors":"Yukihiro Tsuzuki","Abstract":"We propose a numerical procedure for computing the prices of forward contracts, in which the underlying asset price is a Markovian local martingale. If the underlying process is a strict local martingale, multiple solutions exist for the corresponding Black-Scholes equations, and the derivative prices are characterized as the minimal solutions. When applying numerical methods that are set up on a finite grid, such as finite difference methods, additional spatial boundary conditions are required, which causes the solution to uniquely exist. In this case, the problem is reduced to specifying the boundary conditions. Our boundary condition is based on an expression of the derivative price at infinity in terms of those at finite values, and incorporates volatility behaviors beyond the boundary. The proposed procedure is demonstrated through numerical tests, which show that it outperforms the methods proposed in the literature."}
{"Title":"Super-hedging-pricing formulas and Immediate-Profit arbitrage for market models under random horizon","Link":"https:\/\/arxiv.org\/abs\/2401.05713","Authors":"Tahir Choulli, Emmanuel Lepinette","Abstract":"In this paper, we consider the discrete-time setting, and the market model described by (S,F,T)$. Herein F is the ``public\" flow of information which is available to all agents overtime, S is the discounted price process of d-tradable assets, and T is an arbitrary random time whose occurrence might not be observable via F. Thus, we consider the larger flow G which incorporates F and makes T an observable random time. This framework covers the credit risk theory setting, the life insurance setting and the setting of employee stock option valuation. For the stopped model (S^T,G) and for various vulnerable claims, based on this model, we address the super-hedging pricing valuation problem and its intrinsic Immediate-Profit arbitrage (IP hereafter for short). Our first main contribution lies in singling out the impact of change of prior and\/or information on conditional essential supremum, which is a vital tool in super-hedging pricing. The second main contribution consists of describing as explicit as possible how the set of super-hedging prices expands under the stochasticity of T and its risks, and we address the IP arbitrage for (S^T,G) as well. The third main contribution resides in elaborating as explicit as possible pricing formulas for vulnerable claims, and singling out the various informational risks in the prices' dynamics."}
{"Title":"Follow The Money: Exploring the Key Factors Influencing Investment in African Startups","Link":"https:\/\/arxiv.org\/abs\/2401.05760","Authors":"Khalil Liouane","Abstract":"The African continent has witnessed a notable surge in entrepreneurial activity, with the number of startups and investments made in the ecosystem growing significantly in recent years. Against this backdrop, this paper presents an in-depth analysis of the critical key factors influencing funding amounts in African startup deals. A comprehensive analysis of 2,521 startup investment deals, spanning from January 2019 to March 2023, was conducted using a combination of statistical and several machine learning techniques. The results of this study highlight a significant gender diversity gap, the importance of professional experience, and the impact of founders' academic backgrounds. The study reveals that human capital, a diversified sector approach, and cross-border collaboration strategies are crucial for a robust startup ecosystem. Additionally, we identified the potential positive impact of 'Y combinators' for African startups, the implications of exit strategies on deal amounts, and the heterogeneity as well as the incongruity of investment rounds across the continent. In light of these findings, we propose an assortment of policy recommendations aimed at fostering a propitious milieu for African entrepreneurial ventures, promoting equitable investment distribution, and enhancing cross-border collaboration. By providing a rigorous empirical analysis, this study not only contributes to the existing body of literature but also lays the foundation for future research aimed at promoting investment and catalyzing socio-economic development throughout the African continent."}
{"Title":"Quantum Probability Theoretic Asset Return Modeling: A Novel Schr\\\"odinger-Like Trading Equation and Multimodal Distribution","Link":"https:\/\/arxiv.org\/abs\/2401.05823","Authors":"Li Lin","Abstract":"Quantum theory provides a comprehensive framework for quantifying uncertainty, often applied in quantum finance to explore the stochastic nature of asset returns. This perspective likens returns to microscopic particle motion, governed by quantum probabilities akin to physical laws. However, such approaches presuppose specific microscopic quantum effects in return changes, a premise criticized for lack of guarantee. This paper diverges by asserting that quantum probability is a mathematical extension of classical probability to complex numbers. It isn't exclusively tied to microscopic quantum phenomena, bypassing the need for quantum effects in this http URL directly linking quantum probability's mathematical structure to traders' decisions and market behaviors, it avoids assuming quantum effects for returns and invoking the wave function. The complex phase of quantum probability, capturing transitions between long and short decisions while considering information interaction among traders, offers an inherent advantage over classical probability in characterizing the multimodal distribution of asset this http URL Fourier decomposition, we derive a Schr\u00f6dinger-like trading equation, where each term explicitly corresponds to implications of market trading. The equation indicates discrete energy levels in financial trading, with returns following a normal distribution at the lowest level. As the market transitions to higher trading levels, a phase shift occurs in the return distribution, leading to multimodality and fat tails. Empirical research on the Chinese stock market supports the existence of energy levels and multimodal distributions derived from this quantum probability asset returns model."}
{"Title":"Interactions between dynamic team composition and coordination: An agent-based modeling approach","Link":"https:\/\/arxiv.org\/abs\/2401.05832","Authors":"Dar\u00edo Blanco-Fern\u00e1ndez, Stephan Leitner, Alexandra Rausch","Abstract":"This paper examines the interactions between selected coordination modes and dynamic team composition, and their joint effects on task performance under different task complexity and individual learning conditions. Prior research often treats dynamic team composition as a consequence of suboptimal organizational design choices. The emergence of new organizational forms that consciously employ teams that change their composition periodically challenges this perspective. In this paper, we follow the contingency theory and characterize dynamic team composition as a design choice that interacts with other choices such as the coordination mode, and with additional contextual factors such as individual learning and task complexity. We employ an agent-based modeling approach based on the NK framework, which includes a reinforcement learning mechanism, a recurring team formation mechanism based on signaling, and three different coordination modes. Our results suggest that by implementing lateral communication or sequential decision-making, teams may exploit the benefits of dynamic composition more than if decision-making is fully autonomous. The choice of a proper coordination mode, however, is partly moderated by the task complexity and individual learning. Additionally, we show that only a coordination mode based on lateral communication may prevent the negative effects of individual learning."}
{"Title":"Synergy or Rivalry? Glimpses of Regional Modernization and Public Service Equalization: A Case Study from China","Link":"https:\/\/arxiv.org\/abs\/2401.06134","Authors":"Shengwen Shi (1), Jian'an Zhang (2) ((1) School of Economics, Shanghai University of Finance and Economics, Shanghai, China, (2) School of Economics, Shanghai University of Finance and Economics, Shanghai, China)","Abstract":"For most developing countries, increasing the equalization of basic public services is widely recognized as an effective channel to improve people's sense of contentment. However, for many emerging economies like China, the equalization level of basic public services may often be neglected in the trade-off between the speed and quality of development. Taking the Yangtze River Delta region of China as an example, this paper first adopts the coupling coordination degree model to explore current status of basic public services in this region, and then uses Moran's I index to study the overall equalization level of development there. Moreover, this paper uses the Theil index to analyze the main reasons for the spatial differences in the level of public services, followed by the AF method to accurately identify the exact weaknesses of the 40 counties of 10 cities with the weakest level of basic public service development. Based on this, this paper provides targeted optimization initiatives and continues to explore the factors affecting the growth of the level of public service equalization through the convergence model, verifying the convergence trend of the degree of public service equalization, and ultimately providing practical policy recommendations for promoting the equalization of basic public services."}
{"Title":"Stockformer: A Price-Volume Factor Stock Selection Model Based on Wavelet Transform and Multi-Task Self-Attention Networks","Link":"https:\/\/arxiv.org\/abs\/2401.06139","Authors":"Bohan Ma, Yushan Xue, Yuan Lu, Jing Chen","Abstract":"As the Chinese stock market continues to evolve and its market structure grows increasingly complex, traditional quantitative trading methods are facing escalating challenges. Particularly, due to policy uncertainty and the frequent market fluctuations triggered by sudden economic events, existing models often struggle to accurately predict market dynamics. To address these challenges, this paper introduces Stockformer, a price-volume factor stock selection model that integrates wavelet transformation and a multitask self-attention network, aimed at enhancing responsiveness and predictive accuracy regarding market instabilities. Through discrete wavelet transform, Stockformer decomposes stock returns into high and low frequencies, meticulously capturing long-term market trends and short-term fluctuations, including abrupt events. Moreover, the model incorporates a Dual-Frequency Spatiotemporal Encoder and graph embedding techniques to effectively capture complex temporal and spatial relationships among stocks. Employing a multitask learning strategy, it simultaneously predicts stock returns and directional trends. Experimental results show that Stockformer outperforms existing advanced methods on multiple real stock market datasets. In strategy backtesting, Stockformer consistently demonstrates exceptional stability and reliability across market conditions-whether rising, falling, or fluctuating-particularly maintaining high performance during downturns or volatile periods, indicating a high adaptability to market fluctuations. To foster innovation and collaboration in the financial analysis sector, the Stockformer model's code has been open-sourced and is available on the GitHub repository: this https URL."}
{"Title":"The Role of Direct Capital Cash Transfers Towards Poverty and Extreme Poverty Alleviation -- An Omega Risk Process","Link":"https:\/\/arxiv.org\/abs\/2401.06141","Authors":"Jos\u00e9 Miguel Flores-Contr\u00f3, S\u00e9verine Arnold","Abstract":"Trapping refers to the event when a household falls into the area of poverty. Households that live or fall into the area of poverty are said to be in a poverty trap, where a poverty trap is a state of poverty from which it is difficult to escape without external help. Similarly, extreme poverty is considered as the most severe type of poverty, in which households experience severe deprivation of basic human needs. In this article, we consider an Omega risk process with deterministic growth and a multiplicative jump (collapse) structure to model the capital of a household. It is assumed that, when a household's capital level is above a certain capital barrier level that determines a household's eligibility for a capital cash transfer programme, its capital grows exponentially. As soon as its capital falls below the capital barrier level, the capital dynamics incorporate external support in the form of direct transfers (capital cash transfers) provided by donors or governments. Otherwise, once trapped, the capital grows only due to the capital cash transfers. Under this model, we first derive closed-form expressions for the trapping probability and then do the same for the probability of extreme poverty, which only depends on the current value of the capital given by some extreme poverty rate function. Numerical examples illustrate the role of capital cash transfers on poverty and extreme poverty dynamics."}
{"Title":"A Statistical Field Perspective on Capital Allocation and Accumulation: Individual dynamics","Link":"https:\/\/arxiv.org\/abs\/2401.06142","Authors":"Pierre Gosselin (IF), A\u00efleen Lotz","Abstract":"We have shown, in a series of articles, that a classical description of a large number of economic agents can be replaced by a statistical fields formalism. To better understand the accumulation and allocation of capital among different sectors, the present paper applies this statistical fields description to a large number of heterogeneous agents divided into two groups. The first group is composed of a large number of firms in different sectors that collectively own the entire physical capital. The second group, investors, holds the entire financial capital and allocates it between firms across sectors according to investment preferences, expected returns, and stock prices variations on financial markets. In return, firms pay dividends to their investors. Financial capital is thus a function of dividends and stock valuations, whereas physical capital is a function of the total capital allocated by the financial sector. Whereas our previous work focused on the background fields that describe potential long-term equilibria, here we compute the transition functions of individual agents and study their probabilistic dynamics in the background field, as a function of their initial state. We show that capital accumulation depends on various factors. The probability associated with each firm's trajectories is the result of several contradictory effects: the firm tends to shift towards sectors with the greatest long-term return, but must take into account the impact of its shift on its attractiveness for investors throughout its trajectory. Since this trajectory depends largely on the average capital of transition sectors, a firm's attractiveness during its relocation depends on the relative level of capital in those sectors. Thus, an under-capitalized firm reaching a high-capital sector will experience a loss of attractiveness, and subsequently, in investors. Moreover, the firm must also consider the effects of competition in the intermediate sectors. An under-capitalized firm will tend to be ousted out towards sectors with lower average capital, while an over-capitalized firm will tend to shift towards higher averagecapital sectors. For investors, capital allocation depends on their short and long-term returns. These returns are not independent: in the short-term, returns are composed of both the firm's dividends and the increase in its stock prices. In the long-term, returns are based on the firm's growth expectations, but also, indirectly, on expectations of higher stock prices. Investors' capital allocation directly depends on the volatility of stock prices and {\\ldots}rms'dividends. Investors will tend to reallocate their capital to maximize their short and long-term returns. The higher their level of capital, the stronger the reallocation will be."}
{"Title":"Grassroots Innovation Actors: Their Role and Positioning in Economic Ecosystems -- A Comparative Study Through Complex Network Analysis","Link":"https:\/\/arxiv.org\/abs\/2401.06163","Authors":"Marcelo S. Tedesco, Francisco Javier Ramos Soria","Abstract":"This study offers an examination of grassroots innovation actors and their integration within larger economic ecosystems. Through a comparative analysis in Oaxaca, Mexico; La Plata, Argentina; and Araucania, Chile, this research sheds light on the vital role that grassroots innovation plays in broader economic ecosystems. Using Complex Network Analysis and the TE-SER model, the study unveils how these actors interact, collaborate, and influence major economic ecosystems in the context of complex social challenges. The findings highlight that actors from the grassroots innovation ecosystem make up a significant portion of the larger innovation-driven entrepreneurial economic ecosystem, accounting for between 20% and 30% in all three cases and are strategically positioned within the ecosystem's structural network. Additionally, this study emphasizes the potential for greater integration of grassroots innovation actors to leverage resources and foster socio-economic development. The research concludes by advocating for further studies in similar socio-economic contexts to enhance our understanding of integration dynamics and mutual benefits between grassroots innovation ecosystems and other larger economic systems."}
{"Title":"Multimodal Gen-AI for Fundamental Investment Research","Link":"https:\/\/arxiv.org\/abs\/2401.06164","Authors":"Lezhi Li, Ting-Yu Chang, Hai Wang","Abstract":"This report outlines a transformative initiative in the financial investment industry, where the conventional decision-making process, laden with labor-intensive tasks such as sifting through voluminous documents, is being reimagined. Leveraging language models, our experiments aim to automate information summarization and investment idea generation. We seek to evaluate the effectiveness of fine-tuning methods on a base model (Llama2) to achieve specific application-level goals, including providing insights into the impact of events on companies and sectors, understanding market condition relationships, generating investor-aligned investment ideas, and formatting results with stock recommendations and detailed explanations. Through state-of-the-art generative modeling techniques, the ultimate objective is to develop an AI agent prototype, liberating human investors from repetitive tasks and allowing a focus on high-level strategic thinking. The project encompasses a diverse corpus dataset, including research reports, investment memos, market news, and extensive time-series market data. We conducted three experiments applying unsupervised and supervised LoRA fine-tuning on the llama2_7b_hf_chat as the base model, as well as instruction fine-tuning on the GPT3.5 model. Statistical and human evaluations both show that the fine-tuned versions perform better in solving text modeling, summarization, reasoning, and finance domain questions, demonstrating a pivotal step towards enhancing decision-making processes in the financial domain. Code implementation for the project can be found on GitHub: this https URL."}
{"Title":"CRISIS ALERT:Forecasting Stock Market Crisis Events Using Machine Learning Methods","Link":"https:\/\/arxiv.org\/abs\/2401.06172","Authors":"Yue Chen, Xingyi Andrew, Salintip Supasanya","Abstract":"Historically, the economic recession often came abruptly and disastrously. For instance, during the 2008 financial crisis, the SP 500 fell 46 percent from October 2007 to March 2009. If we could detect the signals of the crisis earlier, we could have taken preventive measures. Therefore, driven by such motivation, we use advanced machine learning techniques, including Random Forest and Extreme Gradient Boosting, to predict any potential market crashes mainly in the US market. Also, we would like to compare the performance of these methods and examine which model is better for forecasting US stock market crashes. We apply our models on the daily financial market data, which tend to be more responsive with higher reporting frequencies. We consider 75 explanatory variables, including general US stock market indexes, SP 500 sector indexes, as well as market indicators that can be used for the purpose of crisis prediction. Finally, we conclude, with selected classification metrics, that the Extreme Gradient Boosting method performs the best in predicting US stock market crisis events."}
{"Title":"CNN-DRL for Scalable Actions in Finance","Link":"https:\/\/arxiv.org\/abs\/2401.06179","Authors":"Sina Montazeri, Akram Mirzaeinia, Haseebullah Jumakhan, Amir Mirzaeinia","Abstract":"The published MLP-based DRL in finance has difficulties in learning the dynamics of the environment when the action scale increases. If the buying and selling increase to one thousand shares, the MLP agent will not be able to effectively adapt to the environment. To address this, we designed a CNN agent that concatenates the data from the last ninety days of the daily feature vector to create the CNN input matrix. Our extensive experiments demonstrate that the MLP-based agent experiences a loss corresponding to the initial environment setup, while our designed CNN remains stable, effectively learns the environment, and leads to an increase in rewards."}
{"Title":"SpotV2Net: Multivariate Intraday Spot Volatility Forecasting via Vol-of-Vol-Informed Graph Attention Networks","Link":"https:\/\/arxiv.org\/abs\/2401.06249","Authors":"Alessio Brini, Giacomo Toscano","Abstract":"This paper introduces SpotV2Net, a multivariate intraday spot volatility forecasting model based on a Graph Attention Network architecture. SpotV2Net represents assets as nodes within a graph and includes non-parametric high-frequency Fourier estimates of the spot volatility and co-volatility as node features. Further, it incorporates Fourier estimates of the spot volatility of volatility and co-volatility of volatility as features for node edges, to capture spillover effects. We test the forecasting accuracy of SpotV2Net in an extensive empirical exercise, conducted with the components of the Dow Jones Industrial Average index. The results we obtain suggest that SpotV2Net yields statistically significant gains in forecasting accuracy, for both single-step and multi-step forecasts, compared to a panel heterogenous auto-regressive model and alternative machine-learning models. To interpret the forecasts produced by SpotV2Net, we employ GNNExplainer \\citep{ying2019gnnexplainer}, a model-agnostic interpretability tool, and thereby uncover subgraphs that are critical to a node's predictions."}
{"Title":"Analysis of the Impact of Central bank Digital Currency on the Demand for Transactional Currency","Link":"https:\/\/arxiv.org\/abs\/2401.06457","Authors":"Ruimin Song, Tiantian Zhao, Chunhui Zhou","Abstract":"This paper takes the development of Central bank digital currencies as a perspective, introduces it into the Baumol-Tobin money demand theoretical framework, establishes the transactional money demand model under Central bank Digital Currency, and qualitatively analyzes the influence mechanism of Central bank digital currencies on transactional money demand; meanwhile, quarterly data from 2010-2022 are selected to test the relationship between Central bank digital currencies and transactional money demand through the ARDL model. The long-run equilibrium and short-run dynamics between the demand for Central bank digital currencies and transactional currency are examined by ARDL model. The empirical results show that the issuance and circulation of Central bank digital currencies will reduce the demand for transactional money. Based on the theoretical analysis and empirical test, this paper proposes that China should explore a more effective Currency policy in the context of Central bank digital currencies while promoting the development of Central bank digital currencies in a prudent manner in the future."}
{"Title":"Equity auction dynamics: latent liquidity models with activity acceleration","Link":"https:\/\/arxiv.org\/abs\/2401.06724","Authors":"Mohammed Salek, Damien Challet, Ioane Muni Toke","Abstract":"Equity auctions display several distinctive characteristics in contrast to continuous trading. As the auction time approaches, the rate of events accelerates causing a substantial liquidity buildup around the indicative price. This, in turn, results in a reduced price impact and decreased volatility of the indicative price. In this study, we adapt the latent\/revealed order book framework to the specifics of equity auctions. We provide precise measurements of the model parameters, including order submissions, cancellations, and diffusion rates. Our setup allows us to describe the full dynamics of the average order book during closing auctions in Euronext Paris. These findings support the relevance of the latent liquidity framework in describing limit order book dynamics. Lastly, we analyze the factors contributing to a sub-diffusive indicative price and demonstrate the absence of indicative price predictability."}
{"Title":"A deep implicit-explicit minimizing movement method for option pricing in jump-diffusion models","Link":"https:\/\/arxiv.org\/abs\/2401.06740","Authors":"Emmanuil H. Georgoulis, Antonis Papapantoleon, Costas Smaragdakis","Abstract":"We develop a novel deep learning approach for pricing European basket options written on assets that follow jump-diffusion dynamics. The option pricing problem is formulated as a partial integro-differential equation, which is approximated via a new implicit-explicit minimizing movement time-stepping approach, involving approximation by deep, residual-type Artificial Neural Networks (ANNs) for each time step. The integral operator is discretized via two different approaches: a) a sparse-grid Gauss--Hermite approximation following localised coordinate axes arising from singular value decompositions, and b) an ANN-based high-dimensional special-purpose quadrature rule. Crucially, the proposed ANN is constructed to ensure the asymptotic behavior of the solution for large values of the underlyings and also leads to consistent outputs with respect to a priori known qualitative properties of the solution. The performance and robustness with respect to the dimension of the methods are assessed in a series of numerical experiments involving the Merton jump-diffusion model."}
{"Title":"Austria's KlimaTicket: Assessing the short-term impact of a cheap nationwide travel pass on demand","Link":"https:\/\/arxiv.org\/abs\/2401.06835","Authors":"Hannes Wallimann","Abstract":"Measures to reduce transport-related greenhouse gas emissions are of great importance to policy-makers. A recent example is the nationwide KlimaTicket in Austria, a country with a relatively high share of transport-related emissions. The cheap yearly season ticket introduced in October 2021 allows unlimited access to Austria's public transport network. Using the synthetic control and synthetic difference-in-differences methods, I assess the causal effect of this policy on public transport demand by constructing a data-driven counterfactual out of European railway companies to mimic the number of passengers of the Austrian Federal Railways without the KlimaTicket. The results indicate public transport demand grew slightly faster in Austria, i.e., 3.3 or 6.8 percentage points, depending on the method, than it would have in the absence of the KlimaTicket. However, the growth effect after the COVID-19 pandemic appears only statistically significant when applying the synthetic control method, and the positive effect on public transport demand growth disappears in 2022."}
{"Title":"An empirical model of fleet modernization: on the relationship between market concentration and innovation adoption in the Brazilian airline industry","Link":"https:\/\/arxiv.org\/abs\/2401.06876","Authors":"Alessandro V. M. Oliveira, Thiago Caliari, Rodolfo R. Narcizo","Abstract":"The modernization of an airline's fleet can reduce its operating costs, improve the perceived quality of service offered to passengers, and mitigate emissions. The present paper investigates the market incentives that airlines have to adopt technological innovation from manufacturers by acquiring new generation aircraft. We develop an econometric model of fleet modernization in the Brazilian commercial aviation over two decades. We examine the hypothesis of an inverted-U relationship between market concentration and fleet modernization and find evidence that both the extremes of competition and concentration may inhibit innovation adoption by carriers. We find limited evidence associating either hubbing activity or low-cost carriers with the more intense introduction of new types of aircraft models and variants in the industry. Finally, our results suggest that energy cost rises may provoke boosts in fleet modernization in the long term, with carriers possibly targeting more eco-efficient operations up to two years after an upsurge in fuel price."}
{"Title":"Heterogeneous treatment effect estimation with high-dimensional data in public policy evaluation -- an application to the conditioning of cash transfers in Morocco using causal machine learning","Link":"https:\/\/arxiv.org\/abs\/2401.07075","Authors":"Patrick Rehill, Nicholas Biddle","Abstract":"Causal machine learning methods can be used to search for treatment effect heterogeneity in high-dimensional datasets even where we lack a strong enough theoretical framework to select variables or make parametric assumptions about data. This paper uses causal machine learning methods to estimate heterogeneous treatment effects in the case of an experimental study carried out in Morocco which evaluated the effect of conditionalizing a cash transfer program on several outcomes including maths test scores which is the focus of this work. We explore treatment effects across a dataset of 1936 pre-treatment variables. For the most part, heterogeneity is modelled by two different factors, participation in education (at the baseline) and more general measures of poverty. Those who are more disadvantaged at the baseline benefit less from any treatment. While conditioning generally has a negative effect this more disadvantaged group is also hurt more by conditioning. The second purpose of this paper is to demonstrate and reflect upon a causal machine learning approach to policy evaluation. We propose a novel causal tree method for interpretable modelling of causal effects and reflect on the difficulty of explaining atheoretical results."}
{"Title":"Learning to be Homo Economicus: Can an LLM Learn Preferences from Choice","Link":"https:\/\/arxiv.org\/abs\/2401.07345","Authors":"Jeongbin Kim, Matthew Kovach, Kyu-Min Lee, Euncheol Shin, Hector Tzavellas","Abstract":"This paper explores the use of Large Language Models (LLMs) as decision aids, with a focus on their ability to learn preferences and provide personalized recommendations. To establish a baseline, we replicate standard economic experiments on choice under risk (Choi et al., 2007) with GPT, one of the most prominent LLMs, prompted to respond as (i) a human decision maker or (ii) a recommendation system for customers. With these baselines established, GPT is provided with a sample set of choices and prompted to make recommendations based on the provided data. From the data generated by GPT, we identify its (revealed) preferences and explore its ability to learn from data. Our analysis yields three results. First, GPT's choices are consistent with (expected) utility maximization theory. Second, GPT can align its recommendations with people's risk aversion, by recommending less risky portfolios to more risk-averse decision makers, highlighting GPT's potential as a personalized decision aid. Third, however, GPT demonstrates limited alignment when it comes to disappointment aversion."}
{"Title":"Graph database while computationally efficient filters out quickly the ESG integrated equities in investment management","Link":"https:\/\/arxiv.org\/abs\/2401.07483","Authors":"Partha Sen, Sumana Sen","Abstract":"Design\/methodology\/approach This research evaluated the databases of SQL, No-SQL and graph databases to compare and contrast efficiency and performance. To perform this experiment the data were collected from multiple sources including stock price and financial news. Python is used as an interface to connect and query databases (to create database structures according to the feed file structure, to load data into tables, objects, to read data , to connect PostgreSQL, ElasticSearch, Neo4j. Purpose Modern applications of LLM (Large language model) including RAG (Retrieval Augmented Generation) with Machine Learning, deep learning, NLP (natural language processing) or Decision Analytics are computationally expensive. Finding a better option to consume less resources and time to get the result. Findings The Graph database of ESG (Environmental, Social and Governance) is comparatively better and can be considered for extended analytics to integrate ESG in business and investment. Practical implications A graph ML with a RAG architecture model can be introduced as a new framework with less computationally expensive LLM application in the equity filtering process for portfolio management. Originality\/value Filtering out selective stocks out of two thousand or more listed companies in any stock exchange for active investment, consuming less resource consumption especially memory and energy to integrate artificial intelligence and ESG in business and investment."}
{"Title":"Impermanent Loss Conditions: An Analysis of Decentralized Exchange Platforms","Link":"https:\/\/arxiv.org\/abs\/2401.07689","Authors":"Matthias Hafner, Helmut Dietl","Abstract":"Decentralized exchanges are widely used platforms for trading crypto assets. The most common types work with automated market makers (AMM), allowing traders to exchange assets without needing to find matching counterparties. Thereby, traders exchange against asset reserves managed by smart contracts. These assets are provided by liquidity providers in exchange for a fee. Static analysis shows that small price changes in one of the assets can result in losses for liquidity providers. Despite the success of AMMs, it is claimed that liquidity providers often suffer losses. However, the literature does not adequately consider the dynamic effects of fees over time. Therefore, we investigate the impermanent loss problem in a dynamic setting using Monte Carlo simulations. Our findings indicate that price changes do not necessarily lead to losses. Fees paid by traders and arbitrageurs are equally important. In this respect, we can show that an arbitrage-friendly environment benefits the liquidity provider. Thus, we suggest that AMM developers should promote an arbitrage-friendly environment rather than trying to prevent arbitrage."}
{"Title":"Provisions and Economic Capital for Credit Losses","Link":"https:\/\/arxiv.org\/abs\/2401.07728","Authors":"Dorinel Bastide (LaMME), St\u00e9phane Cr\u00e9pey (LPSM)","Abstract":"Based on supermodularity ordering properties, we show that convex risk measures of credit losses are nondecreasing w.r.t. credit-credit and, in a wrong-way risk setup, credit-market, covariances of elliptically distributed latent factors. These results support the use of such setups for computing credit provisions and economic capital or for conducting stress test exercises and risk management analysis."}
{"Title":"A new model of trust based on neural information processing","Link":"https:\/\/arxiv.org\/abs\/2401.08064","Authors":"Scott E. Allen, Ren\u00e9 F. Kizilcec, A. David Redish","Abstract":"More than 30 years of research has firmly established the vital role of trust in human organizations and relationships, but the underlying mechanisms by which people build, lose, and rebuild trust remains incompletely understood. We propose a mechanistic model of trust that is grounded in the modern neuroscience of decision making. Since trust requires anticipating the future actions of others, any mechanistic model must be built upon up-to-date theories on how the brain learns, represents, and processes information about the future within its decision-making systems. Contemporary neuroscience has revealed that decision making arises from multiple parallel systems that perform distinct, complementary information processing. Each system represents information in different forms, and therefore learns via different mechanisms. When an act of trust is reciprocated or violated, this provides new information that can be used to anticipate future actions. The taxonomy of neural information representations that is the basis for the system boundaries between neural decision-making systems provides a taxonomy for categorizing different forms of trust and generating mechanistic predictions about how these forms of trust are learned and manifested in human behavior. Three key predictions arising from our model are (1) strategic risk-taking can reveal how to best proceed in a relationship, (2) human organizations and environments can be intentionally designed to encourage trust among their members, and (3) violations of trust need not always degrade trust, but can also provide opportunities to build trust."}
{"Title":"A Two-Step Longstaff Schwartz Monte Carlo Approach to Game Option Pricing","Link":"https:\/\/arxiv.org\/abs\/2401.08093","Authors":"Ce Wang","Abstract":"We proposed a two-step Longstaff Schwartz Monte Carlo (LSMC) method with two regression models fitted at each time step to price game options. Although the original LSMC can be used to price game options with an enlarged range of path in regression and a modified cashflow updating rule, we identified a drawback of such approach, which motivated us to propose our approach. We implemented numerical examples with benchmarks using binomial tree and numerical PDE, and it showed that our method produces more reliable results comparing to the original LSMC."}
{"Title":"Optimal Insurance to Maximize Exponential Utility when Premium is Computed by a Convex Functional","Link":"https:\/\/arxiv.org\/abs\/2401.08094","Authors":"Jingyi Cao, Dongchen Li, Virginia R. Young, Bin Zou","Abstract":"We find the optimal indemnity to maximize the expected utility of terminal wealth of a buyer of insurance whose preferences are modeled by an exponential utility. The insurance premium is computed by a convex functional. We obtain a necessary condition for the optimal indemnity; then, because the candidate optimal indemnity is given implicitly, we use that necessary condition to develop a numerical algorithm to compute it. We prove that the numerical algorithm converges to a unique indemnity that, indeed, equals the optimal policy. We also illustrate our results with numerical examples."}
{"Title":"Do backrun auctions protect traders?","Link":"https:\/\/arxiv.org\/abs\/2401.08302","Authors":"Andrew W. Macpherson","Abstract":"We study a new \"laminated\" queueing model for orders on batched trading venues such as decentralised exchanges. The model aims to capture and generalise transaction queueing infrastructure that has arisen to organise MEV activity on public blockchains such as Ethereum, providing convenient channels for sophisticated agents to extract value by acting on end-user order flow by performing arbitrage and related HFT activities. In our model, market orders are interspersed with orders created by arbitrageurs that under idealised conditions reset the marginal price to a global equilibrium between each trade, improving predictability of execution for liquidity traders.\nIf an arbitrageur has a chance to land multiple opportunities in a row, he may attempt to manipulate the execution price of the intervening market order by a probabilistic blind sandwiching strategy. To study how bad this manipulation can get, we introduce and bound a price manipulation coefficient that measures the deviation from global equilibrium of local pricing quoted by a rational arbitrageur. We exhibit cases in which this coefficient is well approximated by a \"zeta value' with interpretable and empirically measurable parameters."}
{"Title":"Dynamic portfolio selection under generalized disappointment aversion","Link":"https:\/\/arxiv.org\/abs\/2401.08323","Authors":"Zongxia Liang, Sheng Wang, Jianming Xia, Fengyi Yuan","Abstract":"This paper addresses the continuous-time portfolio selection problem under generalized disappointment aversion (GDA). The implicit definition of the certainty equivalent within GDA preferences introduces time inconsistency to this problem. We provide the sufficient and necessary condition for a strategy to be an equilibrium by a fully nonlinear integral equation. Investigating the existence and uniqueness of the solution to the integral equation, we establish the existence and uniqueness of the equilibrium. Our findings indicate that under disappointment aversion preferences, non-participation in the stock market is the unique equilibrium. The semi-analytical equilibrium strategies obtained under the constant relative risk aversion utility functions reveal that, under GDA preferences, the investment proportion in the stock market consistently remains smaller than the investment proportion under classical expected utility theory. The numerical analysis shows that the equilibrium strategy's monotonicity concerning the two parameters of GDA preference aligns with the monotonicity of the degree of risk aversion."}
{"Title":"Fitting random cash management models to data","Link":"https:\/\/arxiv.org\/abs\/2401.08548","Authors":"Francisco Salas-Molina","Abstract":"Organizations use cash management models to control balances to both avoid overdrafts and obtain a profit from short-term investments. Most management models are based on control bounds which are derived from the assumption of a particular cash flow probability distribution. In this paper, we relax this strong assumption to fit cash management models to data by means of stochastic and linear programming. We also introduce ensembles of random cash management models which are built by randomly selecting a subsequence of the original cash flow data set. We illustrate our approach by means of a real case study showing that a small random sample of data is enough to fit sufficiently good bound-based models."}
{"Title":"Incremento del precio de los combustibles y su incidencia en los productos de la canasta basica del canton el triunfo, provincia del guayas","Link":"https:\/\/arxiv.org\/abs\/2401.08590","Authors":"Alvear Guzman Katherine, Campozano Buele Jenner, Duran Canarte Paulette, Holguin Cedeno Roger, Mejia Crespin Fernando","Abstract":"The objective of this research was to analyze the impact of the increase in the price of fuels and its incidence on the products of the basic basket of the El Triunfo, the province of Guayas. In the present study, the non-experimental quantitative method was used. The study population was limited to the families of the town, seeking to determine how their level of consumption was impacted after the increase in fuels. Just 95 people were randomly taken. The study instrument that was used was surveys, with a focus on the purchasing power of families with respect to the basic basket after the increase in fuel prices. The results were processed through Cronbach's Alpha and reflected in pie charts. The independent and dependent variable that make up our study, were related through a simple linear regression, to determine if they correlate with each other.\nKeywords: Fuels, Basic basket, Linear regression, Subsidies, Inflation"}
{"Title":"How do we measure trade elasticity for services?","Link":"https:\/\/arxiv.org\/abs\/2401.08594","Authors":"Satoshi Nakano, Kazuhiko Nishimura","Abstract":"This paper is about our attempt of identifying trade elasticities through the variations in the exchange rate, for possible applications to the case of services whose physical transactions are veiled in the trade statistics. The regression analysis to estimate the elasticity entails a situation where the explanatory variable is leaked into the error term through the latent supply equation, causing an endogeneity problem for which an instrumental variable cannot be found. Our identification strategy is to utilize the normalizing condition, which enables the supply parameter to be identified, along with the reduced-form equation of the system of demand and supply equations. We evaluate the performances of the method proposed by applying to several different tangible goods, whose benchmark trade elasticities are estimable by utilizing the information on their physical transactions."}
{"Title":"Non-Banking Sector development effect on Economic Growth. A Nighttime light data approach","Link":"https:\/\/arxiv.org\/abs\/2401.08596","Authors":"Leonard Mushunje, Maxwell Mashasha","Abstract":"This paper uses nighttime light(NTL) data to measure the nexus of the non-banking sector, particularly insurance, and economic growth in South Africa. We hypothesize that insurance sector growth positively propels economic growth due to its economic growth-supportive traits like investment protection and optimal risk mitigation. We also claim that Nighttime light data is a good economic measure than Gross domestic product (GDP). We used weighted regressions to measure the relationships between nighttime light data, GDP, and insurance sector development. We used time series South African GDP data collected from the World Bank for the period running from 2000 to 2018, and the nighttime lights data from the National Geophysical Data Centre (NGDC) in partnership with the National Oceanic and Atmospheric Administration (NOAA). From the models fitted and the reported BIC, AIC, and likelihood ratios, the insurance sector proved to have more predictive power on economic development in South Africa, and radiance light explained economic growth better than GDP and GDP\/Capita. We concluded that nighttime data is a good proxy for economic growth than GDP\/Capita in emerging economies like South Africa, where secondary data needs to be more robust and sometimes inflated. The findings will guide researchers and policymakers on what drives economic development and what policies to put in place. It would be interesting to extend the current study to other sectors such as micro-finances, mutual and hedge funds."}
{"Title":"Reinforcement Learning and Deep Stochastic Optimal Control for Final Quadratic Hedging","Link":"https:\/\/arxiv.org\/abs\/2401.08600","Authors":"Bernhard Hientzsch","Abstract":"We consider two data driven approaches, Reinforcement Learning (RL) and Deep Trajectory-based Stochastic Optimal Control (DTSOC) for hedging a European call option without and with transaction cost according to a quadratic hedging P&L objective at maturity (\"variance-optimal hedging\" or \"final quadratic hedging\"). We study the performance of the two approaches under various market environments (modeled via the Black-Scholes and\/or the log-normal SABR model) to understand their advantages and limitations. Without transaction costs and in the Black-Scholes model, both approaches match the performance of the variance-optimal Delta hedge. In the log-normal SABR model without transaction costs, they match the performance of the variance-optimal Barlett's Delta hedge. Agents trained on Black-Scholes trajectories with matching initial volatility but used on SABR trajectories match the performance of Bartlett's Delta hedge in average cost, but show substantially wider variance. To apply RL approaches to these problems, P&L at maturity is written as sum of step-wise contributions and variants of RL algorithms are implemented and used that minimize expectation of second moments of such sums."}
{"Title":"Forking paths in financial economics","Link":"https:\/\/arxiv.org\/abs\/2401.08606","Authors":"Guillaume Coqueret","Abstract":"We argue that spanning large numbers of degrees of freedom in empirical analysis allows better characterizations of effects and thus improves the trustworthiness of conclusions. Our ideas are illustrated in three studies: equity premium prediction, asset pricing anomalies and risk premia estimation. In the first, we find that each additional degree of freedom in the protocol expands the average range of t-statistics by at least 30%. In the second, we show that resorting to forking paths instead of bootstrapping in multiple testing raises the bar of significance for anomalies: at the 5% confidence level, the threshold for bootstrapped statistics is 4.5, whereas with paths, it is at least 8.2, a bar much higher than those currently used in the literature. In our third application, we reveal the importance of particular steps in the estimation of premia. In addition, we use paths to corroborate prior findings in the three topics. We document heterogeneity in our ability to replicate prior studies: some conclusions seem robust, others do not align with the paths we were able to generate."}
{"Title":"Leverage Staking with Liquid Staking Derivatives (LSDs): Opportunities and Risks","Link":"https:\/\/arxiv.org\/abs\/2401.08610","Authors":"Xihan Xiong, Zhipeng Wang, Xi Chen, William Knottenbelt, Michael Huth","Abstract":"In the Proof of Stake (PoS) Ethereum ecosystem, users can stake ETH on Lido to receive stETH, a Liquid Staking Derivative (LSD) that represents staked ETH and accrues staking rewards. LSDs improve the liquidity of staked assets by facilitating their use in secondary markets, such as for collateralized borrowing on Aave or asset exchanges on Curve. The composability of Lido, Aave, and Curve enables an emerging strategy known as leverage staking, where users supply stETH as collateral on Aave to borrow ETH and then acquire more stETH. This can be done directly by initially staking ETH on Lido or indirectly by swapping ETH for stETH on Curve. While this iterative process enhances financial returns, it also introduces potential risks.\nThis paper explores the opportunities and risks of leverage staking. We establish a formal framework for leverage staking with stETH and identify 442 such positions on Ethereum over 963 days. These positions represent a total volume of 537,123 ETH (877m USD). Our data reveal that the majority (81.7%) of leverage staking positions achieved an Annual Percentage Rate (APR) higher than that of conventional staking on Lido. Despite the high returns, we also recognize the risks of leverage staking. From the Terra crash incident, we understand that token devaluation can greatly impact the market. Therefore, we conduct stress tests under extreme conditions, particularly during stETH devaluations, to thoroughly evaluate the associated risks. Our simulations indicate that leverage staking can exacerbate the risk of cascading liquidations by introducing additional selling pressures from liquidation and deleveraging activities. Moreover, this strategy poses broader systemic risks as it undermines the stability of ordinary positions by intensifying their liquidations."}
{"Title":"Automated Design Appraisal: Estimating Real Estate Price Growth and Value at Risk due to Local Development","Link":"https:\/\/arxiv.org\/abs\/2401.08645","Authors":"Adam R. Swietek","Abstract":"Financial criteria in architectural design evaluation are limited to cost performance. Here, I introduce a method, Automated Design Appraisal (ADA), to predict the market price of a generated building design concept within a local urban context. Integrating ADA with 3D building performance simulations enables financial impact assessment that exceeds the spatial resolution of previous work. Within an integrated impact assessment, ADA measures the direct and localized effect of urban development. To demonstrate its practical utility, I study local devaluation risk due to nearby development associated with changes to visual landscape quality. The results shed light on the relationship between amenities and property value, identifying clusters of properties physically exposed or financially sensitive to local land-use change. Beyond its application as a financial sensitivity tool, ADA serves as a blueprint for architectural design optimization procedures, in which economic performance is evaluated based on learned preferences derived from financial market data."}
{"Title":"Spurious Default Probability Projections in Credit Risk Stress Testing Models","Link":"https:\/\/arxiv.org\/abs\/2401.08892","Authors":"Bernd Engelmann","Abstract":"Credit risk stress testing has become an important risk management device which is used both by banks internally and by regulators. Stress testing is complex because it essentially means projecting a bank's full balance sheet conditional on a macroeconomic scenario over multiple years. Part of the complexity stems from using a wide range of model parameters for, e.g., rating transition, write-off rules, prepayment, or origination of new loans. A typical parameterization of a credit risk stress test model specifies parameters linked to an average economic, the through-the-cycle, state. These parameters are transformed to a stressed state by utilizing a macroeconomic model. It will be shown that the model parameterization implies a unique through-the-cycle portfolio which is unrelated to a bank's current portfolio. Independent of the stress imposed to the model, the current portfolio will have a tendency to propagate towards the through-the-cycle portfolio. This could create unwanted spurious effects on projected portfolio default rates especially when a stress test model's parameterization is inconsistent with a bank's current portfolio."}
{"Title":"On conditioning and consistency for nonlinear functionals","Link":"https:\/\/arxiv.org\/abs\/2401.09054","Authors":"Edoardo Berton, Alessandro Doldi, Marco Maggis","Abstract":"We consider a family of conditional nonlinear expectations defined on the space of bounded random variables and indexed by the class of all the sub-sigma-algebras of a given underlying sigma-algebra. We show that if this family satisfies a natural consistency property, then it collapses to a conditional certainty equivalent defined in terms of a state-dependent utility function. This result is obtained by embedding our problem in a decision theoretical framework and providing a new characterization of the Sure-Thing Principle. In particular we prove that this principle characterizes those preference relations which admit consistent backward conditional projections. We build our analysis on state-dependent preferences for a general state space as in Wakker and Zank (1999) and show that their numerical representation admits a continuous version of the state-dependent utility. In this way, we also answer positively to a conjecture posed in the aforementioned paper."}
{"Title":"AI Thrust: Ranking Emerging Powers for Tech Startup Investment in Latin America","Link":"https:\/\/arxiv.org\/abs\/2401.09056","Authors":"Abraham Ramos Torres, Laura N Montoya","Abstract":"Artificial intelligence (AI) is rapidly transforming the global economy, and Latin America is no exception. In recent years, there has been a growing interest in AI development and implementation in the region. This paper presents a ranking of Latin American (LATAM) countries based on their potential to become emerging powers in AI. The ranking is based on three pillars: infrastructure, education, and finance. Infrastructure is measured by the availability of electricity, high-speed internet, the quality of telecommunications networks, and the availability of supercomputers. Education is measured by the quality of education and the research status. Finance is measured by the cost of investments, history of investments, economic metrics, and current implementation of AI.\nWhile Brazil, Chile, and Mexico have established themselves as major players in the AI industry in Latin America, our ranking demonstrates the new emerging powers in the region. According to the results, Argentina, Colombia, Uruguay, Costa Rica, and Ecuador are leading as new emerging powers in AI in Latin America. These countries have strong education systems, well-developed infrastructure, and growing financial resources. The ranking provides a useful tool for policymakers, investors, and businesses interested in AI development in Latin America. It can help to identify emerging LATAM countries with the greatest potential for AI growth and success."}
{"Title":"Airline delays, congestion internalization and non-price spillover effects of low cost carrier entry","Link":"https:\/\/arxiv.org\/abs\/2401.09174","Authors":"William E. Bendinelli, Humberto F. A. J. Bettini, Alessandro V. M. Oliveira","Abstract":"This paper develops an econometric model of flight delays to investigate the influence of competition and dominance on the incentives of carriers to maintain on-time performance. We consider both the route and the airport levels to inspect the local and global effects of competition, with a unifying framework to test the hypotheses of 1. airport congestion internalization and 2. the market competition-quality relationship in a single econometric model. In particular, we examine the impacts of the entry of low cost carriers (LCC) on the flight delays of incumbent full service carriers in the Brazilian airline industry. The main results indicate a highly significant effect of airport congestion self-internalization in parallel with route-level quality competition. Additionally, the potential competition caused by LCC presence provokes a global effect that suggests the existence of non-price spillovers of the LCC entry to non-entered routes."}
{"Title":"A closer look at the chemical potential of an ideal agent system","Link":"https:\/\/arxiv.org\/abs\/2401.09233","Authors":"Christoph J. B\u00f6rner, Ingo Hoffmann, John H. Stiebel","Abstract":"Models for spin systems known from statistical physics are used in econometrics in the form of agent-based models. Econophysics research in econometrics is increasingly developing general market models that describe exchange phenomena and use the chemical potential \\mu known from physics in the context of particle number changes. In statistical physics, equations of state are known for the chemical potential, which take into account the respective model framework and the corresponding state variables. A simple transfer of these equations of state to problems in econophysics appears difficult. To the best of our knowledge, the equation of state for the chemical potential is currently missing even for the simplest conceivable model of an ideal agent system. In this paper, this research gap is closed and the equation of state for the chemical potential is derived from the econophysical model assumptions of the ideal agent system. An interpretation of the equation of state leads to fundamental relationships that could also have been guessed, but are shown here by the theory."}
{"Title":"Equity Premium in Efficient Markets","Link":"https:\/\/arxiv.org\/abs\/2401.09265","Authors":"B.N. Kausik","Abstract":"Equity premium, the surplus returns of stocks over bonds, has been an enduring puzzle. While numerous prior works approach the problem assuming the utility of money is invariant across contexts, our approach implies that in efficient markets the utility of money is polymorphic, with risk aversion dependent on the information available in each context, i.e. the discount on each future cash flow depends on all information available on that cash flow. Specifically, we prove that in efficient markets, informed investors maximize return on volatility by being risk-neutral with riskless bonds, and risk-averse with equities, thereby resolving the puzzle. We validate our results on historical data with surprising consistency.\nJEL Classification: C58, G00, G12, G17"}
{"Title":"Neural Hawkes: Non-Parametric Estimation in High Dimension and Causality Analysis in Cryptocurrency Markets","Link":"https:\/\/arxiv.org\/abs\/2401.09361","Authors":"Timoth\u00e9e Fabre, Ioane Muni Toke","Abstract":"We propose a novel approach to marked Hawkes kernel inference which we name the moment-based neural Hawkes estimation method. Hawkes processes are fully characterized by their first and second order statistics through a Fredholm integral equation of the second kind. Using recent advances in solving partial differential equations with physics-informed neural networks, we provide a numerical procedure to solve this integral equation in high dimension. Together with an adapted training pipeline, we give a generic set of hyperparameters that produces robust results across a wide range of kernel shapes. We conduct an extensive numerical validation on simulated data. We finally propose two applications of the method to the analysis of the microstructure of cryptocurrency markets. In a first application we extract the influence of volume on the arrival rate of BTC-USD trades and in a second application we analyze the causality relationships and their directions amongst a universe of 15 cryptocurrency pairs in a centralized exchange."}
{"Title":"AI and the Opportunity for Shared Prosperity: Lessons from the History of Technology and the Economy","Link":"https:\/\/arxiv.org\/abs\/2401.09718","Authors":"Guy Ben-Ishai, Jeff Dean, James Manyika, Ruth Porat, Hal Varian, Kent Walker","Abstract":"Recent progress in artificial intelligence (AI) marks a pivotal moment in human history. It presents the opportunity for machines to learn, adapt, and perform tasks that have the potential to assist people, from everyday activities to their most creative and ambitious projects. It also has the potential to help businesses and organizations harness knowledge, increase productivity, innovate, transform, and power shared prosperity. This tremendous potential raises two fundamental questions: (1) Will AI actually advance national and global economic transformation to benefit society at large? and (2) What issues must we get right to fully realize AI's economic value, expand prosperity and improve lives everywhere? We explore these questions by considering the recent history of technology and innovation as a guide for the likely impact of AI and what we must do to realize its economic potential to benefit society. While we do not presume the future will be entirely like that past, for reasons we will discuss, we do believe prior experience with technological change offers many useful lessons. We conclude that while progress in AI presents a historic opportunity to advance our economic prosperity and future wellbeing, its economic benefits will not come automatically and that AI risks exacerbating existing economic challenges unless we collectively and purposefully act to enable its potential and address its challenges. We suggest a collective policy agenda - involving developers, deployers and users of AI, infrastructure providers, policymakers, and those involved in workforce training - that may help both realize and harness AI's economic potential and address its risks to our shared prosperity."}
{"Title":"Cross-Domain Behavioral Credit Modeling: transferability from private to central data","Link":"https:\/\/arxiv.org\/abs\/2401.09778","Authors":"O. Didkovskyi, N. Jean, G. Le Pera, C. Nordio","Abstract":"This paper introduces a credit risk rating model for credit risk assessment in quantitative finance, aiming to categorize borrowers based on their behavioral data. The model is trained on data from Experian, a widely recognized credit bureau, to effectively identify instances of loan defaults among bank customers. Employing state-of-the-art statistical and machine learning techniques ensures the model's predictive accuracy. Furthermore, we assess the model's transferability by testing it on behavioral data from the Bank of Italy, demonstrating its potential applicability across diverse datasets during prediction. This study highlights the benefits of incorporating external behavioral data to improve credit risk assessment in financial institutions."}
{"Title":"A Framework for Digital Currencies for Financial Inclusion in Latin America and the Caribbean","Link":"https:\/\/arxiv.org\/abs\/2401.09811","Authors":"Gabriel Bizama, Alexander Wu, Bernardo Paniagua, Max Mitre","Abstract":"This research aims to provide a framework to assess the contribution of digital currencies to promote financial inclusion, based on a diagnosis of the landscape of financial inclusion and domestic and cross-border payments in Latin America and the Caribbean. It also provides insights from central banks in the region on key aspects regarding a possible implementation of central bank digital currencies. Findings show that although digital currencies development is at an early stage, a well-designed system could reduce the cost of domestic and cross-border payments, improve the settlement of transactions to achieve real-time payments, expand the accessibility of central bank money, incorporate programmable payments and achieve system performance demands."}
{"Title":"Consistent asset modelling with random coefficients and switches between regimes","Link":"https:\/\/arxiv.org\/abs\/2401.09955","Authors":"Felix L. Wolf, Griselda Deelstra, Lech A. Grzelak","Abstract":"We explore a stochastic model that enables capturing external influences in two specific ways. The model allows for the expression of uncertainty in the parametrisation of the stochastic dynamics and incorporates patterns to account for different behaviours across various times or regimes. To establish our framework, we initially construct a model with random parameters, where the switching between regimes can be dictated either by random variables or deterministically. Such a model is highly interpretable. We further ensure mathematical consistency by demonstrating that the framework can be elegantly expressed through local volatility models taking the form of standard jump diffusions. Additionally, we consider a Markov-modulated approach for the switching between regimes characterised by random parameters. For all considered models, we derive characteristic functions, providing a versatile tool with wide-ranging applications. In a numerical experiment, we apply the framework to the financial problem of option pricing. The impact of parameter uncertainty is analysed in a two-regime model, where the asset process switches between periods of high and low volatility imbued with high and low uncertainty, respectively."}
{"Title":"An Exploration to the Correlation Structure and Clustering of Macroeconomic Variables","Link":"https:\/\/arxiv.org\/abs\/2401.10162","Authors":"Garvit Arora, Shubhangi Tiwari, Ying Wu, Xuan Mei","Abstract":"As a quantitative characterization of the complicated economy, Macroeconomic Variables (MEVs), including GDP, inflation, unemployment, income, spending, interest rate, etc., are playing a crucial role in banks' portfolio management and stress testing exercise. In recent years, especially during the COVID-19 period and the current high inflation environment, people are frequently talking about the changing \"correlation structure\" of MEVs. In this paper, we use a principal component based algorithm to perform unsupervised clustering on MEVs so we can quantify and better understand MEVs' correlation structure in any given period. We also demonstrate how this method can be used to visualize historical MEVs pattern changes between 2000 and 2022. Further, we use this method to compare different hypothetical and\/or historical macroeconomic scenarios and present our key findings. One of these interesting observations is that, for a list of 132 transformations derived from 44 targeted MEVs that cover 5 different aspects of the U.S. economy (which takes as a subset the 10+ key MEVs published by FRB), compared to benign years where there are typically 20-25 clusters, during the great financial crisis (GFC), i.e., 2007-2010, they exhibited a more synchronized and less diversified pattern of movement, forming roughly 15 clusters. We also see this contrast in the hypothetical CCAR2023 FRB scenarios where the Severely Adverse scenario has 15 clusters and the Baseline scenario has 21 clusters. We provide our interpretation to this observation and hope this research can inspire and benefit researchers from different domains all over the world."}
{"Title":"Equilibrium Multiplicity: A Systematic Approach using Homotopies, with an Application to Chicago","Link":"https:\/\/arxiv.org\/abs\/2401.10181","Authors":"Amine C-L. Ouazad","Abstract":"Discrete choice models with social interactions or spillovers may exhibit multiple equilibria. This paper provides a systematic approach to enumerating them for a quantitative spatial model with discrete locations, social interactions, and elastic housing supply. The approach relies on two homotopies. A homotopy is a smooth function that transforms the solutions of a simpler city where solutions are known, to a city with heterogeneous locations and finite supply elasticity. The first homotopy is that, in the set of cities with perfectly elastic floor surface supply, an economy with heterogeneous locations is homotopic to an economy with homogeneous locations, whose solutions can be comprehensively enumerated. Such an economy is epsilon close to an economy whose equilibria are the zeros of a system of polynomials. This is a well-studied area of mathematics where the enumeration of equilibria can be guaranteed. The second homotopy is that a city with perfectly elastic housing supply is homotopic to a city with an arbitrary supply elasticity. In a small number of cases, the path may bifurcate and a single path yields two or more equilibria. By running the method on thousands of cities, we obtain a large number of equilibria. Each equilibrium has different population distributions. We provide a method that is computationally feasible for economies with a large number of locations choices, with an empirical application to the City of Chicago. There exist multiple ``counterfactual Chicagos'' consistent with the estimated parameters. Population distribution, prices, and welfare are not uniquely pinned down by amenities. The paper's method can be applied to models in trade and IO. Further applications of algebraic geometry are suggested."}
{"Title":"Interplay between Cryptocurrency Transactions and Online Financial Forums","Link":"https:\/\/arxiv.org\/abs\/2401.10238","Authors":"Ana Fern\u00e1ndez Vilas, Rebeca P. D\u00edaz Redondo, Daniel Couto Cancela, Alejandro Torrado Pazos","Abstract":"Cryptocurrencies are a type of digital money meant to provide security and anonymity while using cryptography techniques. Although cryptocurrencies represent a breakthrough and provide some important benefits, their usage poses some risks that are a result of the lack of supervising institutions and transparency. Because disinformation and volatility is discouraging for personal investors, cryptocurrencies emerged hand-in-hand with the proliferation of online users' communities and forums as places to share information that can alleviate users' mistrust. This research focuses on the study of the interplay between these cryptocurrency forums and fluctuations in cryptocurrency values. In particular, the most popular cryptocurrency Bitcoin (BTC) and a related active discussion community, Bitcointalk, are analyzed. This study shows that the activity of Bitcointalk forum keeps a direct relationship with the trend in the values of BTC, therefore analysis of this interaction would be a perfect base to support personal investments in a non-regulated market and, to confirm whether cryptocurrency forums show evidences to detect abnormal behaviors in BTC values as well as to predict or estimate these values. The experiment highlights that forum data can explain specific events in the financial field. It also underlines the relevance of quotes (regular mechanism to response a post) at periods: (1) when there is a high concentration of posts around certain topics; (2) when peaks in the BTC price are observed; and, (3) when the BTC price gradually shifts downwards and users intend to sell."}
{"Title":"Nowcasting Madagascar's real GDP using machine learning algorithms","Link":"https:\/\/arxiv.org\/abs\/2401.10255","Authors":"Franck Ramaharo, Gerzhino Rasolofomanana","Abstract":"We investigate the predictive power of different machine learning algorithms to nowcast Madagascar's gross domestic product (GDP). We trained popular regression models, including linear regularized regression (Ridge, Lasso, Elastic-net), dimensionality reduction model (principal component regression), k-nearest neighbors algorithm (k-NN regression), support vector regression (linear SVR), and tree-based ensemble models (Random forest and XGBoost regressions), on 10 Malagasy quarterly macroeconomic leading indicators over the period 2007Q1--2022Q4, and we used simple econometric models as a benchmark. We measured the nowcast accuracy of each model by calculating the root mean square error (RMSE), mean absolute error (MAE), and mean absolute percentage error (MAPE). Our findings reveal that the Ensemble Model, formed by aggregating individual predictions, consistently outperforms traditional econometric models. We conclude that machine learning models can deliver more accurate and timely nowcasts of Malagasy economic performance and provide policymakers with additional guidance for data-driven decision making."}
{"Title":"How industrial clusters influence the growth of the regional GDP: A spatial-approach","Link":"https:\/\/arxiv.org\/abs\/2401.10261","Authors":"Vahidin Jeleskovic, Steffen Loeber","Abstract":"In this paper, we employ spatial econometric methods to analyze panel data from German NUTS 3 regions. Our goal is to gain a deeper understanding of the significance and interdependence of industry clusters in shaping the dynamics of GDP. To achieve a more nuanced spatial differentiation, we introduce indicator matrices for each industry sector which allows for extending the spatial Durbin model to a new version of it. This approach is essential due to both the economic importance of these sectors and the potential issue of omitted variables. Failing to account for industry sectors can lead to omitted variable bias and estimation problems. To assess the effects of the major industry sectors, we incorporate eight distinct branches of industry into our analysis. According to prevailing economic theory, these clusters should have a positive impact on the regions they are associated with. Our findings indeed reveal highly significant impacts, which can be either positive or negative, of specific sectors on local GDP growth. Spatially, we observe that direct and indirect effects can exhibit opposite signs, indicative of heightened competitiveness within and between industry sectors. Therefore, we recommend that industry sectors should be taken into consideration when conducting spatial analysis of GDP. Doing so allows for a more comprehensive understanding of the economic dynamics at play."}
{"Title":"Deep Generative Modeling for Financial Time Series with Application in VaR: A Comparative Review","Link":"https:\/\/arxiv.org\/abs\/2401.10370","Authors":"Lars Ericson, Xuejun Zhu, Xusi Han, Rao Fu, Shuang Li, Steve Guo, Ping Hu","Abstract":"In the financial services industry, forecasting the risk factor distribution conditional on the history and the current market environment is the key to market risk modeling in general and value at risk (VaR) model in particular. As one of the most widely adopted VaR models in commercial banks, Historical simulation (HS) uses the empirical distribution of daily returns in a historical window as the forecast distribution of risk factor returns in the next day. The objectives for financial time series generation are to generate synthetic data paths with good variety, and similar distribution and dynamics to the original historical data. In this paper, we apply multiple existing deep generative methods (e.g., CGAN, CWGAN, Diffusion, and Signature WGAN) for conditional time series generation, and propose and test two new methods for conditional multi-step time series generation, namely Encoder-Decoder CGAN and Conditional TimeVAE. Furthermore, we introduce a comprehensive framework with a set of KPIs to measure the quality of the generated time series for financial modeling. The KPIs cover distribution distance, autocorrelation and backtesting. All models (HS, parametric and neural networks) are tested on both historical USD yield curve data and additional data simulated from GARCH and CIR processes. The study shows that top performing models are HS, GARCH and CWGAN models. Future research directions in this area are also discussed."}
{"Title":"Dynamic Programming: Finite States","Link":"https:\/\/arxiv.org\/abs\/2401.10473","Authors":"Thomas J. Sargent, John Stachurski","Abstract":"This book is about dynamic programming and its applications in economics, finance, and adjacent fields. It brings together recent innovations in the theory of dynamic programming and provides applications and code that can help readers approach the research frontier. The book is aimed at graduate students and researchers, although most chapters are accessible to undergraduate students with solid quantitative backgrounds."}
{"Title":"Stylized Facts and Market Microstructure: An In-Depth Exploration of German Bond Futures Market","Link":"https:\/\/arxiv.org\/abs\/2401.10722","Authors":"Hamza Bodor, Laurent Carlier","Abstract":"This paper presents an in-depth analysis of stylized facts in the context of futures on German bonds. The study examines four futures contracts on German bonds: Schatz, Bobl, Bund and Buxl, using tick-by-tick limit order book datasets. It uncovers a range of stylized facts and empirical observations, including the distribution of order sizes, patterns of order flow, and inter-arrival times of orders. The findings reveal both commonalities and unique characteristics across the different futures, thereby enriching our understanding of these markets. Furthermore, the paper introduces insightful realism metrics that can be used to benchmark market simulators. The study contributes to the literature on financial stylized facts by extending empirical observations to this class of assets, which has been relatively underexplored in existing research. This work provides valuable guidance for the development of more accurate and realistic market simulators."}
{"Title":"An Experimental Study of Decentralized Matching","Link":"https:\/\/arxiv.org\/abs\/2401.10872","Authors":"Federico Echenique, Alejandro Robinson-Cort\u00e9s, Leeat Yariv","Abstract":"We present an experimental study of decentralized two-sided matching markets with no transfers. Experimental participants are informed of everyone's preferences and can make arbitrary non-binding match offers that get finalized when a period of market inactivity has elapsed. Several insights emerge. First, stable outcomes are prevalent. Second, while centralized clearinghouses commonly aim at implementing extremal stable matchings, our decentralized markets most frequently culminate in the median stable matching. Third, preferences' cardinal representations impact the stable partners participants match with. Last, the dynamics underlying our results exhibit strategic sophistication, with agents successfully avoiding cycles of blocking pairs."}
{"Title":"Application of Machine Learning in Stock Market Forecasting: A Case Study of Disney Stock","Link":"https:\/\/arxiv.org\/abs\/2401.10903","Authors":"Dengxin Huang","Abstract":"This document presents a stock market analysis conducted on a dataset consisting of 750 instances and 16 attributes donated in 2014-10-23. The analysis includes an exploratory data analysis (EDA) section, feature engineering, data preparation, model selection, and insights from the analysis. The Fama French 3-factor model is also utilized in the analysis. The results of the analysis are presented, with linear regression being the best-performing model."}
{"Title":"Forecasting Cryptocurrency Staking Rewards","Link":"https:\/\/arxiv.org\/abs\/2401.10931","Authors":"Sauren Gupta, Apoorva Hathi Katharaki, Yifan Xu, Bhaskar Krishnamachari, Rajarshi Gupta","Abstract":"This research explores a relatively unexplored area of predicting cryptocurrency staking rewards, offering potential insights to researchers and investors. We investigate two predictive methodologies: a) a straightforward sliding-window average, and b) linear regression models predicated on historical data. The findings reveal that ETH staking rewards can be forecasted with an RMSE within 0.7% and 1.1% of the mean value for 1-day and 7-day look-aheads respectively, using a 7-day sliding-window average approach. Additionally, we discern diverse prediction accuracies across various cryptocurrencies, including SOL, XTZ, ATOM, and MATIC. Linear regression is identified as superior to the moving-window average for perdicting in the short term for XTZ and ATOM. The results underscore the generally stable and predictable nature of staking rewards for most assets, with MATIC presenting a noteworthy exception."}
{"Title":"BioFinBERT: Finetuning Large Language Models (LLMs) to Analyze Sentiment of Press Releases and Financial Text Around Inflection Points of Biotech Stocks","Link":"https:\/\/arxiv.org\/abs\/2401.11011","Authors":"Valentina Aparicio, Daniel Gordon, Sebastian G. Huayamares, Yuhuai Luo","Abstract":"Large language models (LLMs) are deep learning algorithms being used to perform natural language processing tasks in various fields, from social sciences to finance and biomedical sciences. Developing and training a new LLM can be very computationally expensive, so it is becoming a common practice to take existing LLMs and finetune them with carefully curated datasets for desired applications in different fields. Here, we present BioFinBERT, a finetuned LLM to perform financial sentiment analysis of public text associated with stocks of companies in the biotechnology sector. The stocks of biotech companies developing highly innovative and risky therapeutic drugs tend to respond very positively or negatively upon a successful or failed clinical readout or regulatory approval of their drug, respectively. These clinical or regulatory results are disclosed by the biotech companies via press releases, which are followed by a significant stock response in many cases. In our attempt to design a LLM capable of analyzing the sentiment of these press releases,we first finetuned BioBERT, a biomedical language representation model designed for biomedical text mining, using financial textual databases. Our finetuned model, termed BioFinBERT, was then used to perform financial sentiment analysis of various biotech-related press releases and financial text around inflection points that significantly affected the price of biotech stocks."}
{"Title":"Long-term Effects of India's Childhood Immunization Program on Earnings and Consumption Expenditure: Comment","Link":"https:\/\/arxiv.org\/abs\/2401.11100","Authors":"David Roodman","Abstract":"Summan, Nandi, and Bloom (2023), hereafter SNB, find that exposure during infancy to India's Universal Immunization Programme (UIP) increased wages and per-capita household expenditure in early adulthood. SNB regress these outcomes on a treatment indicator that depends upon year and district of birth while controlling for age at follow-up. Because year of birth and age are nearly collinear, SNB's identifying variation does not come from the staggered introduction of the UIP, but rather from the progression of time during the follow-up period. Within the 12-month follow-up period, those interviewed later were more likely to have been treated and, on average, reported higher wages and household expenditure. Wages and household expenditure, however, rose by at least as much in a control group composed of people too old to have been exposed as infants to the UIP as in the treated group. SNB's results are best explained by inflation, economic growth, and non-random survey sequencing during the follow-up survey period."}
{"Title":"Data-driven Option Pricing","Link":"https:\/\/arxiv.org\/abs\/2401.11158","Authors":"Min Dai, Hanqing Jin, Xi Yang","Abstract":"We propose an innovative data-driven option pricing methodology that relies exclusively on the dataset of historical underlying asset prices. While the dataset is rooted in the objective world, option prices are commonly expressed as discounted expectations of their terminal payoffs in a risk-neutral world. Bridging this gap motivates us to identify a pricing kernel process, transforming option pricing into evaluating expectations in the objective world. We recover the pricing kernel by solving a utility maximization problem, and evaluate the expectations in terms of a functional optimization problem. Leveraging the deep learning technique, we design data-driven algorithms to solve both optimization problems over the dataset. Numerical experiments are presented to demonstrate the efficiency of our methodology."}
{"Title":"An income-based approach to modeling commuting distance in the Toronto area","Link":"https:\/\/arxiv.org\/abs\/2401.11343","Authors":"Shawn Berry","Abstract":"The purpose of this article is to propose a novel model of the effects of changes in shelter and driving costs on car commuting distances in the overheated Toronto housing market from 2011 to 2016. The model borrows from theoretical concepts of microeconomics and urban geography to examine the Toronto housing market. Using 2011 and 2016 Census data for census metropolitan areas (CMAs) and census agglomerations (CAs) in Southern Ontario and computed driving costs, the model of car commuting distance is based on variables of allocation of monthly household income to monthly shelter costs and driving costs as a function of the car driving distance to Toronto. Using this model, we can predict the effect on car commuting distance due to changes in any of the variables. The model also offers an explanation for communities of Toronto car commuters beyond a driving radius that we might expect for daily commuting. The model confirms that increases in shelter costs in the Toronto housing market from 2011 to 2016 have forced the boundaries of feasible housing locations outward, and forced households to move farther away, thus increasing car commuting distance."}
{"Title":"Fake Google restaurant reviews and the implications for consumers and restaurants","Link":"https:\/\/arxiv.org\/abs\/2401.11345","Authors":"Shawn Berry","Abstract":"The use of online reviews to aid with purchase decisions is popular among consumers as it is a simple heuristic tool based on the reported experiences of other consumers. However, not all online reviews are written by real consumers or reflect actual experiences, and present implications for consumers and businesses. This study examines the effects of fake online reviews written by artificial intelligence (AI) on consumer decision making. Respondents were surveyed about their attitudes and habits concerning online reviews using an online questionnaire (n=351), and participated in a restaurant choice experiment using varying proportions of fake and real reviews. While the findings confirm prior studies, new insights are gained about the confusion for consumers and consequences for businesses when reviews written by AI are believed rather than real reviews. The study presents a fake review detection model using logistic regression modeling to score and flag reviews as a solution."}
{"Title":"Analyzing the Impact of Financial Inclusion on Economic Growth in Bangladesh","Link":"https:\/\/arxiv.org\/abs\/2401.11585","Authors":"Ganapati Kumar Biswas","Abstract":"Financial inclusion is touted one of the principal drivers for economic growth for an economy. The study aims to explore the impact of financial inclusion on economic growth in Bangladesh. In my study, I used the number of loan accounts as the proxy for financial inclusion. Using time series data from spans from 2004-2021, the study revealed that there exists a long-run relationship between GDP, financial inclusion, and other macroeconomic variables in Bangladesh. The study also found that financial inclusion had a positive impact on economic growth of Bangladesh during the study period. Therefore, the policymakers and the central bank of Bangladesh as the apex authority of financial system should promote financial inclusion activities to achieve sustainable economic growth."}
{"Title":"The geometry of multi-curve interest rate models","Link":"https:\/\/arxiv.org\/abs\/2401.11619","Authors":"Claudio Fontana, Giacomo Lanaro, Agatha Murgoci","Abstract":"We study the problems of consistency and of the existence of finite-dimensional realizations for multi-curve interest rate models of Heath-Jarrow-Morton type, generalizing the geometric approach developed by T. Bj\u00f6rk and co-authors in the classical single-curve setting. We characterize when a multi-curve interest rate model is consistent with a given parameterized family of forward curves and spreads and when a model can be realized by a finite-dimensional state process. We illustrate the general theory in a number of model classes and examples, providing explicit constructions of finite-dimensional realizations. Based on these theoretical results, we perform the calibration of a three-curve Hull-White model to market data and analyse the stability of the estimated parameters."}
{"Title":"A Novel Decision Ensemble Framework: Customized Attention-BiLSTM and XGBoost for Speculative Stock Price Forecasting","Link":"https:\/\/arxiv.org\/abs\/2401.11621","Authors":"Riaz Ud Din, Salman Ahmed, Saddam Hussain Khan","Abstract":"Forecasting speculative stock prices is essential for effective investment risk management that drives the need for the development of innovative algorithms. However, the speculative nature, volatility, and complex sequential dependencies within financial markets present inherent challenges which necessitate advanced techniques. This paper proposes a novel framework, CAB-XDE (customized attention BiLSTM-XGB decision ensemble), for predicting the daily closing price of speculative stock Bitcoin-USD (BTC-USD). CAB-XDE framework integrates a customized bi-directional long short-term memory (BiLSTM) with the attention mechanism and the XGBoost algorithm. The customized BiLSTM leverages its learning capabilities to capture the complex sequential dependencies and speculative market trends. Additionally, the new attention mechanism dynamically assigns weights to influential features, thereby enhancing interpretability, and optimizing effective cost measures and volatility forecasting. Moreover, XGBoost handles nonlinear relationships and contributes to the proposed CAB-XDE framework robustness. Additionally, the weight determination theory-error reciprocal method further refines predictions. This refinement is achieved by iteratively adjusting model weights. It is based on discrepancies between theoretical expectations and actual errors in individual customized attention BiLSTM and XGBoost models to enhance performance. Finally, the predictions from both XGBoost and customized attention BiLSTM models are concatenated to achieve diverse prediction space and are provided to the ensemble classifier to enhance the generalization capabilities of CAB-XDE. The proposed CAB-XDE framework is empirically validated on volatile Bitcoin market, sourced from Yahoo Finance and outperforms state-of-the-art models with a MAPE of 0.0037, MAE of 84.40, and RMSE of 106.14."}
{"Title":"Forecasting and Backtesting Gradient Allocations of Expected Shortfall","Link":"https:\/\/arxiv.org\/abs\/2401.11701","Authors":"Takaaki Koike, Cathy W.S. Chen, Edward M.H. Lin","Abstract":"Capital allocation is a procedure for quantifying the contribution of each source of risk to aggregated risk. The gradient allocation rule, also known as the Euler principle, is a prevalent rule of capital allocation under which the allocated capital captures the diversification benefit of the marginal risk as a component of overall risk. This research concentrates on Expected Shortfall (ES) as a regulatory standard and focuses on the gradient allocations of ES, also called ES contributions (ESCs). We present the comprehensive treatment of backtesting the tuple of ESCs in the framework of the traditional and comparative backtests based on the concepts of joint identifiability and multi-objective elicitability. For robust forecast evaluation against the choice of scoring function, we also extend the Murphy diagram, a graphical tool to check whether one forecast dominates another under a class of scoring functions, to the case of ESCs. Finally, leveraging the recent concept of multi-objective elicitability, we propose a novel semiparametric model for forecasting dynamic ESCs based on a compositional regression model. In an empirical analysis of stock returns we evaluate and compare a variety of models for forecasting dynamic ESCs and demonstrate the outstanding performance of the proposed model."}
{"Title":"Market Responses to Genuine Versus Strategic Generosity: An Empirical Examination of NFT Charity Fundraisers","Link":"https:\/\/arxiv.org\/abs\/2401.12064","Authors":"Chen Liang, Murat Tunc, Gordon Burtch","Abstract":"Crypto donations now represent a significant fraction of charitable giving worldwide. Nonfungible token (NFT) charity fundraisers, which involve the sale of NFTs of artistic works with the proceeds donated to philanthropic causes, have emerged as a novel development in this space. A unique aspect of NFT charity fundraisers is the significant potential for donors to reap financial gains from the rising value of purchased NFTs. Questions may arise about the motivations of donors in these charity fundraisers, resulting in a negative social image. NFT charity fundraisers thus offer a unique opportunity to understand the economic consequences of a donor's social image. We investigate these effects in the context of a large NFT charity fundraiser. We identify the causal effect of purchasing an NFT within the charity fundraiser on a donor's later market outcomes by leveraging random variation in transaction processing times on the blockchain. Further, we demonstrate a clear pattern of heterogeneity, based on an individual's decision to relist (versus hold) the purchased charity NFTs (a sign of strategic generosity), and based on an individual's degree of social exposure within the NFT marketplace. We show that charity-NFT \"relisters\" experience significant penalties in the market, in terms of the prices they are able to command on other NFT listings, particularly among those who relist quickly and those who are more socially exposed. Our study underscores the growing importance of digital visibility and traceability, features that characterize crypto-philanthropy, and online philanthropy more broadly."}
{"Title":"Metrics matter, a Formal comment on Ward et al Plos-One 2016 paper : Is decoupling GDP growth from environmental impact possible?","Link":"https:\/\/arxiv.org\/abs\/2401.12100","Authors":"Herv\u00e9 Bercegol, Paul E. Brockway","Abstract":"The Ward et al. (2016) Plos-One paper is an important, heavily-cited paper in the decoupling literature. The authors present evidence of 1990-2015 growth in material and energy consumption and GDP at a world level, and for selected countries. They find only relative decoupling has occurred, leading to their central claim that future absolute decoupling is implausible. However, the authors have made two key errors in their collected data: GDP data is in current prices which includes inflation, and their global material use data is the total mass of fossil energy materials. Strictly, GDP data should be in constant prices to allow for its comparison over time, and material inputs to an economy should be the sum of mineral raw materials. Amending for these errors, we find much smaller levels of energy-GDP relative decoupling, and no materials-GDP decoupling at all at a global level. We check these new results by adding data for 1900-1990 to provide a longer time series, and find consistently low (and even no) levels of global relative decoupling of material use. The central claim for materials over the implausibility of future absolute decoupling therefore not only remains valid but is reinforced by the corrected datasets."}
{"Title":"Measures of the Capital Network of the U.S. Economy","Link":"https:\/\/arxiv.org\/abs\/2401.12118","Authors":"Ben Klemens","Abstract":"About two million U.S. corporations and partnerships are linked to each other and human investors by about 15 million owner-subsidiary links. Comparable social networks such as corporate board memberships and socially-built systems such as the network of Internet links are \"small worlds,\" meaning a network with a small diameter and link densities with a power-law distribution, but these properties had not yet been measured for the business entity network. This article shows that both inbound links and outbound links display a power-law distribution with a coefficient of concentration estimable to within a generally narrow confidence interval, overall, for subnetworks including only business entities, only for the great connected component of the network, and in subnetworks with edges associated with certain industries, for all years 2009-2021. In contrast to other networks with power-law distributed link densities, the network is mostly a tree, and has a diameter an order of magnitude larger than a small-world network with the same link distribution. The regularity of the power-law distribution indicates that its coefficient can be used as a new, well-defined macroeconomic metric for the concentration of capital flows in an economy. Economists might use it as a new measure of market concentration which is more comprehensive than measures based only on the few biggest firms. Comparing capital link concentrations across countries would facilitate modeling the relationship between business network characteristics and other macroeconomic indicators."}
{"Title":"Are Charter Value and Supervision Aligned? A Segmentation Analysis","Link":"https:\/\/arxiv.org\/abs\/2401.12274","Authors":"Juan Aparicio, Miguel A. Duran, Ana Lozano-Vivas, Jesus T. Pastor","Abstract":"Previous work suggests that the charter value hypothesis is theoretically grounded and empirically supported, but not universally. Accordingly, this paper aims to perform an analysis of the relations between charter value, risk taking, and supervision, taking into account the relations' complexity. Specifically, using the CAMELS rating system as a general framework for supervision, we study how charter value relates to risk and supervision by means of classification and regression tree analysis. The sample covers the period 2005-2016 and consists of listed banks in countries that were members of the Eurozone when it came into existence, along with Greece. To evaluate the crisis consequences, we also separately analyze four subperiods and countries that required financial aid from third parties and those that did not so, along with large and small banks. Our results reflect the complexity of the relations between charter value, supervision, and risk. Indeed, supervision and charter value seem aligned regarding only some types of risk"}
{"Title":"Pricing and Usage: An Empirical Analysis of Lines of Credit","Link":"https:\/\/arxiv.org\/abs\/2401.12301","Authors":"Miguel A. Duran","Abstract":"The hypothesis that committed revolving credit lines with fixed spreads can provide firms with interest rate insurance is a standard feature of models on these credit facilities' interest rate structure. Nevertheless, this hypothesis has not been tested. Its empirical examination is the main contribution of this paper. To perform this analysis, and given the unavailability of data, we hand-collect data on usage at the credit line level itself. The resulting dataset enables us also to take into account characteristics of credit lines that have been ignored by previous research. One of them is that credit lines can have simultaneously fixed and performance-based spreads."}
{"Title":"The Risk-Return Relation in the Corporate Loan Market","Link":"https:\/\/arxiv.org\/abs\/2401.12315","Authors":"Miguel A. Duran","Abstract":"This paper analyzes the hypothesis that returns play a risk-compensating role in the market for corporate revolving lines of credit. Specifically, we test whether borrower risk and the expected return on these debt instruments are positively related. Our main findings support this prediction, in contrast to the only previous work that examined this problem two decades ago. Nevertheless, we find evidence of mispricing regarding the risk of deteriorating firms using their facilities more intensively and during the subprime crisis."}
{"Title":"Bank Business Models, Size, and Profitability","Link":"https:\/\/arxiv.org\/abs\/2401.12323","Authors":"F. Bolivar, M. A. Duran, A. Lozano-Vivas","Abstract":"To examine the relation between profitability and business models (BMs) across bank sizes, the paper proposes a research strategy based on machine learning techniques. This strategy allows for analyzing whether size and profit performance underlie BM heterogeneity, with BM identification being based on how the components of the bank portfolio contribute to profitability. The empirical exercise focuses on the European Union banking system. Our results suggest that banks with analogous levels of performance and different sizes share strategic features. Additionally, high capital ratios seem compatible with high profitability if banks, relative to their size peers, adopt a standard retail BM."}
{"Title":"Business Model Contributions to Bank Profit Performance: A Machine Learning Approach","Link":"https:\/\/arxiv.org\/abs\/2401.12334","Authors":"F. Bolivar, Miguel A. Duran, A. Lozano-Vivas","Abstract":"This paper analyzes the relation between bank profit performance and business models. Using a machine learning-based approach, we propose a methodological strategy in which balance sheet components' contributions to profitability are the identification instruments of business models. We apply this strategy to the European Union banking system from 1997 to 2021. Our main findings indicate that the standard retail-oriented business model is the profile that performs best in terms of profitability, whereas adopting a non-specialized business profile is a strategic decision that leads to poor profitability. Additionally, our findings suggest that the effect of high capital ratios on profitability depends on the business profile. The contributions of business models to profitability decreased during the Great Recession. Although the situation showed signs of improvement afterward, the European Union banking system's ability to yield returns is still problematic in the post-crisis period, even for the best-performing group."}
{"Title":"New approximate stochastic dominance approaches for Enhanced Indexation models","Link":"https:\/\/arxiv.org\/abs\/2401.12669","Authors":"Francesco Cesarone, Justo Puerto","Abstract":"In this paper, we discuss portfolio selection strategies for Enhanced Indexation (EI), which are based on stochastic dominance relations. The goal is to select portfolios that stochastically dominate a given benchmark but that, at the same time, must generate some excess return with respect to a benchmark index. To achieve this goal, we propose a new methodology that selects portfolios using the ordered weighted average (OWA) operator, which generalizes previous approaches based on minimax selection rules and still leads to solving linear programming models. We also introduce a new type of approximate stochastic dominance rule and show that it implies the almost Second-order Stochastic Dominance (SSD) criterion proposed by Lizyayev and Ruszczynski (2012). We prove that our EI model based on OWA selects portfolios that dominate a given benchmark through this new form of stochastic dominance criterion. We test the performance of the obtained portfolios in an extensive empirical analysis based on real-world datasets. The computational results show that our proposed approach outperforms several SSD-based strategies widely used in the literature, as well as the global minimum variance portfolio."}
{"Title":"Generative AI Triggers Welfare-Reducing Decisions in Humans","Link":"https:\/\/arxiv.org\/abs\/2401.12773","Authors":"Fabian Dvorak, Regina Stumpf, Sebastian Fehrler, Urs Fischbacher","Abstract":"Generative artificial intelligence (AI) is poised to reshape the way individuals communicate and interact. While this form of AI has the potential to efficiently make numerous human decisions, there is limited understanding of how individuals respond to its use in social interaction. In particular, it remains unclear how individuals engage with algorithms when the interaction entails consequences for other people. Here, we report the results of a large-scale pre-registered online experiment (N = 3,552) indicating diminished fairness, trust, trustworthiness, cooperation, and coordination by human players in economic twoplayer games, when the decision of the interaction partner is taken over by ChatGPT. On the contrary, we observe no adverse welfare effects when individuals are uncertain about whether they are interacting with a human or generative AI. Therefore, the promotion of AI transparency, often suggested as a solution to mitigate the negative impacts of generative AI on society, shows a detrimental effect on welfare in our study. Concurrently, participants frequently delegate decisions to ChatGPT, particularly when the AI's involvement is undisclosed, and individuals struggle to discern between AI and human decisions."}
{"Title":"Reference-dependent asset pricing with a stochastic consumption-dividend ratio","Link":"https:\/\/arxiv.org\/abs\/2401.12856","Authors":"Luca De Gennaro Aquino, Xuedong He, Moris Simon Strub, Yuting Yang","Abstract":"We study a discrete-time consumption-based capital asset pricing model under expectations-based reference-dependent preferences. More precisely, we consider an endowment economy populated by a representative agent who derives utility from current consumption and from gains and losses in consumption with respect to a forward-looking, stochastic reference point. First, we consider a general model in which the agent's preferences include both contemporaneous gain-loss utility, that is, utility from the difference between current consumption and previously held expectations about current consumption, and prospective gain-loss utility, that is, utility from the difference between intertemporal beliefs about future consumption. A semi-closed form solution for equilibrium asset prices is derived for this case. We then specialize to a model in which the agent derives contemporaneous gain-loss utility only, obtaining equilibrium asset prices in closed form. Extensive numerical experiments show that, with plausible values of risk aversion and loss aversion, our models can generate equity premia that match empirical estimates. Interestingly, the models turn out to be consistent with some well-known empirical facts, namely procyclical variation in the price-dividend ratio and countercyclical variation in the conditional expected equity premium and in the conditional volatility of the equity premium. Furthermore, we find that prospective gain-loss utility is necessary for the model to predict reasonable values of the price-dividend ratio."}
{"Title":"Optimizing Transition Strategies for Small to Medium Sized Portfolios","Link":"https:\/\/arxiv.org\/abs\/2401.13126","Authors":"Nakul Upadhya, Alexandre Granzer-Guay","Abstract":"This work discusses the benefits of constrained portfolio turnover strategies for small to medium-sized portfolios. We propose a dynamic multi-period model that aims to minimize transaction costs and maximize terminal wealth levels whilst adhering to strict portfolio turnover constraints. Our results demonstrate that using our framework in combination with a reasonable forecast, can lead to higher portfolio values and lower transaction costs on average when compared to a naive, single-period model. Such results were maintained given different problem cases, such as, trading horizon, assets under management, wealth levels, etc. In addition, the proposed model lends itself to a reformulation that makes use of the column generation algorithm which can be strategically leveraged to reduce complexity and solving times."}
{"Title":"Environmental impacts, nutritional profiles, and retail prices of commonly sold retail food items in 181 countries: an observational study","Link":"https:\/\/arxiv.org\/abs\/2401.13159","Authors":"Elena M. Martinez, Nicole Tichenor Blackstone, Parke E. Wilde, Anna W. Herforth, William A. Masters","Abstract":"Affordability is often seen as a barrier to consuming sustainable diets. This study provides the first worldwide test of how retail food prices relate to empirically estimated environmental impacts and nutritional profile scores between and within food groups. We use prices for 811 retail food items commonly sold in 181 countries during 2011 and 2017, matched to estimated carbon and water footprints and nutritional profiles, to test whether healthier and more environmentally sustainable foods are more expensive between and within food groups. We find that within almost all groups, less expensive items have significantly lower carbon and water footprints. Associations are strongest for animal source foods, where each 10% lower price is associated with 20 grams lower CO2-equivalent carbon and 5 liters lower water footprint per 100kcal. Gradients between price and nutritional profile vary by food group, price range, and nutritional attribute. In contrast, lower-priced items have lower nutritional value in only some groups over some price ranges, and that relationship is sometimes reversed. These findings reveal opportunities to reduce financial and environmental costs of diets, contributing to transitions towards healthier, more environmentally sustainable food systems."}
{"Title":"An Explicit Scheme for Pathwise XVA Computations","Link":"https:\/\/arxiv.org\/abs\/2401.13314","Authors":"Lokman Abbas-Turki (LPSM), St\u00e9phane Cr\u00e9pey (LPSM), Botao Li (LPSM), Bouazza Saadeddine (LPSM, LaMME)","Abstract":"Motivated by the equations of cross valuation adjustments (XVAs) in the realistic case where capital is deemed fungible as a source of funding for variation margin, we introduce a simulation\/regression scheme for a class of anticipated BSDEs, where the coefficient entails a conditional expected shortfall of the martingale part of the solution. The scheme is explicit in time and uses neural network least-squares and quantile regressions for the embedded conditional expectations and expected shortfall computations. An a posteriori Monte Carlo validation procedure allows assessing the regression error of the scheme at each time step. The superiority of this scheme with respect to Picard iterations is illustrated in a high-dimensional and hybrid market\/default risks XVA use-case."}
{"Title":"Real-time Risk Metrics for Programmatic Stablecoin Crypto Asset-Liability Management (CALM)","Link":"https:\/\/arxiv.org\/abs\/2401.13399","Authors":"Marcel Bluhm (1), Adrian Cachinero Vasiljevi\u0107 (2), S\u00e9bastien Derivaux (2), S\u00f8ren Terp H\u00f8rl\u00fcck Jessen (3) ((1) The Block, (2) Steakhouse Financial Limited, (3) Balloonist ApS)","Abstract":"Stablecoins have turned out to be the \"killer\" use case of the growing digital asset space. However, risk management frameworks, including regulatory ones, have been largely absent. In this paper, we address the critical question of measuring and managing risk in stablecoin protocols, which operate on public blockchain infrastructure. The on-chain environment makes it possible to monitor risk and automate its management via transparent smart-contracts in real-time. We propose two risk metrics covering capitalization and liquidity of stablecoin protocols. We then explore in a case-study type analysis how our risk management framework can be applied to DAI, the biggest decentralized stablecoin by market capitalisation to-date, governed by MakerDAO. Based on our findings, we recommend that the protocol explores implementing automatic capital buffer adjustments and dynamic maturity gap matching. Our analysis demonstrates the practical benefits for scalable (prudential) risk management stemming from real-time availability of high-quality, granular, tamper-resistant on-chain data in the digital asset space. We name this approach Crypto Asset-Liability Management (CALM)."}
{"Title":"\"The Roller Conduction Effect\" from the A-share Data Evidence","Link":"https:\/\/arxiv.org\/abs\/2401.13670","Authors":"Wenbo Lyu","Abstract":"In the post-epidemic era, consumption recovery has obvious time and space transmission laws, and there are different valuation criteria for consumption segments. Using the A-share data of the consumption recovery stage from January to April 2022, this paper quantitatively compares the rotation effect between different consumption sectors when the valuation returns to the reasonable range. According to the new classification of \"sensory-based consumption\", it interprets the internal logic of digital consumption as A consumption upgrade tool and a higher valuation target, and expounds the \"the roller conduction effect\". The law of consumption recovery and valuation return period is explained from the perspective of time and space conduction. The study found that in the early stage of consumption recovery, the recovery of consumer confidence was slow. In this period, A-shares were mainly dominated by the stock capital game, and there was an obvious plate rotation law in the game. Being familiar with this law has strong significance, which not only helps policy makers to adjust the direction of policy guidance, but also helps financial investors to make better investment strategies. The disadvantage of this paper is that it has not yet studied the roller conduction effect of the global financial market, and more rigorous mathematical models are still needed to support the definition of stock funds, which is also the main direction of the author's future research."}
{"Title":"Determinants of renewable energy consumption in Madagascar: Evidence from feature selection algorithms","Link":"https:\/\/arxiv.org\/abs\/2401.13671","Authors":"Franck Ramaharo, Fitiavana Randriamifidy","Abstract":"The aim of this note is to identify the factors influencing renewable energy consumption in Madagascar. We tested 12 features covering macroeconomic, financial, social, and environmental aspects, including economic growth, domestic investment, foreign direct investment, financial development, industrial development, inflation, income distribution, trade openness, exchange rate, tourism development, environmental quality, and urbanization. To assess their significance, we assumed a linear relationship between renewable energy consumption and these features over the 1990-2021 period. Next, we applied different machine learning feature selection algorithms classified as filter-based (relative importance for linear regression, correlation method), embedded (LASSO), and wrapper-based (best subset regression, stepwise regression, recursive feature elimination, iterative predictor weighting partial least squares, Boruta, simulated annealing, and genetic algorithms) methods. Our analysis revealed that the five most influential drivers stem from macroeconomic aspects. We found that domestic investment, foreign direct investment, and inflation positively contribute to the adoption of renewable energy sources. On the other hand, industrial development and trade openness negatively affect renewable energy consumption in Madagascar."}
{"Title":"Sacred Ecology: The Environmental Impact of African Traditional Religions","Link":"https:\/\/arxiv.org\/abs\/2401.13673","Authors":"Neha Deopa, Daniele Rinaldo","Abstract":"Do religions codify ecological principles? This paper explores theoretically and empirically the role religious beliefs play in shaping environmental interactions. We study African Traditional Religions (ATR) which place forests within a sacred sphere. We build a model of non-market interactions of the mean-field type where the actions of agents with heterogeneous religious beliefs continuously affect the spatial density of forest cover. The equilibrium extraction policy shows how individual beliefs and their distribution among the population can be a key driver of forest conservation. The model also characterizes the role of resource scarcity in both individual and population extraction decisions. We test the model predictions empirically relying on the unique case of Benin, where ATR adherence is freely reported. Using an instrumental variable strategy that exploits the variation in proximity to the Benin-Nigerian border, we find that a 1 standard deviation increase in ATR adherence has a 0.4 standard deviation positive impact on forest cover change. We study the impact of historically belonging to the ancient Kingdom of Dahomey, birthplace of the Vodun religion. Using the original boundaries as a spatial discontinuity, we find positive evidence of Dahomey affiliation on contemporary forest change. Lastly, we compare observed forest cover to counterfactual outcomes by simulating the absence of ATR beliefs across the population."}
{"Title":"Analisis de la incidencia de la inversion extranjera directa y la inversion nacional, en el crecimiento economico de Chile","Link":"https:\/\/arxiv.org\/abs\/2401.13674","Authors":"Alvear Guzman Katherine, Campozano Buele Jenner, Duran Canarte Paulette, Holguin Cedeno Roger, Mejia Crespin Fernando","Abstract":"The research aims to assess the impact of foreign direct investment (FDI) and domestic investment on Chile's economic growth. By elucidating the relationship between FDI and domestic investment, the study contributes valuable insights for economic policy formulation and future investments. The findings hold significance in shaping Chile's international perception as an investment destination, potentially influencing its standing in the global economic landscape. Demonstrating that FDI is a significant driver of economic growth could enhance confidence among foreign investors. The project's importance lies in contributing to economic knowledge and guiding strategic decisions for sustainable economic growth in Chile. Understanding the interplay of FDI and domestic investment allows for a balanced approach, promoting stable economic development and mitigating issues like excessive reliance on foreign investment. The study highlights the theory of internationalization as a conceptual framework for understanding the motives and strategies of multinational companies investing abroad. Leveraging data from sources like the Central Bank of Chile, the research analyzes variables such as Chile's economic growth (GDP), FDI, and domestic investment. The hypothesis posits a significant long-term causal relationship between FDI, National Investment (NI), and Chile's Economic Growth (GDP). Statistical analysis using the Eviews 6 software tool confirms that attracting foreign investments and promoting internal investment are imperative for sustainable economic growth in Chile."}
{"Title":"Social costs of curcular economy in European Union","Link":"https:\/\/arxiv.org\/abs\/2401.13675","Authors":"Shteryo Nozharov","Abstract":"Two fundamental issues are incorporated in the present monograph: the issue related to the quantification of the social costs and the issue, related to the defining of the circular economy concept as a theoretical model. The analysis is based on the methodology of the new institutional economics, which fact distinguishes it from the many other circular economy analysis based on the neo-classical methodological apparatus."}
{"Title":"The impact of Hong Kong's anti-ELAB movement on political related firms","Link":"https:\/\/arxiv.org\/abs\/2401.13676","Authors":"Ziqi Wang","Abstract":"Hong Kong's anti-ELAB movement had a significant impact on the stock market the stock price of listed companies. Using the number of protestors as the measurement of daily protesting intensity from 2019\/6\/6 to 2020\/1\/17, this paper documents that the stock price of listed companies associated with the pan-democratic parties were more negatively affected by protesting than other companies. Furthermore, this paper finds that after the implementation of the anti-mask law, protesting had a positive impact on red chips but a negative impact on companies related to pan-democracy parties. Therefore, this paper believes that after the central government and the HKSAR government adopted strict measures to stop violence and chaos, the value of the political connection of red chips became positive while the value of the connection with pan-democracy parties became negative."}
{"Title":"I Can't Go to Work Tomorrow! Work-Family Policies, Well-Being and Absenteeism","Link":"https:\/\/arxiv.org\/abs\/2401.13678","Authors":"Jose Aurelio Medina-Garrido, Jose Maria Biedma-Ferrer, Jaime Sanchez-Ortiz","Abstract":"Among the main causes of absenteeism are health problems, emotional problems, and inadequate work-family policies (WFP). This paper analyses the impact of the existence and accessibility of WFP on work absenteeism, by considering the mediating role of the well-being, which includes emotional as well as physical or health problems, that is generated by these policies. We differentiate between the existence of the WFP and its accessibility, as the mere existence of the WFP in an organisation is not enough. Additionally, workers must be able to access these policies easily and without retaliation of any kind. The model includes the hierarchy and the gender as moderating variables. To test the proposed hypotheses, a structural equation model based on the partial least squares structural equation modelling (PLS-SEM) approach is applied to a sample of employees in the service sector in Spain. On the one hand, the findings show that the existence of WFP has no direct effect on absenteeism; however, accessibility to these policies does have a direct effect on absenteeism. On the other hand, both the existence and accessibility of WFP have positive direct effects on emotional well-being. In addition, emotional well-being is positively related to physical well-being which, in turn, promotes a reduction in absenteeism. Finally, significant differences in the relationship between the existence of WFP and emotional well-being confirm the special difficulty of female managers in reconciling family life and work life."}
{"Title":"Determinants of the Propensity for Innovation among Entrepreneurs in the Tourism Industry","Link":"https:\/\/arxiv.org\/abs\/2401.13679","Authors":"Miguel Angel Montanes-Del-Rio, Jose Aurelio Medina-Garrido","Abstract":"Tourism's increasing share of Gross Domestic Product throughout the world, its impact on employment and its continuous growth justifies the interest it raises amongst entrepreneurs and public authorities. However, this growth coexists with intense competition; as a result of which, tourism companies must continuously innovate in order to survive and grow. This is evident in the diversification of tourism products and destinations, the improvement of business processes and the incorporation of new technologies for intermediation, amongst other examples. This paper expounds on the factors that explain the propensity for innovation amongst tourism entrepreneurs and it may help governments to promote innovation that is based on those determining factors. The hypotheses are tested using a logistic regression on 699 international tourism entrepreneurs, taken from the 2014 Global Adult Population Survey of the Global Entrepreneurship Monitor project. The propensity for innovation amongst tourism entrepreneurs has a statistically significant relationship to gender, age, level of education and informal investments in previous businesses."}
{"Title":"Moderating effects of gender and family responsibilities on the relations between work-family policies and job performance","Link":"https:\/\/arxiv.org\/abs\/2401.13681","Authors":"Jose Aurelio Medina-Garrido, Jose Maria Biedma-Ferrer, Antonio Rafael Ramos-Rodriguez","Abstract":"This study analyzes the impact of work-family policies (WFP) on job performance, and the possible moderating role of gender and family responsibilities. Hypothesis testing was performed using a structural equation model based on a PLS-SEM approach applied to a sample of 1,511 employees of the Spanish banking sector. The results show that neither the existence nor the accessibility of the WFP has a direct, positive impact on performance, unlike what we expected, but both have an indirect effect via the well-being generated by these policies. We also find that neither gender nor family responsibilities have a significant moderating role on these relations, contrary to what we initially expected."}
{"Title":"Why not now? Intended timing in entrepreneurial intentions","Link":"https:\/\/arxiv.org\/abs\/2401.13682","Authors":"Antonio Rafael Ramos-Rodriguez, Jose Aurelio Medina-Garrido, Jose Ruiz-Navarro","Abstract":"Purpose: Understanding the formation of entrepreneurial intentions is critical, given that it is the first step in the entrepreneurial process. Although entrepreneurial intention has been extensively studied, little attention has been paid on the intended timing of future entrepreneurial projects. This paper analyses entrepreneurial intentions among final-year university students after graduation in terms of the timeframe to start a business. Potentially rapid entrepreneurs and entrepreneurs-in-waiting were compared using the Theory of Planned Behaviour (TPB). Methodology: A variance-based structural equation modelling approach was used for the sample of 851 final-year university students with entrepreneurial intentions who participated in GUESSS project. Findings: The results obtained contribute to the understanding of how entrepreneurial intentions are formed, particularly, how intended timing plays a moderating role in the relationships of the variables of the theoretical model of TPB. This study provides empirical evidence that significant differences exist between potential rapid entrepreneurs and entrepreneurs-in-waiting. Practical implications: The findings of this study have practical implications for entrepreneurship education, and they can help policy makers develop more effective policies and programs to promote entrepreneurship. Originality: Intention-based models have traditionally examined the intent -- but not the timing -- of new venture creation. However, the time elapsed between the formation of the entrepreneurial intent and the identification of a business opportunity can vary considerably. Therefore, analysing the moderating role of intended timing could be relevant to entrepreneurial intention research."}
{"Title":"Relationship between work-family balance, employee well-being and job performance","Link":"https:\/\/arxiv.org\/abs\/2401.13683","Authors":"Jose Aurelio Medina-Garrido, Jose Maria Biedma-Ferrer, Antonio Rafael Ramos-Rodriguez","Abstract":"Purpose: To assess the impact of the existence of and access to different work-family policies on employee well-being and job performance.\nDesign-methodology-approach: Hypothesis testing was performed using a structural equation model based on a PLS-SEM approach applied to a sample of 1,511 employees of the Spanish banking sector.\nFindings: The results obtained demonstrate that the existence and true access to different types of work-family policies such as flexible working hours (flexi-time), long leaves, and flexible work location (flexi-place) are not directly related to job performance, but indirectly so, when mediated by the well-being of employees generated by work-family policies. In a similar vein, true access to employee and family support services also has an indirect positive impact on job performance mediated by the well-being produced. In contrast, the mere existence of employee and family support services does not have any direct or indirect effect on job performance.\nOriginality-value: This study makes a theoretical and empirical contribution to better understand the impact that of the existence of and access to work-family policies on job performance mediated by employee well-being. In this sense, we posited and tested an unpublished theoretical model where the concept of employee well-being gains special relevance at academic and organizational level due to its implications for human resource management."}
{"Title":"Global Entrepreneurship Monitor versus Panel Study of Entrepreneurial Dynamics: comparing their intellectual structures","Link":"https:\/\/arxiv.org\/abs\/2401.13684","Authors":"Antonio Rafael Ramos-Rodriguez, Salustiano Martinez-Fierro, Jose Aurelio Medina-Garrido, Jose Ruiz-Navarro","Abstract":"In the past 15 years, two international observatories have been intensively studying entrepreneurship using empirical studies with different methodologies: GEM and PSED. Both projects have generated a considerable volume of scientific production, and their intellectual structures are worth analyzing. The current work is an exploratory study of the knowledge base of the articles generated by each of these two observatories and published in prestigious journals. The value added of this work lies in its novel characterization of the intellectual structure of entrepreneurship according to the academic production of these two initiatives. The results may be of interest to the managers and members of these observatories, as well as to academics, researchers, sponsors and policymakers interested in entrepreneurship."}
{"Title":"Determinants of Hotels and Restaurants entrepreneurship: A study using GEM data","Link":"https:\/\/arxiv.org\/abs\/2401.13685","Authors":"Antonio Rafael Ramos-Rodriguez, Jose Aurelio Medina-Garrido, Jose Ruiz-Navarro","Abstract":"The objective of this work is to assess the influence of certain factors on the likelihood of being a Hotels and Restaurants (H&R) entrepreneur. The factors evaluated are demographic and economic variables, variables related to perceptions of the environment and personal traits, and variables measuring the individual's intellectual and social capital. The work uses logistic regression techniques to analyze a sample of 33,711 individuals in the countries participating in the GEM project in 2008. The findings show that age, gender, income, perception of opportunities, fear of failure, entrepreneurial ability, knowing other entrepreneurs and being a business angel are explanatory factors of the probability of being an H&R entrepreneur."}
{"Title":"Capturing the Tax-Revenue Bracketing System via a predator-prey model: Evidence from South Africa","Link":"https:\/\/arxiv.org\/abs\/2401.13686","Authors":"Leonard Mushunje","Abstract":"Revenues obtained from the corporate tax heads play significant roles in any economy as they can be prioritized for producing public goods and employment creations, among others. As such, corporate tax revenue should be paid enough attention. This study, therefore, explores the tax-revenue harvesting system of an economy where we focused on the corporate tax head. The system comprises three players; the government and formal and informal firms. We applied the predator-prey model to model the effect of the government-gazetted tax rate on corporate survivability. It is a new approach to modeling economic system relations and games. Critical combinatory points are derived, with stability analysis provided after that. Dynamics associated with the tax-revenue system are established and critically analyzed. Lastly, we provide the mathematical way the system can be optimized for the government to harvest as much Revenue as possible, including optimal conditions."}
{"Title":"Econometric Approach to Analyzing Determinants of Sustained Prosperity","Link":"https:\/\/arxiv.org\/abs\/2401.13687","Authors":"Anika Dixit","Abstract":"Every year, substantial resources are allocated to foreign aid with the aim of catalyzing prosperity and development in recipient countries. The diverse body of research on the relationship between aid and gross domestic product (GDP) has yielded varying results, finding evidence of both positive, negative, and negligible associations between the two. This study employs econometric techniques, namely Fully Modified Ordinary Least Squares Regression (FMOLS) and the Generalized Method of Moments (GMM), to explore the intricate links between innovation and different types of official development assistance (ODA) with the overarching construct of prosperity. The paper also reviews the linkages between foundational metrics, such as the rule of law, education, and economic infrastructure and services, in enabling self-sustaining prosperity. Drawing upon panel data of relevant determinants for 74 countries across the years 2013 to 2021, the study found that there was a negligible relationship between both ODA and innovation indices with prosperity. Notably, foreign aid targeted specifically toward education was observed to have a positive impact on prosperity, as was the presence of rule of law in a state. The results of the study are then examined through the lens of a case-study on Reliance Jio, exemplifying how the company engineered an ecosystem that harnessed resources and facilitated infrastructure development, thereby contributing to self-sustaining economic growth and prosperity in India."}
{"Title":"In the Aftermath of Oil Prices Fall of 2014\/2015-Socioeconomic Facts and Changes in the Public Policies in the Sultanate of Oman","Link":"https:\/\/arxiv.org\/abs\/2401.13688","Authors":"Osama A. Marzouk","Abstract":"Since the start of its national renaissance in 1970, the Sultanate of Oman (Oman) has gone over a major development in several areas, such as education, infrastructure, and urbanization. This has been powered by the revenues from exporting crude oil and natural gas, which together form the skeleton of the country's economy. In the second half of 2014, the oil prices declined strongly to about 50% of its price. This was followed by another moderate decline in the second half of 2015 and the beginning of 2016, leaving the barrel price at a low level below 30 US in January 2016 (as compared to above 110 US in June 2014). This drop had direct impacts on the economy of Oman, manifested in a large budget deficit, reduced governmental expenditure, reduced or cancelled subsidy of fuels and electricity, increase in the water tariff, and decline in deposits in banks. The country is coping with this through its 9th five-year plan (2016-2020), which adopts a strategy of diversifying the income and relying less on the traditional oil and gas sector. The country has also taken measures to facilitate private businesses. This article sheds light on these topics as well as miscellaneous data about Oman."}
{"Title":"The Arrival of Fast Internet and Employment in Africa: Comment","Link":"https:\/\/arxiv.org\/abs\/2401.13694","Authors":"David Roodman","Abstract":"Hjort and Poulsen (2019) frames the staggered arrival of submarine Internet cables on the shores of Africa circa 2010 as a difference-in-differences natural experiment. The paper finds positive impacts of broadband on individual- and firm-level employment and nighttime light emissions. These results largely are not robust to alternative geocoding of survey locations, to correcting for a satellite changeover at end-2009, and to revisiting a definition of the treated zone that has no clear technological basis, is narrower than the spatial resolution of nearly all the data sources, and is empirically suboptimal as a representation of the geography of broadband."}
{"Title":"Discrete Hawkes process with flexible residual distribution and filtered historical simulation","Link":"https:\/\/arxiv.org\/abs\/2401.13890","Authors":"Kyungsub Lee","Abstract":"We introduce a new model which can be considered as a extended version of the Hawkes process in a discrete sense. This model enables the integration of various residual distributions while preserving the fundamental properties of the original Hawkes process. The rich nature of this model enables a filtered historical simulation which incorporate the properties of original time series more accurately. The process naturally extends to multi-variate models with easy implementations of estimation and simulation. We investigate the effect of flexible residual distribution on estimation of high frequency financial data compared with the Hawkes process."}
{"Title":"Higher order approximation of option prices in Barndorff-Nielsen and Shephard models","Link":"https:\/\/arxiv.org\/abs\/2401.14390","Authors":"\u00c1lvaro Guinea Juli\u00e1, Alet Roux","Abstract":"We present an approximation method based on the mixing formula (Hull & White 1987, Romano & Touzi 1997) for pricing European options in Barndorff-Nielsen and Shephard models. This approximation is based on a Taylor expansion of the option price. It is implemented using a recursive algorithm that allows us to obtain closed form approximations of the option price of any order (subject to technical conditions on the background driving L\u00e9vy process). This method can be used for any type of Barndorff-Nielsen and Shephard stochastic volatility model. Explicit results are presented in the case where the stationary distribution of the background driving L\u00e9vy process is inverse Gaussian or gamma. In both of these cases, the approximation compares favorably to option prices produced by the characteristic function. In particular, we also perform an error analysis of the approximation, which is partially based on the results of Das & Langren\u00e9 (2022). We obtain asymptotic results for the error of the N^{\\text{th}} order approximation and error bounds when the variance process satisfies an inverse Gaussian Ornstein-Uhlenbeck process or a gamma Ornstein-Uhlenbeck process."}
{"Title":"Islamic Law, Western European Law and the Roots of Middle East's Long Divergence: a Comparative Empirical Investigation (800-1600)","Link":"https:\/\/arxiv.org\/abs\/2401.14435","Authors":"Hans-Bernd Schaefer, Rok Spruk","Abstract":"We examine the contribution of Islamic legal institutions to the comparative economic decline of the Middle East behind Latin Europe, which can be observed since the late Middle Ages. To this end, we explore whether the sacralization of Islamic law and its focus on the Sharia as supreme, sacred and unchangeable legal text, which reached its culmination in the 13th century had an impact on economic development. We use the population size of 145 cities in Islamic countries and 648 European cities for the period 800-1800 as proxies for the level of economic development, and construct novel estimates of the number of law schools (i.e. madaris) and estimate their contribution to the pre-industrial economic development. Our triple-differences estimates show that a higher density of madrasas before the sacralization of Islamic law predicts a more vibrant urban economy characterized by higher urban growth. After the consolidation of the sharia sacralization of law in the 13th century, greater density of law schools is associated with stagnating population size. We show that the economic decline of the Middle East can be partly explained by the absence of legal innovations or substitutes of them, which paved the way for the economic rise of Latin Europe, where ground-breaking legal reforms introduced a series of legal innovations conducive for economic growth. We find that the number of learned lawyers trained in universities with law schools is highly and positively correlated with the western European city population. Our counterfactual estimates show that almost all Islamic cities under consideration would have had much larger size by the year 1700 if legal innovations comparable to those in Western Europe were introduced. By making use of a series of synthetic control and difference-in-differences estimators our findings are robust against a large number of model specification checks."}
{"Title":"Cash non-additive risk measures: horizon risk and generalized entropy","Link":"https:\/\/arxiv.org\/abs\/2401.14443","Authors":"Giulia Di Nunno, Emanuela Rosazza Gianin","Abstract":"Horizon risk (see arXiv:2301.04971) is studied in the context of cash non-additive fully-dynamic risk measures induced by BSDEs. Furthermore, we introduce a risk measure based on generalized Tsallis entropy which can dynamically evaluate the riskiness of losses considering both horizon risk and interest rate uncertainty. The new q-entropic risk measure on losses can be used as a quantification of capital requirement."}
{"Title":"Analysis of an aggregate loss model in a Markov renewal regime","Link":"https:\/\/arxiv.org\/abs\/2401.14553","Authors":"Pepa Ram\u00edrez-Cobo, Emilio Carrizosa, Rosa Elvira Lillo","Abstract":"In this article we consider an aggregate loss model with dependent losses. The losses occurrence process is governed by a two-state Markovian arrival process (MAP2), a Markov renewal process process that allows for (1) correlated inter-losses times, (2) non-exponentially distributed inter-losses times and, (3) overdisperse losses counts. Some quantities of interest to measure persistence in the loss occurrence process are obtained. Given a real operational risk database, the aggregate loss model is estimated by fitting separately the inter-losses times and severities. The MAP2 is estimated via direct maximization of the likelihood function, and severities are modeled by the heavy-tailed, double-Pareto Lognormal distribution. In comparison with the fit provided by the Poisson process, the results point out that taking into account the dependence and overdispersion in the inter-losses times distribution leads to higher capital charges."}
{"Title":"Optimal portfolio under ratio-type periodic evaluation in incomplete markets with stochastic factors","Link":"https:\/\/arxiv.org\/abs\/2401.14672","Authors":"Wenyuan Wang, Kaixin Yan, Xiang Yu","Abstract":"This paper studies a type of periodic utility maximization for portfolio management in an incomplete market model, where the underlying price diffusion process depends on some external stochastic factors. The portfolio performance is periodically evaluated on the relative ratio of two adjacent wealth levels over an infinite horizon. For both power and logarithmic utilities, we formulate the auxiliary one-period optimization problems with modified utility functions, for which we develop the martingale duality approach to establish the existence of the optimal portfolio processes and the dual minimizers can be identified as the \"least favorable\" completion of the market. With the help of the duality results in the auxiliary problems and some fixed point arguments, we further derive and verify the optimal portfolio processes in a periodic manner for the original periodic evaluation problems over an infinite horizon."}
{"Title":"How to Use Data Science in Economics -- a Classroom Game Based on Cartel Detection","Link":"https:\/\/arxiv.org\/abs\/2401.14757","Authors":"Hannes Wallimann, Silvio Sticher","Abstract":"We present a classroom game that integrates economics and data-science competencies. In the first two parts of the game, participants assume the roles of firms in a procurement market, where they must either adopt competitive behaviors or have the option to engage in collusion. Success in these parts hinges on their comprehension of market dynamics. In the third part of the game, participants transition to the role of competition-authority members. Drawing from recent literature on machine-learning-based cartel detection, they analyze the bids for patterns indicative of collusive (cartel) behavior. In this part of the game, success depends on data-science skills. We offer a detailed discussion on implementing the game, emphasizing considerations for accommodating diverging levels of preexisting knowledge in data science."}
{"Title":"ESG driven pairs algorithm for sustainable trading: Analysis from the Indian market","Link":"https:\/\/arxiv.org\/abs\/2401.14761","Authors":"Eeshaan Dutta, Sarthak Diwan, Siddhartha P. Chakrabarty","Abstract":"This paper proposes an algorithmic trading framework integrating Environmental, Social, and Governance (ESG) ratings with a pairs trading strategy. It addresses the demand for socially responsible investment solutions by developing a unique algorithm blending ESG data with methods for identifying co-integrated stocks. This allows selecting profitable pairs adhering to ESG principles. Further, it incorporates technical indicators for optimal trade execution within this sustainability framework. Extensive back-testing provides evidence of the model's effectiveness, consistently generating positive returns exceeding conventional pairs trading strategies, while upholding ESG principles. This paves the way for a transformative approach to algorithmic trading, offering insights for investors, policymakers, and academics."}
{"Title":"Free public transport to the destination: A causal analysis of tourists' travel mode choice","Link":"https:\/\/arxiv.org\/abs\/2401.14945","Authors":"Kevin Bl\u00e4ttler, Hannes Wallimann, Widar von Arx","Abstract":"In this paper, we assess the impact of a fare-free public transport policy for overnight guests on travel mode choice to a Swiss tourism destination. The policy directly targets domestic transport to and from a destination, the substantial contributor to the CO2 emissions of overnight trips. Based on a survey sample, we identify the effect with the help of the random element that the information on the offer from a hotelier to the guest varies in day-to-day business. We estimate a shift from private cars to public transport due to the policy of, on average, 14.8 and 11.6 percentage points, depending on the application of propensity score matching and causal forest. This knowledge is relevant for policy-makers to design future offers that include more sustainable travels to a destination. Overall, our paper exemplifies how such an effect of comparable natural experiments in the travel and tourism industry can be properly identified with a causal framework and underlying assumptions."}
{"Title":"Sustainable Market Incentives -- Lessons from European Feebates for a ZEV Future","Link":"https:\/\/arxiv.org\/abs\/2401.15069","Authors":"Aditya Ramji, Daniel Sperling, Lewis Fulton","Abstract":"Strong policies with sustainable incentives are needed to accelerate the EV transition. This paper assesses various feebate designs assessing recent policy evolution in five European countries. While there are key design elements that should be considered, there is no optimal feebate design. Different policy objectives could be served by feebates influencing its design and effectiveness. Using feebates to transition to EVs has emerged a key objective. With the financial sustainability of EV incentive programs being questioned, a self financing market mechanism could be the need of the hour solution. Irrespective of the policy goals, a feebate will impact both the supply side, i.e., the automotive industry and the consumer side. Globally, feebates can be used to effect technology leapfrogging while navigating the political economy of clean transportation policy in different country contexts. This paper highlights thirteen design elements of an effective feebate policy that can serve as a foundation for policymakers."}
{"Title":"FDR-Controlled Portfolio Optimization for Sparse Financial Index Tracking","Link":"https:\/\/arxiv.org\/abs\/2401.15139","Authors":"Jasin Machkour, Daniel P. Palomar, Michael Muma","Abstract":"In high-dimensional data analysis, such as financial index tracking or biomedical applications, it is crucial to select the few relevant variables while maintaining control over the false discovery rate (FDR). In these applications, strong dependencies often exist among the variables (e.g., stock returns), which can undermine the FDR control property of existing methods like the model-X knockoff method or the T-Rex selector. To address this issue, we have expanded the T-Rex framework to accommodate overlapping groups of highly correlated variables. This is achieved by integrating a nearest neighbors penalization mechanism into the framework, which provably controls the FDR at the user-defined target level. A real-world example of sparse index tracking demonstrates the proposed method's ability to accurately track the S&P 500 index over the past 20 years based on a small number of stocks. An open-source implementation is provided within the R package TRexSelector on CRAN."}
{"Title":"Fast and General Simulation of L\\'evy-driven OU processes for Energy Derivatives","Link":"https:\/\/arxiv.org\/abs\/2401.15483","Authors":"Roberto Baviera, Pietro Manzoni","Abstract":"L\u00e9vy-driven Ornstein-Uhlenbeck (OU) processes represent an intriguing class of stochastic processes that have garnered interest in the energy sector for their ability to capture typical features of market dynamics. However, in the current state of play, Monte Carlo simulations of these processes are not straightforward for two main reasons: i) algorithms are available only for some specific processes within this class; ii) they are often computationally expensive. In this paper, we introduce a new simulation technique designed to address both challenges. It relies on the numerical inversion of the characteristic function, offering a general methodology applicable to all L\u00e9vy-driven OU processes. Moreover, leveraging FFT, the proposed methodology ensures fast and accurate simulations, providing a solid basis for the widespread adoption of these processes in the energy sector. Lastly, the algorithm allows an optimal control of the numerical error. We apply the technique to the pricing of energy derivatives, comparing the results with the existing benchmarks. Our findings indicate that the proposed methodology is at least one order of magnitude faster than the existing algorithms, while maintaining an equivalent level of accuracy."}
{"Title":"The WTP-WTA Gap for Public Goods: New Insights from Compensating and Equivalent Variation Closed-Form Solutions","Link":"https:\/\/arxiv.org\/abs\/2401.15493","Authors":"Daniel H. Karney, Khyati Malik","Abstract":"This study finds exact closed-form solutions for compensating variation (CV) and equivalent variation (EV) for both marginal and non-marginal changes in public goods given homothetic utility. The parameters for these solutions are recoverable from observable data in empirical applications as a single sufficient statistic summarizes consumer preferences. The closed-form CV and EV expressions identify three economic mechanisms that determine the magnitudes of CV and EV. One of these mechanisms, the relative preference effect, helps explain the disparity between willingness to pay (WTP) and willingness to accept (WTA) for public goods."}
{"Title":"The McCormick martingale optimal transport","Link":"https:\/\/arxiv.org\/abs\/2401.15552","Authors":"Erhan Bayraktar, Bingyan Han, Dominykas Norgilas","Abstract":"Martingale optimal transport (MOT) often yields broad price bounds for options, constraining their practical applicability. In this study, we extend MOT by incorporating causality constraints among assets, inspired by the nonanticipativity condition of stochastic processes. However, this introduces a computationally challenging bilinear program. To tackle this issue, we propose McCormick relaxations to ease the bicausal formulation and refer to it as McCormick MOT. The primal attainment and strong duality of McCormick MOT are established under standard assumptions. Empirically, using the lower and upper bounds derived from marginal constraints, the McCormick relaxations reduce the price gap by an average of 1% for stocks with liquid option markets and 4% for those with moderately liquid markets. When tighter bounds on probability masses are applied, the average reduction increases to 12.66%."}
{"Title":"Estimation of domain truncation error for a system of PDEs arising in option pricing","Link":"https:\/\/arxiv.org\/abs\/2401.15570","Authors":"Anindya Goswami, Kuldip Singh Patel","Abstract":"In this paper, a multidimensional system of parabolic partial differential equations arising in European option pricing under a regime-switching market model is studied in details. For solving that numerically, one must truncate the domain and impose an artificial boundary data. By deriving an estimate of the domain truncation error at all the points in the truncated domain, we extend some results in the literature those deal with option pricing equation under constant regime case only. We differ from the existing approach to obtain the error estimate that is sharper in certain region of the domain. Hence, the minimum of proposed and existing gives a strictly sharper estimate. A comprehensive comparison with the existing literature is carried out by considering some numerical examples. Those examples confirm that the improvement in the error estimates is significant."}
{"Title":"Analytic Pricing of SOFR Futures Contracts with Smile and Skew","Link":"https:\/\/arxiv.org\/abs\/2401.15728","Authors":"Aurelio Romero-Berm\u00fadez, Colin Turfus","Abstract":"We introduce a perturbative formalism to solve the backward-looking futures pricing problem. The formalism is based on a time-ordered exponential series which allows to derive the functional form of the integral kernel associated to the backward-Kolmogorov diffusion PDE. We present an analytic pricing formula for SOFR futures contracts under an extension of the Hull-White model which incorporates not only the intrinsic convexity adjustments captured by Mercurio [2018], but also the skew and smile observed in options markets as done in Turfus and Romero-Berm\u00fadez [2023]."}
{"Title":"Does green innovation crowd out other innovation of firms? Based on the extended CDM model and unconditional quantile regressions","Link":"https:\/\/arxiv.org\/abs\/2401.16030","Authors":"Yi Jiang, Richard S.J. Tol","Abstract":"In the era of sustainability, firms grapple with the decision of how much to invest in green innovation and how it influences their economic trajectory. This study employs the Crepon, Duguet, and Mairesse (CDM) framework to examine the conversion of R&D funds into patents and their impact on productivity, effectively addressing endogeneity by utilizing predicted dependent variables at each stage to exclude unobservable factors. Extending the classical CDM model, this study contrasts green and non-green innovations' economic effects. The results show non-green patents predominantly drive productivity gains, while green patents have a limited impact in non-heavy polluting firms. However, in high-pollution and manufacturing sectors, both innovation types equally enhance productivity. Using unconditional quantile regression, I found green innovation's productivity impact follows an inverse U-shape, unlike the U-shaped pattern of non-green innovation. Significantly, in the 50th to 80th productivity percentiles of manufacturing and high-pollution firms, green innovation not only contributes to environmental sustainability but also outperforms non-green innovation economically."}
{"Title":"The Carbon Premium: Correlation or Causation? Evidence from S&P 500 Companies","Link":"https:\/\/arxiv.org\/abs\/2401.16455","Authors":"Namasi G. Sankar, Suryadeepto Nag, Siddhartha P. Chakrabarty, Sankarshan Basu","Abstract":"In the context of whether investors are aware of carbon-related risks, it is often hypothesized that there may be a carbon premium in the value of stocks of firms, conferring an abnormal excess value to firms' shares as a form of compensation to investors for their transition risk exposure through the ownership of carbon instensive stocks. However, there is little consensus in the literature regarding the existence of such a premium. Moreover few studies have examined whether the correlation that is often observed is actually causal. The pertinent question is whether more polluting firms give higher returns or do firms with high returns have less incentive to decarbonize? In this study, we investigate whether firms' emissions is causally linked to the presence of a carbon premium in a panel of 141 firms listed in the S\\&P500 index using fixed-effects analysis, with propensity score weighting to control for selection bias in which firms increase their emissions. We find that there is a statistically significant positive carbon premium associated with Scope 1 emissions, while there is no significant premium associated with Scope 2 emissions, implying that risks associated with direct emissions by the firm are priced, while bought emissions are not."}
{"Title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from Loan Descriptions in P2P Lending","Link":"https:\/\/arxiv.org\/abs\/2401.16458","Authors":"Mario Sanz-Guerrero, Javier Arroyo","Abstract":"Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism, linking borrowers with lenders through online platforms. However, P2P lending faces the challenge of information asymmetry, as lenders often lack sufficient data to assess the creditworthiness of borrowers. This paper proposes a novel approach to address this issue by leveraging the textual descriptions provided by borrowers during the loan application process. Our methodology involves processing these textual descriptions using a Large Language Model (LLM), a powerful tool capable of discerning patterns and semantics within the text. Transfer learning is applied to adapt the LLM to the specific task at hand.\nOur results derived from the analysis of the Lending Club dataset show that the risk score generated by BERT, a widely used LLM, significantly improves the performance of credit risk classifiers. However, the inherent opacity of LLM-based systems, coupled with uncertainties about potential biases, underscores critical considerations for regulatory frameworks and engenders trust-related concerns among end-users, opening new avenues for future research in the dynamic landscape of P2P lending and artificial intelligence."}
{"Title":"Improving Business Insurance Loss Models by Leveraging InsurTech Innovation","Link":"https:\/\/arxiv.org\/abs\/2401.16723","Authors":"Zhiyu Quan, Changyue Hu, Panyi Dong, Emiliano A. Valdez","Abstract":"Recent transformative and disruptive advancements in the insurance industry have embraced various InsurTech innovations. In particular, with the rapid progress in data science and computational capabilities, InsurTech is able to integrate a multitude of emerging data sources, shedding light on opportunities to enhance risk classification and claims management. This paper presents a groundbreaking effort as we combine real-life proprietary insurance claims information together with InsurTech data to enhance the loss model, a fundamental component of insurance companies' risk management. Our study further utilizes various machine learning techniques to quantify the predictive improvement of the InsurTech-enhanced loss model over that of the insurance in-house. The quantification process provides a deeper understanding of the value of the InsurTech innovation and advocates potential risk factors that are unexplored in traditional insurance loss modeling. This study represents a successful undertaking of an academic-industry collaboration, suggesting an inspiring path for future partnerships between industry and academic institutions."}
{"Title":"Enhancing Urban Traffic Safety: An Evaluation on Taipei's Neighborhood Traffic Environment Improvement Program","Link":"https:\/\/arxiv.org\/abs\/2401.16752","Authors":"Frank Y. Huang, Po-Chun Huang","Abstract":"In densely populated urban areas, where interactions between pedestrians, vehicles, and motorcycles are frequent and complex, traffic safety is a critical concern. This paper evaluates the Neighborhood Traffic Environment Improvement Program in Taipei, which involved painting green pedestrian paths, adjusting no-parking red\/yellow lines, and painting speed limit and stop\/slow signs on lanes and alleys. Exploiting staggered rollout of policy implementation and administrative traffic accident data, we found that the program reduced daytime traffic accidents by 5 percent and injuries by 8 percent, while having no significant impact on nighttime incidents. The effectiveness of the program during the day is mainly attributed to the painted green sidewalks, with adequate sunlight playing a part in the program's success. Our findings indicate that cost-effective strategies like green pedestrian lanes can be effective in areas with dense populations and high motorcycle traffic, as they improve safety by encouraging pedestrians to use marked areas and deterring vehicles from these zones."}
{"Title":"Sparse Portfolio Selection via Topological Data Analysis based Clustering","Link":"https:\/\/arxiv.org\/abs\/2401.16920","Authors":"Anubha Goel, Damir Filipovi\u0107, Puneet Pasricha","Abstract":"This paper uses topological data analysis (TDA) tools and introduces a data-driven clustering-based stock selection strategy tailored for sparse portfolio construction. Our asset selection strategy exploits the topological features of stock price movements to select a subset of topologically similar (different) assets for a sparse index tracking (Markowitz) portfolio. We introduce new distance measures, which serve as an input to the clustering algorithm, on the space of persistence diagrams and landscapes that consider the time component of a time series. We conduct an empirical analysis on the S\\&P index from 2009 to 2020, including a study on the COVID-19 data to validate the robustness of our methodology. Our strategy to integrate TDA with the clustering algorithm significantly enhanced the performance of sparse portfolios across various performance measures in diverse market scenarios."}
{"Title":"Determinants of well-being","Link":"https:\/\/arxiv.org\/abs\/2401.17047","Authors":"Cristina Pereira, Herm\u00ednia Gon\u00e7alves, Teresa Sequeira","Abstract":"Traditionally, European social policies have focused on material well-being and social justice, neglecting subjective indicators. This review systematically examines the scientific understanding of well-being, its indicators, and its relationship with governance. It suggests that political systems and institutions significantly impact well-being, and that subjective indicators should be incorporated into public policy decisions. The findings advocate for a more holistic approach to well-being measurement, encompassing both objective and subjective dimensions."}
{"Title":"Partial Law Invariance and Risk Measures","Link":"https:\/\/arxiv.org\/abs\/2401.17265","Authors":"Yi Shen, Zachary Van Oosten, Ruodu Wang","Abstract":"We introduce the concept of partial law invariance, generalizing the concepts of law invariance and probabilistic sophistication widely used in decision theory, as well as statistical and financial applications. This new concept is motivated by practical considerations of decision making under uncertainty, thus connecting the literature on decision theory and that on financial risk management. We fully characterize partially law-invariant coherent risk measures via a novel representation formula. Strong partial law invariance is defined to bridge the gap between the above characterization and the classic representation formula of Kusuoka. We propose a few classes of new risk measures, including partially law-invariant versions of the Expected Shortfall and the entropic risk measures, and illustrate their applications in risk assessment under different types of uncertainty. We provide a tractable optimization formula for computing a class of partially law-invariant coherent risk measures and give a numerical example."}
{"Title":"Assessing Public Perception of Car Automation in Iran: Acceptance and Willingness to Pay for Adaptive Cruise Control","Link":"https:\/\/arxiv.org\/abs\/2401.17329","Authors":"Sina Sahebi, Sahand Heshami, Mohammad Khojastehpour, Ali Rahimi, Mahyar Mollajani","Abstract":"Adaptive cruise control (ACC) is a technology that can reduce fuel consumption and air pollution in the automotive industry. However, its availability in Iran is low compared to industrialized countries. This study examines the acceptance and willingness to pay (WTP) for ACC among Iranian drivers. Data from an online survey of 453 respondents were analyzed using the Technology Acceptance Model (TAM) and an ordered logit model. The results show that perceived ease of use and perceived usefulness affect attitudes toward using ACC, which in turn influence behavioral intentions. The logit model also shows that drivers who find ACC easy and useful, who have higher vehicle prices, and who are women with cruise control (CC) experience are more likely to pay for ACC. To increase the adoption of ACC in Iran, it is suggested to target early adopters, especially women and capitalists, who can influence others with their positive feedback. The benefits of ACC for traffic safety and environmental sustainability should also be emphasized."}
{"Title":"Efficient estimation of parameters in marginals in semiparametric multivariate models","Link":"https:\/\/arxiv.org\/abs\/2401.17334","Authors":"Ivan Medovikov, Valentyn Panchenko, Artem Prokhorov","Abstract":"We consider a general multivariate model where univariate marginal distributions are known up to a parameter vector and we are interested in estimating that parameter vector without specifying the joint distribution, except for the marginals. If we assume independence between the marginals and maximize the resulting quasi-likelihood, we obtain a consistent but inefficient QMLE estimator. If we assume a parametric copula (other than independence) we obtain a full MLE, which is efficient but only under a correct copula specification and may be biased if the copula is misspecified. Instead we propose a sieve MLE estimator (SMLE) which improves over QMLE but does not have the drawbacks of full MLE. We model the unknown part of the joint distribution using the Bernstein-Kantorovich polynomial copula and assess the resulting improvement over QMLE and over misspecified FMLE in terms of relative efficiency and robustness. We derive the asymptotic distribution of the new estimator and show that it reaches the relevant semiparametric efficiency bound. Simulations suggest that the sieve MLE can be almost as efficient as FMLE relative to QMLE provided there is enough dependence between the marginals. We demonstrate practical value of the new estimator with several applications. First, we apply SMLE in an insurance context where we build a flexible semi-parametric claim loss model for a scenario where one of the variables is censored. As in simulations, the use of SMLE leads to tighter parameter estimates. Next, we consider financial risk management examples and show how the use of SMLE leads to superior Value-at-Risk predictions. The paper comes with an online archive which contains all codes and datasets."}
{"Title":"Modeling how and why aquatic vegetation removal can free rural households from poverty-disease traps","Link":"https:\/\/arxiv.org\/abs\/2401.17384","Authors":"Molly J Doruska, Christopher B Barrett, Jason R Rohr","Abstract":"Infectious disease can reduce labor productivity and incomes, trapping subpopulations in a vicious cycle of ill health and poverty. Efforts to boost African farmers' agricultural production through fertilizer use can inadvertently promote the growth of aquatic vegetation that hosts disease vectors. Recent trials established that removing aquatic vegetation habitat for snail intermediate hosts reduces schistosomiasis infection rates in children, while converting the harvested vegetation into compost boosts agricultural productivity and incomes. Our model illustrates how this ecological intervention changes the feedback between the human and natural systems, potentially freeing rural households from poverty-disease traps. We develop a bioeconomic model that interacts an analytical microeconomic model of agricultural households' behavior, health status and incomes over time with a dynamic model of schistosomiasis disease ecology. We calibrate the model with field data from northern Senegal. We show analytically and via simulation that local conversion of invasive aquatic vegetation to compost changes the feedbacks among interlinked disease, aquatic and agricultural systems, reducing schistosomiasis infection and increasing incomes relative to the current status quo, in which villagers rarely remove vegetation. Aquatic vegetation removal disrupts the poverty-disease trap by reducing habitat for snails that vector the infectious helminth and by promoting production of compost that returns to agricultural soils nutrients that currently leach into surface water from on-farm fertilizer applications. The result is healthier people, more productive labor, cleaner water, more productive agriculture, and higher incomes."}
{"Title":"Education Policy and Intergenerational Educational Persistence: Evidence from rural Benin","Link":"https:\/\/arxiv.org\/abs\/2401.17391","Authors":"Christelle Zozoungbo","Abstract":"This paper employs a nonlinear difference-in-differences approach to empirically examine the Maximally Maintained Inequality (MMI) hypothesis in rural Benin. The findings of this study confirm the MMI hypothesis. In particular, it is observed that when 76% of educated parents choose to educate their daughters in the absence of educational programs, in contrast to only 37% among non-educated parents, the average impact of tuition fee subsidy on enrollment probability in primary schools stands at 3.8\\% for non-educated households and 0.27% for educated households. Conversely, in cases where only 27% of educated parents decide to educate their daughters without education programs, the average effect of tuition fee waivers on enrollment probability in primary schools increases to 19.64\\% for non-educated households and 24\\% for educated households. From the analysis of household education decisions influenced by a preference for education and budget constraints, three key conclusions emerge to explain the mechanism behind the MMI. Firstly, when the income advantage of educated households compared to non-educated households is significantly high, irrespective of the level of their preference advantage, reducing the financial cost of education induces a greater shift in education decisions among non-educated households. Secondly, in situations where educated households do not possess an income advantage relative to non-educated households, the reduction in education-related financial costs leads to a more pronounced change in education decisions among educated households. Lastly, for the low-income advantage of educated households, as the income advantage of educated households increases, non-educated households respond more to education policy than educated parents, if the preference advantage of educated households is relatively smaller."}
{"Title":"What you know or who you know? The role of intellectual and social capital in opportunity recognition","Link":"https:\/\/arxiv.org\/abs\/2401.17448","Authors":"Antonio Rafael Ramos-Rodriguez, Jose Aurelio Medina-Garrido, Jose Daniel Lorenzo-Gomez, Jose Ruiz-Navarro","Abstract":"The recognition of business opportunities is the first stage in the entrepreneurial process. The current work analyzes the effects of individuals' possession of and access to knowledge on the probability of recognizing good business opportunities in their area of residence. The authors use an eclectic theoretical framework consisting of intellectual and social capital concepts. In particular, they analyze the role of individuals' educational level, their perception that they have the right knowledge and skills to start a business, whether they own and manage a firm, their contacts with other entrepreneurs, and whether they have been business angels. The hypotheses proposed here are tested using data collected for the GEM project in Spain in 2007. The results show that individuals' access to external knowledge through the social networks in which they participate is fundamental for developing the capacity to recognize new business opportunities."}
{"Title":"Tradeoffs and Comparison Complexity","Link":"https:\/\/arxiv.org\/abs\/2401.17578","Authors":"Cassidy Shubatt, Jeffrey Yang","Abstract":"This paper develops a theory of how tradeoffs govern comparison complexity, and how this complexity generates systematic mistakes in choice. In our model, options are easier to compare when they involve less pronounced tradeoffs, in particular when they are 1) more similar feature-by-feature and 2) closer to dominance. These two postulates yield tractable measures of comparison complexity in the domains of multiattribute, lottery, and intertemporal choice. We show how our model organizes a range of behavioral regularities in choice and valuation, such as context effects, preference reversals, and apparent probability weighting and hyperbolic discounting. We test our model experimentally by varying the strength and nature of tradeoffs. First, we show that our complexity measures predict choice errors, choice inconsistency, and cognitive uncertainty in binary choice data across all three domains. Second, we document that manipulations of comparison complexity can reverse classic behavioral regularities, in line with the predictions of the theory. We apply our theory to study strategic obfuscation by firms in a pricing game."}
{"Title":"Filipino Use of Designer and Luxury Perfumes: A Pilot Study of Consumer Behavior","Link":"https:\/\/arxiv.org\/abs\/2401.17886","Authors":"John Paul P. Miranda, Maria Anna D. Cruz, Dina D. Gonzales, Ma. Rebecca G. Del Rosario, Aira May B. Canlas, Joseph Alexander Bansil","Abstract":"This study investigates the usage patterns and purposes of designer perfumes among Filipino consumers, employing purposive and snowball sampling methods as non-probability sampling techniques. Data was collected using Google Forms, and the majority of respondents purchased full bottles of designer perfumes from retailers, wholesalers, and physical stores, with occasional \"blind purchases.\" Daily usage was common, with respondents applying an average of 5.88 sprays in the morning, favoring fresh scent notes and Eau De Parfum concentration. They tended to alternate perfumes daily, selecting different scent profiles according to the Philippine climate. The study reveals that Filipino respondents primarily use designer perfumes to achieve a pleasant and fresh fragrance. Additionally, these perfumes play a role in boosting self-esteem, elevating mood, and enhancing personal presentation. Some respondents reported fewer common applications, such as using perfume to address insomnia and migraines. Overall, the research highlights the significant role of perfume in the grooming routine of Filipino consumers. This study represents the first attempt to comprehend perfume usage patterns and purposes specifically within the Filipino context. Consequently, its findings are invaluable for manufacturers and marketers targeting the Filipino market, providing insights into consumer preferences and motivations."}
{"Title":"Technological Shocks and Algorithmic Decision Aids in Credence Goods Markets","Link":"https:\/\/arxiv.org\/abs\/2401.17929","Authors":"Alexander Erlei, Lukas Meub","Abstract":"In credence goods markets such as health care or repair services, consumers rely on experts with superior information to adequately diagnose and treat them. Experts, however, are constrained in their diagnostic abilities, which hurts market efficiency and consumer welfare. Technological breakthroughs that substitute or complement expert judgments have the potential to alleviate consumer mistreatment. This article studies how competitive experts adopt novel diagnostic technologies when skills are heterogeneously distributed and obfuscated to consumers. We differentiate between novel technologies that increase expert abilities, and algorithmic decision aids that complement expert judgments, but do not affect an expert's personal diagnostic precision. We show that high-ability experts may be incentivized to forego the decision aid in order to escape a pooling equilibrium by differentiating themselves from low-ability experts. Results from an online experiment support our hypothesis, showing that high-ability experts are significantly less likely than low-ability experts to invest into an algorithmic decision aid. Furthermore, we document pervasive under-investments, and no effect on expert honesty."}
{"Title":"Let's roll back! The challenging task of regulating temporary contracts","Link":"https:\/\/arxiv.org\/abs\/2401.17971","Authors":"Davide Fiaschi, Cristina Tealdi","Abstract":"In this paper, we evaluate the impact of a reform introduced in Italy in 2018 (Decreto Dignit\u00e0), which increased the rigidity of employment protection legislation (EPL) of temporary contracts, rolling back previous policies, to reduce job instability. We use longitudinal labour force data from 2016 to 2019 and adopt a time-series technique within a Rubin Casual Model (RCM) framework to estimate the causal effect of the reform. We find that the reform was successful in reducing persistence into temporary employment and increasing the flow from temporary to permanent employment, in particular among women and young workers in the North of Italy, with significant effects on the stocks of permanent employment (+), temporary employment (-) and unemployment (-). However, this positive outcome came at the cost of higher persistence into inactivity, lower outflows from unemployment to temporary employment and higher outflows from unemployment to inactivity among males and low-educated workers."}
{"Title":"Synthetic Data Applications in Finance","Link":"https:\/\/arxiv.org\/abs\/2401.00081","Authors":"Vamsi K. Potluru, Daniel Borrajo, Andrea Coletta, Niccol\u00f2 Dalmasso, Yousef El-Laham, Elizabeth Fons, Mohsen Ghassemi, Sriram Gopalakrishnan, Vikesh Gosai, Eleonora Krea\u010di\u0107, Ganapathy Mani, Saheed Obitayo, Deepak Paramanand, Natraj Raman, Mikhail Solonin, Srijan Sood, Svitlana Vyetrenko, Haibei Zhu, Manuela Veloso, Tucker Balch","Abstract":"Synthetic data has made tremendous strides in various commercial settings including finance, healthcare, and virtual reality. We present a broad overview of prototypical applications of synthetic data in the financial sector and in particular provide richer details for a few select ones. These cover a wide variety of data modalities including tabular, time-series, event-series, and unstructured arising from both markets and retail financial applications. Since finance is a highly regulated industry, synthetic data is a potential approach for dealing with issues related to privacy, fairness, and explainability. Various metrics are utilized in evaluating the quality and effectiveness of our approaches in these applications. We conclude with open directions in synthetic data in the context of the financial domain."}
{"Title":"Matching of Users and Creators in Two-Sided Markets with Departures","Link":"https:\/\/arxiv.org\/abs\/2401.00313","Authors":"Daniel Huttenlocher, Hannah Li, Liang Lyu, Asuman Ozdaglar, James Siderius","Abstract":"Many online platforms of today, including social media sites, are two-sided markets bridging content creators and users. Most of the existing literature on platform recommendation algorithms largely focuses on user preferences and decisions, and does not simultaneously address creator incentives. We propose a model of content recommendation that explicitly focuses on the dynamics of user-content matching, with the novel property that both users and creators may leave the platform permanently if they do not experience sufficient engagement. In our model, each player decides to participate at each time step based on utilities derived from the current match: users based on alignment of the recommended content with their preferences, and creators based on their audience size. We show that a user-centric greedy algorithm that does not consider creator departures can result in arbitrarily poor total engagement, relative to an algorithm that maximizes total engagement while accounting for two-sided departures. Moreover, in stark contrast to the case where only users or only creators leave the platform, we prove that with two-sided departures, approximating maximum total engagement within any constant factor is NP-hard. We present two practical algorithms, one with performance guarantees under mild assumptions on user preferences, and another that tends to outperform algorithms that ignore two-sided departures in practice."}
{"Title":"Financial Time-Series Forecasting: Towards Synergizing Performance And Interpretability Within a Hybrid Machine Learning Approach","Link":"https:\/\/arxiv.org\/abs\/2401.00534","Authors":"Shun Liu, Kexin Wu, Chufeng Jiang, Bin Huang, Danqing Ma","Abstract":"In the realm of cryptocurrency, the prediction of Bitcoin prices has garnered substantial attention due to its potential impact on financial markets and investment strategies. This paper propose a comparative study on hybrid machine learning algorithms and leverage on enhancing model interpretability. Specifically, linear regression(OLS, LASSO), long-short term memory(LSTM), decision tree regressors are introduced. Through the grounded experiments, we observe linear regressor achieves the best performance among candidate models. For the interpretability, we carry out a systematic overview on the preprocessing techniques of time-series statistics, including decomposition, auto-correlational function, exponential triple forecasting, which aim to excavate latent relations and complex patterns appeared in the financial time-series forecasting. We believe this work may derive more attention and inspire more researches in the realm of time-series analysis and its realistic applications."}
{"Title":"Urban Street Network Design and Transport-Related Greenhouse Gas Emissions around the World","Link":"https:\/\/arxiv.org\/abs\/2401.01411","Authors":"Geoff Boeing, Clemens Pilgram, Yougeng Lu","Abstract":"This study estimates the relationships between street network characteristics and transport-sector CO2 emissions across every urban area in the world and investigates whether they are the same across development levels and urban design paradigms. The prior literature has estimated relationships between street network design and transport emissions -- including greenhouse gases implicated in climate change -- primarily through case studies focusing on certain world regions or relatively small samples of cities, complicating generalizability and applicability for evidence-informed practice. Our worldwide study finds that straighter, more-connected, and less-overbuilt street networks are associated with lower transport emissions, all else equal. Importantly, these relationships vary across development levels and design paradigms -- yet most prior literature reports findings from urban areas that are outliers by global standards. Planners need a better empirical base for evidence-informed practice in under-studied regions, particularly the rapidly urbanizing Global South."}
{"Title":"Non-Atomic Arbitrage in Decentralized Finance","Link":"https:\/\/arxiv.org\/abs\/2401.01622","Authors":"Lioba Heimbach, Vabuk Pahari, Eric Schertenleib","Abstract":"The prevalence of maximal extractable value (MEV) in the Ethereum ecosystem has led to a characterization of the latter as a dark forest. Studies of MEV have thus far largely been restricted to purely on-chain MEV, i.e., sandwich attacks, cyclic arbitrage, and liquidations. In this work, we shed light on the prevalence of non-atomic arbitrage on decentralized exchanges (DEXes) on the Ethereum blockchain. Importantly, non-atomic arbitrage exploits price differences between DEXes on the Ethereum blockchain as well as exchanges outside the Ethereum blockchain (i.e., centralized exchanges or DEXes on other blockchains). Thus, non-atomic arbitrage is a type of MEV that involves actions on and off the Ethereum blockchain.\nIn our study of non-atomic arbitrage, we uncover that more than a fourth of the volume on Ethereum's biggest five DEXes from the merge until 31 October 2023 can likely be attributed to this type of MEV. We further highlight that only eleven searchers are responsible for more than 80% of the identified non-atomic arbitrage volume sitting at a staggering $132 billion and draw a connection between the centralization of the block construction market and non-atomic arbitrage. Finally, we discuss the security implications of these high-value transactions that account for more than 10% of Ethereum's total block value and outline possible mitigations."}
{"Title":"Text mining arXiv: a look through quantitative finance papers","Link":"https:\/\/arxiv.org\/abs\/2401.01751","Authors":"Michele Leonardo Bianchi","Abstract":"This paper explores articles hosted on the arXiv preprint server with the aim to uncover valuable insights hidden in this vast collection of research. Employing text mining techniques and through the application of natural language processing methods, we examine the contents of quantitative finance papers posted in arXiv from 1997 to 2022. We extract and analyze crucial information from the entire documents, including the references, to understand the topics trends over time and to find out the most cited researchers and journals on this domain. Additionally, we compare numerous algorithms to perform topic modeling, including state-of-the-art approaches."}
{"Title":"Negatively dependent optimal risk sharing","Link":"https:\/\/arxiv.org\/abs\/2401.03328","Authors":"Jean-Gabriel Lauzier, Liyuan Lin, Ruodu Wang","Abstract":"We analyze the problem of optimally sharing risk using allocations that exhibit counter-monotonicity, the most extreme form of negative dependence. Counter-monotonic allocations take the form of either \"winner-takes-all\" lotteries or \"loser-loses-all\" lotteries, and we respectively refer to these (normalized) cases as jackpot or scapegoat allocations. Our main theorem, the counter-monotonic improvement theorem, states that for a given set of random variables that are either all bounded from below or all bounded from above, one can always find a set of counter-monotonic random variables such that each component is greater or equal than its counterpart in the convex order. We show that Pareto optimal allocations, if they exist, must be jackpot allocations when all agents are risk seeking. We essentially obtain the opposite when all agents have discontinuous Bernoulli utility functions, as scapegoat allocations maximize the probability of being above the discontinuity threshold. We also consider the case of rank-dependent expected utility (RDU) agents and find conditions which guarantee that RDU agents prefer jackpot allocations. We provide an application for the mining of cryptocurrencies and show that in contrast to risk-averse miners, RDU miners with small computing power never join a mining pool. Finally, we characterize the competitive equilibria with risk-seeking agents, providing a first and second fundamental theorem of welfare economics where all equilibrium allocations are jackpot allocations."}
{"Title":"Computing the Gerber-Shiu function with interest and a constant dividend barrier by physics-informed neural networks","Link":"https:\/\/arxiv.org\/abs\/2401.04378","Authors":"Zan Yu, Lianzeng Zhang","Abstract":"In this paper, we propose a new efficient method for calculating the Gerber-Shiu discounted penalty function. Generally, the Gerber-Shiu function usually satisfies a class of integro-differential equation. We introduce the physics-informed neural networks (PINN) which embed a differential equation into the loss of the neural network using automatic differentiation. In addition, PINN is more free to set boundary conditions and does not rely on the determination of the initial value. This gives us an idea to calculate more general Gerber-Shiu functions. Numerical examples are provided to illustrate the very good performance of our approximation."}
{"Title":"On the Martingale Schr\\\"odinger Bridge between Two Distributions","Link":"https:\/\/arxiv.org\/abs\/2401.05209","Authors":"Marcel Nutz, Johannes Wiesel","Abstract":"We study a martingale Schr\u00f6dinger bridge problem: given two probability distributions, find their martingale coupling with minimal relative entropy. Our main result provides Schr\u00f6dinger potentials for this coupling. Namely, under certain conditions, the log-density of the optimal coupling is given by a triplet of real functions representing the marginal and martingale constraints. The potentials are also described as the solution of a dual problem."}
{"Title":"Designing Heterogeneous LLM Agents for Financial Sentiment Analysis","Link":"https:\/\/arxiv.org\/abs\/2401.05799","Authors":"Frank Xing","Abstract":"Large language models (LLMs) have drastically changed the possible ways to design intelligent systems, shifting the focuses from massive data acquisition and new modeling training to human alignment and strategical elicitation of the full potential of existing pre-trained models. This paradigm shift, however, is not fully realized in financial sentiment analysis (FSA), due to the discriminative nature of this task and a lack of prescriptive knowledge of how to leverage generative models in such a context. This study investigates the effectiveness of the new paradigm, i.e., using LLMs without fine-tuning for FSA. Rooted in Minsky's theory of mind and emotions, a design framework with heterogeneous LLM agents is proposed. The framework instantiates specialized agents using prior domain knowledge of the types of FSA errors and reasons on the aggregated agent discussions. Comprehensive evaluation on FSA datasets show that the framework yields better accuracies, especially when the discussions are substantial. This study contributes to the design foundations and paves new avenues for LLMs-based FSA. Implications on business and management are also discussed."}
{"Title":"Optimal Investment with Herd Behaviour Using Rational Decision Decomposition","Link":"https:\/\/arxiv.org\/abs\/2401.07183","Authors":"Huisheng Wang, H. Vicky Zhao","Abstract":"In this paper, we study the optimal investment problem considering the herd behaviour between two agents, including one leading expert and one following agent whose decisions are influenced by those of the leading expert. In the objective functional of the optimal investment problem, we introduce the average deviation term to measure the distance between the two agents' decisions and use the variational method to find its analytical solution. To theoretically analyze the impact of the following agent's herd behaviour on his\/her decision, we decompose his\/her optimal decision into a convex linear combination of the two agents' rational decisions, which we call the rational decision decomposition. Furthermore, we define the weight function in the rational decision decomposition as the following agent's investment opinion to measure the preference of his\/her own rational decision over that of the leading expert. We use the investment opinion to quantitatively analyze the impact of the herd behaviour, the following agent's initial wealth, the excess return, and the volatility of the risky asset on the optimal decision. We validate our analyses through numerical experiments on real stock data. This study is crucial to understanding investors' herd behaviour in decision-making and designing effective mechanisms to guide their decisions."}
{"Title":"Cash and Card Acceptance in Retail Payments: Motivations and Factors","Link":"https:\/\/arxiv.org\/abs\/2401.07682","Authors":"Samuel Vandak, Geoffrey Goodell","Abstract":"The landscape of payment methods in retail is a complex and evolving area. Vendors are motivated to conduct an appropriate analysis to decide what payment methods to accept out of a vast range of options. Many factors are included in this decision process, some qualitative and some quantitative. The following research project investigates vendors' acceptance of cards and cash from various viewpoints, all chosen to represent a novel perspective, including the barriers and preferences for each and correlations with external demographic factors. We observe that lower interchange fees, limited in this instance by the regulatory framework, play a crucial role in facilitating merchants' acceptance of card payments. The regulatory constraints on interchange fees create a favorable cost structure for merchants, making card payment adoption financially feasible. However, additional factors like technological readiness and consumer preferences might also play a significant role in their decision-making process. We also note that aggregate Merchant Service Providers (MSPs) have positively impacted the payment landscape by offering more competitive fee rates, particularly beneficial for small merchants and entrepreneurs. However, associated risks, such as account freezes or abrupt terminations, pose challenges and often lack transparency. Last, the quantitative analysis of the relationship between demographic variables and acceptance of payment types is presented. This analysis combines the current landscape of payment acceptance in the UK with data from the most recent census from 2021. We show that the unemployment rates shape card and cash acceptance, age affects contactless preference, and work-from-home impacts credit card preference."}
{"Title":"A Day-to-Day Dynamical Approach to the Most Likely User Equilibrium Problem","Link":"https:\/\/arxiv.org\/abs\/2401.08013","Authors":"Jiayang Li, Qianni Wang, Liyang Feng, Jun Xie, Yu Marco Nie","Abstract":"The lack of a unique user equilibrium (UE) route flow in traffic assignment has posed a significant challenge to many transportation applications. The maximum-entropy principle, which advocates for the consistent selection of the most likely solution as a representative, is often used to address the challenge. Built on a recently proposed day-to-day (DTD) discrete-time dynamical model called cumulative logit (CULO), this study provides a new behavioral underpinning for the maximum-entropy UE (MEUE) route flow. It has been proven that CULO can reach a UE state without presuming travelers are perfectly rational. Here, we further establish that CULO always converges to the MEUE route flow if (i) travelers have zero prior information about routes and thus are forced to give all routes an equal choice probability, or (ii) all travelers gather information from the same source such that the so-called general proportionality condition is satisfied. Thus, CULO may be used as a practical solution algorithm for the MEUE problem. To put this idea into practice, we propose to eliminate the route enumeration requirement of the original CULO model through an iterative route discovery scheme. We also examine the discrete-time versions of four popular continuous-time dynamical models and compare them to CULO. The analysis shows that the replicator dynamic is the only one that has the potential to reach the MEUE solution with some regularity. The analytical results are confirmed through numerical experiments."}
{"Title":"Transformer-based approach for Ethereum Price Prediction Using Crosscurrency correlation and Sentiment Analysis","Link":"https:\/\/arxiv.org\/abs\/2401.08077","Authors":"Shubham Singh, Mayur Bhat","Abstract":"The research delves into the capabilities of a transformer-based neural network for Ethereum cryptocurrency price forecasting. The experiment runs around the hypothesis that cryptocurrency prices are strongly correlated with other cryptocurrencies and the sentiments around the cryptocurrency. The model employs a transformer architecture for several setups from single-feature scenarios to complex configurations incorporating volume, sentiment, and correlated cryptocurrency prices. Despite a smaller dataset and less complex architecture, the transformer model surpasses ANN and MLP counterparts on some parameters. The conclusion presents a hypothesis on the illusion of causality in cryptocurrency price movements driven by sentiments."}
{"Title":"A techno-economic model for avoiding conflicts of interest between owners of offshore wind farms and maintenance suppliers","Link":"https:\/\/arxiv.org\/abs\/2401.08251","Authors":"Alberto Pliego Marug\u00e1n, Fausto Pedro Garc\u00eda M\u00e1rquez, Jes\u00fas Mar\u00eda Pinar P\u00e9rez","Abstract":"Currently, wind energy is one of the most important sources of renewable energy. Offshore locations for wind turbines are increasingly exploited because of their numerous advantages. However, offshore wind farms require high investment in maintenance service. Due to its complexity and special requirements, maintenance service is usually outsourced by wind farm owners. In this paper, we propose a novel approach to determine, quantify, and reduce the possible conflicts of interest between owners and maintenance suppliers. We created a complete techno-economic model to address this problem from an impartial point of view. An iterative process was developed to obtain statistical results that can help stakeholders negotiate the terms of the contract, in which the availability of the wind farm is the reference parameter by which to determine penalisations and incentives. Moreover, a multi-objective programming problem was addressed that maximises the profits of both parties without losing the alignment of their interests. The main scientific contribution of this paper is the maintenance analysis of offshore wind farms from two perspectives: that of the owner and the maintenance supplier. This analysis evaluates the conflicts of interest of both parties. In addition, we demonstrate that proper adjustment of some parameters, such as penalisation, incentives, and resources, and adequate control of availability can help reduce this conflict of interests."}
{"Title":"Mean-Field SDEs driven by\nG\n-Brownian Motion","Link":"https:\/\/arxiv.org\/abs\/2401.09113","Authors":"Karl-Wilhelm Georg Bollweg, Thilo Meyer-Brandis","Abstract":"We extend the notion of mean-field SDEs to SDEs driven by G-Brownian motion. More precisely, we consider a G-SDE where the coefficients depend not only on time and the current state but also on the solution as random variable."}
{"Title":"Functional Limit Theorems for Hawkes Processes","Link":"https:\/\/arxiv.org\/abs\/2401.11495","Authors":"Ulrich Horst, Wei Xu","Abstract":"We prove that the long-run behavior of Hawkes processes is fully determined by the average number and the dispersion of child events. For subcritical processes we provide FLLNs and FCLTs under minimal conditions on the kernel of the process with the precise form of the limit theorems depending strongly on the dispersion of child events. For a critical Hawkes process with weakly dispersed child events, functional central limit theorems do not hold. Instead, we prove that the rescaled intensity processes and rescaled Hawkes processes behave like CIR-processes without mean-reversion, respectively integrated CIR-processes. We provide the rate of convergence by establishing an upper bound on the Wasserstein distance between the distributions of rescaled Hawkes process and the corresponding limit process. By contrast, critical Hawkes process with heavily dispersed child events share many properties of subcritical ones. In particular, functional limit theorems hold. However, unlike subcritical processes critical ones with heavily dispersed child events display long-range dependencies."}
{"Title":"General duality and dual attainment for adapted transport","Link":"https:\/\/arxiv.org\/abs\/2401.11958","Authors":"Daniel Kr\u0161ek, Gudmund Pammer","Abstract":"We investigate duality and existence of dual optimizers for several adapted optimal transport problems under minimal assumptions. This includes the causal and bicausal transport, the barycenter problem, and a general multimarginal problem incorporating causality constraints. Moreover, we discuss applications of our results in robust finance. We consider a non-dominated model of several financial markets where stocks are traded dynamically, but the joint stock dynamics are unknown. We show that a no-arbitrage assumption in a quasi-sure sense naturally leads to sets of multicausal couplings. Consequently, computing the robust superhedging price is equivalent to solving an adapted transport problem, and finding a superhedging strategy means solving the corresponding dual."}
{"Title":"From Numbers to Words: Multi-Modal Bankruptcy Prediction Using the ECL Dataset","Link":"https:\/\/arxiv.org\/abs\/2401.12652","Authors":"Henri Arno, Klaas Mulier, Joke Baeck, Thomas Demeester","Abstract":"In this paper, we present ECL, a novel multi-modal dataset containing the textual and numerical data from corporate 10K filings and associated binary bankruptcy labels. Furthermore, we develop and critically evaluate several classical and neural bankruptcy prediction models using this dataset. Our findings suggest that the information contained in each data modality is complementary for bankruptcy prediction. We also see that the binary bankruptcy prediction target does not enable our models to distinguish next year bankruptcy from an unhealthy financial situation resulting in bankruptcy in later years. Finally, we explore the use of LLMs in the context of our task. We show how GPT-based models can be used to extract meaningful summaries from the textual data but zero-shot bankruptcy prediction results are poor. All resources required to access and update the dataset or replicate our experiments are available on this http URL."}
{"Title":"Multicausal transport: barycenters and dynamic matching","Link":"https:\/\/arxiv.org\/abs\/2401.12748","Authors":"Beatrice Acciaio, Daniel Kr\u0161ek, Gudmund Pammer","Abstract":"We introduce a multivariate version of adapted transport, which we name multicausal transport, involving several filtered processes among which causality constraints are imposed. Subsequently, we consider the barycenter problem for stochastic processes with respect to causal and bicausal optimal transport, and study its connection to specific multicausal transport problems. Attainment and duality of the aforementioned problems are provided. As an application, we study a matching problem in a dynamic setting where agents' types evolve over time. We link this to a causal barycenter problem and thereby show existence of equilibria."}
{"Title":"Evaluating the Determinants of Mode Choice Using Statistical and Machine Learning Techniques in the Indian Megacity of Bengaluru","Link":"https:\/\/arxiv.org\/abs\/2401.13977","Authors":"Tanmay Ghosh, Nithin Nagaraj","Abstract":"The decision making involved behind the mode choice is critical for transportation planning. While statistical learning techniques like discrete choice models have been used traditionally, machine learning (ML) models have gained traction recently among the transportation planners due to their higher predictive performance. However, the black box nature of ML models pose significant interpretability challenges, limiting their practical application in decision and policy making. This study utilised a dataset of 1350 households belonging to low and low-middle income bracket in the city of Bengaluru to investigate mode choice decision making behaviour using Multinomial logit model and ML classifiers like decision trees, random forests, extreme gradient boosting and support vector machines. In terms of accuracy, random forest model performed the best (0.788 on training data and 0.605 on testing data) compared to all the other models. This research has adopted modern interpretability techniques like feature importance and individual conditional expectation plots to explain the decision making behaviour using ML models. A higher travel costs significantly reduce the predicted probability of bus usage compared to other modes (a 0.66\\% and 0.34\\% reduction using Random Forests and XGBoost model for 10\\% increase in travel cost). However, reducing travel time by 10\\% increases the preference for the metro (0.16\\% in Random Forests and 0.42% in XGBoost). This research augments the ongoing research on mode choice analysis using machine learning techniques, which would help in improving the understanding of the performance of these models with real-world data in terms of both accuracy and interpretability."}
{"Title":"MTRGL:Effective Temporal Correlation Discerning through Multi-modal Temporal Relational Graph Learning","Link":"https:\/\/arxiv.org\/abs\/2401.14199","Authors":"Junwei Su, Shan Wu, Jinhui Li","Abstract":"In this study, we explore the synergy of deep learning and financial market applications, focusing on pair trading. This market-neutral strategy is integral to quantitative finance and is apt for advanced deep-learning techniques. A pivotal challenge in pair trading is discerning temporal correlations among entities, necessitating the integration of diverse data modalities. Addressing this, we introduce a novel framework, Multi-modal Temporal Relation Graph Learning (MTRGL). MTRGL combines time series data and discrete features into a temporal graph and employs a memory-based temporal graph neural network. This approach reframes temporal correlation identification as a temporal graph link prediction task, which has shown empirical success. Our experiments on real-world datasets confirm the superior performance of MTRGL, emphasizing its promise in refining automated pair trading strategies."}
{"Title":"Robust Estimation of the Tail Index of a Single Parameter Pareto Distribution from Grouped Data","Link":"https:\/\/arxiv.org\/abs\/2401.14593","Authors":"Chudamani Poudyal","Abstract":"Numerous robust estimators exist as alternatives to the maximum likelihood estimator (MLE) when a completely observed ground-up loss severity sample dataset is available. However, the options for robust alternatives to MLE become significantly limited when dealing with grouped loss severity data, with only a handful of methods like least squares, minimum Hellinger distance, and optimal bounded influence function available. This paper introduces a novel robust estimation technique, the Method of Truncated Moments (MTuM), specifically designed to estimate the tail index of a Pareto distribution from grouped data. Inferential justification of MTuM is established by employing the central limit theorem and validating them through a comprehensive simulation study."}
{"Title":"Tacit algorithmic collusion in deep reinforcement learning guided price competition: A study using EV charge pricing game","Link":"https:\/\/arxiv.org\/abs\/2401.15108","Authors":"Diwas Paudel, Tapas K. Das","Abstract":"Players in pricing games with complex structures are increasingly adopting artificial intelligence (AI) aided learning algorithms to make pricing decisions for maximizing profits. This is raising concern for the antitrust agencies as the practice of using AI may promote tacit algorithmic collusion among otherwise independent players. Recent studies of games in canonical forms have shown contrasting claims ranging from none to a high level of tacit collusion among AI-guided players. In this paper, we examine the concern for tacit collusion by considering a practical game where EV charging hubs compete by dynamically varying their prices. Such a game is likely to be commonplace in the near future as EV adoption grows in all sectors of transportation. The hubs source power from the day-ahead (DA) and real-time (RT) electricity markets as well as from in-house battery storage systems. Their goal is to maximize profits via pricing and efficiently managing the cost of power usage. To aid our examination, we develop a two-step data-driven methodology. The first step obtains the DA commitment by solving a stochastic model. The second step generates the pricing strategies by solving a competitive Markov decision process model using a multi-agent deep reinforcement learning (MADRL) framework. We evaluate the resulting pricing strategies using an index for the level of tacit algorithmic collusion. An index value of zero indicates no collusion (perfect competition) and one indicates full collusion (monopolistic behavior). Results from our numerical case study yield collusion index values between 0.14 and 0.45, suggesting a low to moderate level of collusion."}
{"Title":"A Mean Field Game Approach to Relative Investment-Consumption Games with Habit Formation","Link":"https:\/\/arxiv.org\/abs\/2401.15659","Authors":"Zongxia Liang, Keyu Zhang","Abstract":"This paper studies an optimal investment-consumption problem for competitive agents with exponential or power utilities and a common finite time horizon. Each agent regards the average of habit formation and wealth from all peers as benchmarks to evaluate the performance of her decision. We formulate the n-agent game problems and the corresponding mean field game problems under the two utilities. One mean field equilibrium is derived in a closed form in each problem. In each problem with n agents, an approximate Nash equilibrium is then constructed using the obtained mean field equilibrium when n is sufficiently large. The explicit convergence order in each problem can also be obtained. In addition, we provide some numerical illustrations of our results."}
{"Title":"Robust Functional Data Analysis for Stochastic Evolution Equations in Infinite Dimensions","Link":"https:\/\/arxiv.org\/abs\/2401.16286","Authors":"Dennis Schroers","Abstract":"We develop an asymptotic theory for the jump robust measurement of covariations in the context of stochastic evolution equation in infinite dimensions. Namely, we identify scaling limits for realized covariations of solution processes with the quadratic covariation of the latent random process that drives the evolution equation which is assumed to be a Hilbert space-valued semimartingale. We discuss applications to dynamically consistent and outlier-robust dimension reduction in the spirit of functional principal components and the estimation of infinite-dimensional stochastic volatility models."}
{"Title":"AI Oversight and Human Mistakes: Evidence from Centre Court","Link":"https:\/\/arxiv.org\/abs\/2401.16754","Authors":"David Almog, Romain Gauriot, Lionel Page, Daniel Martin","Abstract":"Powered by the increasing predictive capabilities of machine learning algorithms, artificial intelligence (AI) systems have begun to be used to overrule human mistakes in many settings. We provide the first field evidence this AI oversight carries psychological costs that can impact human decision-making. We investigate one of the highest visibility settings in which AI oversight has occurred: the Hawk-Eye review of umpires in top tennis tournaments. We find that umpires lowered their overall mistake rate after the introduction of Hawk-Eye review, in line with rational inattention given psychological costs of being overruled by AI. We also find that umpires increased the rate at which they called balls in, which produced a shift from making Type II errors (calling a ball out when in) to Type I errors (calling a ball in when out). We structurally estimate the psychological costs of being overruled by AI using a model of rational inattentive umpires, and our results suggest that because of these costs, umpires cared twice as much about Type II errors under AI oversight."}
{"Title":"Convergence of the deep BSDE method for stochastic control problems formulated through the stochastic maximum principle","Link":"https:\/\/arxiv.org\/abs\/2401.17472","Authors":"Zhipeng Huang, Balint Negyesi, Cornelis W. Oosterlee","Abstract":"It is well-known that decision-making problems from stochastic control can be formulated by means of a forward-backward stochastic differential equation (FBSDE). Recently, the authors of Ji et al. 2022 proposed an efficient deep learning algorithm based on the stochastic maximum principle (SMP). In this paper, we provide a convergence result for this deep SMP-BSDE algorithm and compare its performance with other existing methods. In particular, by adopting a strategy as in Han and Long 2020, we derive a-posteriori estimate, and show that the total approximation error can be bounded by the value of the loss functional and the discretization error. We present numerical examples for high-dimensional stochastic control problems, both in case of drift- and diffusion control, which showcase superior performance compared to existing algorithms."}
